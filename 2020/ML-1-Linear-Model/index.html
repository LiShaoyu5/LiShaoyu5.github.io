<!DOCTYPE html>
<html lang="en">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/tabicon.png">
  <link rel="icon" type="image/png" href="/img/tabicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="Li Shaoyu">
  <meta name="keywords" content="">
  <title>[机器学习理论]线性模型 - AiArt</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/atom-one-dark.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


<meta name="generator" content="Hexo 4.2.1"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>AiArt</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/default.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2020-08-04 17:22">
      August 4, 2020 pm
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      3k 字
    </span>
  

  

  
  
    
      <!-- LeanCloud 统计文章PV -->
      <span id="leancloud-post-views-container" class="post-meta" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="leancloud-post-views"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
              <p class="note note-info">
                
                  本文最后更新于：August 5, 2020 am
                
              </p>
            
            <article class="markdown-body">
              <h1 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h1><p>对于数据集$D={(x_1, y_1),(x_2, y_2),…,(x_m, y_m)}$，其中$x_i$为$d$维向量，$y_i$为实数。试图求得一个使用属性的线性组合进行预测的函数：</p>
<script type="math/tex; mode=display">f(x)=w_1x_1+w_2x_2+...w_dx_d+b</script><p>或用向量形式表示：</p>
<script type="math/tex; mode=display">f(x)=w^Tx+b</script><p>这种模型称作<strong>线性模型</strong>。</p>
<p>线性模型形式简单、易于建模，且具有较好的可解释性。</p>
<h2 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1. 线性回归"></a>1. 线性回归</h2><p>对于数据集$D={(x_1, y_1),(x_2, y_2),…,(x_m, y_m)}$，其中$x_i$为$d$维向量，$y_i$为实数。</p>
<h3 id="1-1-一元线性回归的最优值求解"><a href="#1-1-一元线性回归的最优值求解" class="headerlink" title="1.1 一元线性回归的最优值求解"></a>1.1 一元线性回归的最优值求解</h3><p>从一元线性回归开始考虑（即$xi$为实数），线性回归试图学得：</p>
<script type="math/tex; mode=display">f(x_i)=wx_i+b，使得f(x_i)\simeq y_i</script><p><strong>均方误差</strong>(Mean-Square Error, MSE)是回归任务中最常用的性能度量。线性回归的目标是让预测值与真实值之间的差别最小，在这里就是让均方差最小化，即：</p>
<script type="math/tex; mode=display">(w^*, b^*)=\underset{(w, b)}{arg\ min}\sum_{i=1}^{m}(y_i-f(x_i))^2</script><p>基于均方差最小化求解模型的方法称为<strong>最小二乘法</strong>。在几何意义上，最小二乘法就是试图找到一条直线，使得所有样本点到直线的欧氏距离之和最小。</p>
<p>将$E_{(w,b)}$分别对$w$和$b$求导，得到：</p>
<script type="math/tex; mode=display">\frac{\partial E_{(w,b)}}{\partial w} = 2(w\sum_{i=1}^{m}{x_i}^2-\sum_{i=1}^{m}x_i(y_i-b))</script><script type="math/tex; mode=display">\frac{\partial E_{(w,b)}}{\partial b} = 2(mb-\sum_{i=1}^{m}(y_i-wx_i))</script><p>推导过程：<br><img src="https://wx1.sbimg.cn/2020/08/04/oM2sD.jpg" srcset="/img/loading.gif" alt="求导"></p>
<p>求$E_{(w,b)}$的最小值，需要用到以下定理：</p>
<blockquote>
<p>设函数$f(x,y)$在区间$D$上具有二阶连续偏导数，记$A=f<em>{xx}^{‘ ‘ }(x,y)$，$B=f</em>{xy}^{‘ ‘ }(x,y)$，$C=f_{yy}^{‘ ‘ }(x,y)$则：<br>在$D$上恒有$A&gt;0$，且$AC-B^2\geq 0$时，$f(x,y)$在$D$上是凸函数。<br>在$D$上恒有$A&lt;0$，且$AC-B^2\geq 0$时，$f(x,y)$在$D$上是凹函数。</p>
</blockquote>
<p>

<blockquote>
<p>设函数$f(x,y)$是在开区间$D$上具有连续偏导数的凸（凹）函数，$x_0,y_0\in D$且$f_x ‘(x_0,y_0)=0,f_y ‘(x_0,y_0)=0$，则$f(x_0,y_0)$必为$f(x,y)$在$D$上的最小值（最大值）。</p>
</blockquote>
<p>首先证明$E_{(w,b)}$是其定义域上的凸函数：<br><img src="https://wx2.sbimg.cn/2020/08/04/oMpEO.jpg" srcset="/img/loading.gif" alt="线性回归-凸函数1"><br><img src="https://wx2.sbimg.cn/2020/08/04/oMXo7.jpg" srcset="/img/loading.gif" alt="线性回归-凸函数2"></p>
<p>因此，当$E_{(w,b)}$关于$w$和$b$的导数均为0时，$w$和$b$取得最优解。令上面两式等于0，可求得：</p>
<script type="math/tex; mode=display">w=\frac{ \sum_{i=1}^{m} y_i(x_i-\bar{x})}{\sum_{i=1}^{m}{x_i}^2-\frac{1}{m}(\sum_{i=1}^{m}x_i)^2}</script><script type="math/tex; mode=display">b=\frac{\sum_{i=1}^{m}(y_i-wx_i)}{m}</script><p>推导过程：<br><img src="https://wx2.sbimg.cn/2020/08/04/oMoBk.jpg" srcset="/img/loading.gif" alt="线性回归-最优解"></p>
<h3 id="1-2-向量化"><a href="#1-2-向量化" class="headerlink" title="1.2 向量化"></a>1.2 向量化</h3><p>上面得到的结果中，使用了连续求和方式进行运算。在编程中，这种计算方式一般需要使用循环结构实现。如果能将结果表示成向量之间的运算，就可以调用<code>numpy</code>等矩阵运算库快速计算。</p>
<p>首先将$w$转换成可以使用向量表示的形式：</p>
<script type="math/tex; mode=display">w=\frac{\sum_{i=1}^{m}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{m}(x_i-\bar{x})^2}</script><p>推导过程：<br><img src="https://wx2.sbimg.cn/2020/08/04/oM0ke.jpg" srcset="/img/loading.gif" alt="线性回归-向量化"></p>
<p>规定向量：</p>
<script type="math/tex; mode=display">\mathbf{x}=(x_1,x_2,...,x_m)^T,\;\;\;  \mathbf{x}_d=(x_1-\bar{x},x_2-\bar{x},...,x_m-\bar{x})^T</script><script type="math/tex; mode=display">\mathbf{y}=(y_1,y_2,...,y_m)^T,\;\;\;  \mathbf{y}_d=(y_1-\bar{y},y_2-\bar{y},...,y_m-\bar{y})^T</script><p>则$w$可向量化表示为：</p>
<script type="math/tex; mode=display">w=\frac{\mathbf{x}_d^T\mathbf{y}_d}{\mathbf{x}_d^T\mathbf{x}_d}</script><h3 id="1-3-多元线性回归"><a href="#1-3-多元线性回归" class="headerlink" title="1.3 多元线性回归"></a>1.3 多元线性回归</h3><p>在更一般的情况中，样本$x<em>i$由多个属性表示，即$x_i=(x</em>{i1};x<em>{i2};…;x</em>{id})$。此时线性回归求解的目标是：</p>
<script type="math/tex; mode=display">f(\mathbf{x_i})=\mathbf{w}^T\mathbf{x_i}+b，使得f(\mathbf{x_i})\simeq y_i</script><p>为了方便以矩阵形式表示和运算，令：</p>
<script type="math/tex; mode=display">\hat{\mathbf{w}}=(w_1;w_2;...;w_d;b)</script><script type="math/tex; mode=display">\hat{\mathbf{x}}_i=(x_{i1};x_{i2};...;x_{id};1)</script><p>这样，求解目标就可以表示为：</p>
<script type="math/tex; mode=display">f(\hat{\mathbf{x}}_i)=\hat{\mathbf{w}}^T\hat{\mathbf{x}}_i，使得f(\hat{\mathbf{x}}_i)\simeq y_i</script><p>通过最小二乘法导出损失函数$E(\hat{\mathbf{w}})$：</p>
<script type="math/tex; mode=display">E(\hat{\mathbf{w}})=\sum_{i=1}^{m}(y_i-f(\hat{\mathbf{x}}_i))^2=\sum_{i=1}^{m}(y_i-\hat{\mathbf{w}}^T\hat{\mathbf{x}}_i)^2</script><p>接下来进行向量化表示。令：</p>
<script type="math/tex; mode=display">\mathbf{X}=\begin{pmatrix}
x_{11} & x_{12} & \cdots & x_{1d} & 1\\
x_{21} & x_{22} & \cdots & x_{2d} & 1\\
\vdots & \vdots & \ddots & \vdots & 1\\
x_{m1} & x_{m2} & \cdots & x_{md} & 1
\end{pmatrix}=\begin{pmatrix}
\mathbf{\hat{x}_1}\\
\mathbf{\hat{x}_2}\\
\vdots\\
\mathbf{\hat{x}_m}
\end{pmatrix}</script><script type="math/tex; mode=display">\mathbf {y}=\begin{pmatrix}
y_1\\
y_2\\
\vdots\\
y_m
\end{pmatrix}</script><p>则可以将$E(\hat{\mathbf{w}})$向量化表示为：</p>
<script type="math/tex; mode=display">E(\hat{\mathbf{w}})=(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}})^T(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}})</script><p>推导过程：<br><img src="https://wx2.sbimg.cn/2020/08/04/oMmq6.jpg" srcset="/img/loading.gif" alt="多元线性回归-向量化"></p>
<p>与一元线性回归类似，通过凸函数的性质来求$E(\hat{\mathbf{w}})$的最优解。需要用到以下定理：</p>
<blockquote>
<p><strong>凸集</strong>：设集合$D\in R^n$，如果对任意$x,y\in D$与任意$a\in [0,1]$，有$ax+(1-a)y\in D$，则称集合$D$为凸集。<br>凸集的几何意义是：若两个点属于此集合，则这两点连线上的任意点都属于此集合。</p>
</blockquote>
<p>

<blockquote>
<p><strong>Hessian矩阵</strong>：设n元函数$f(\mathbf{x})$对自变量$\mathbf{x}=(x_1,x_2,…,x_n)^T$的各分量$x_i$的二阶偏导数$\frac{\partial ^2f(\mathbf{x})}{\partial x_i\partial x_j}, i=1,2,…,n;j=1,2,…,n$都存在，则称$f(\mathbf{x})$在点$\mathbf{x}$处二阶可导，并称矩阵</p>
<script type="math/tex; mode=display">\triangledown ^2f(\mathbf{x})=\begin{pmatrix}
\frac{\partial ^2f(\mathbf{x})}{\partial x_i^2} & \frac{\partial ^2f(\mathbf{x})}{\partial x_1\partial x_2} & \cdots & \frac{\partial ^2f(\mathbf{x})}{\partial x_1\partial x_n}\\
\frac{\partial ^2f(\mathbf{x})}{\partial x_2\partial x_1} & \frac{\partial ^2f(\mathbf{x})}{\partial x_2^2} & \cdots & \frac{\partial ^2f(\mathbf{x})}{\partial x_2\partial x_n}\\
\vdots & \vdots & \ddots & \vdots\\
\frac{\partial ^2f(\mathbf{x})}{\partial x_n\partial x_1} & \frac{\partial ^2f(\mathbf{x})}{\partial x_n\partial x_2} & \cdots & \frac{\partial ^2f(\mathbf{x})}{\partial x_n^2}
\end{pmatrix}</script><p>为$f(\mathbf{x})$在点$\mathbf{x}$处的二阶导数或Hessian矩阵，记为$ \triangledown ^2f(\mathbf{x}) $。当$f(\mathbf{x})$对$\mathbf{x}$各分量的所有二阶偏导数都连续时，$ \triangledown ^2f(\mathbf{x}) $为对称矩阵。</p>
</blockquote>
<p>

<blockquote>
<p><strong>多元实值函数凹凸性判断定理</strong>：设$D\subset R^n$是非空凸开集，$f:D\subset R^n\to R$，且$f(\mathbf{x})$在$D$上二阶连续可微，如果$f(\mathbf{x})$的Hessian矩阵$ \triangledown ^2f(\mathbf{x}) $在$D$上是正定的，则$f(\mathbf{x})$是$D$上的严格凸函数。</p>
</blockquote>
<p>

<blockquote>
<p><strong>凸充分性定理</strong>：若$f:R^n\to R$是凸函数，且$f(\mathbf{x})$一阶连续可微，则$\mathbf{x}^<em>$是全局解的充分必要条件是$ \triangledown f(\mathbf{x}^</em>)=0$。</p>
</blockquote>
<p>要求$E(\hat{\mathbf{w}})$的Hessian矩阵，首先求其一阶偏导数：</p>
<script type="math/tex; mode=display">\frac{\partial E(\hat{\mathbf{w}})}{\partial \hat{\mathbf{w}}}=2\mathbf{X}^T(\mathbf{X}\hat{\mathbf{w}}-\mathbf{y})</script><p>推导过程：<br><img src="https://wx2.sbimg.cn/2020/08/04/oMIJI.jpg" srcset="/img/loading.gif" alt="多元线性回归-一阶导数"></p>
<p>再次求导，得到Hessian矩阵：</p>
<script type="math/tex; mode=display">\frac{\partial ^2 E(\hat{\mathbf{w}})}{\partial \hat{\mathbf{w}}\partial \hat{\mathbf{w}}^T}=2\mathbf{X}^T\mathbf{X}</script><p>推导过程：<br><img src="https://wx1.sbimg.cn/2020/08/04/oMF2R.jpg" srcset="/img/loading.gif" alt="多元线性回归-二阶导数"></p>
<p>假设$\mathbf{X}^T\mathbf{X}$是正定矩阵（现实中往往不是，此时按照之后的方法可以求得多个解，都可使均方差最小化，因此这里可以假设它是正定的以方便证明），则$E(\hat{\mathbf{w}})$是关于$\hat{\mathbf{w}}$的凸函数。</p>
<p>因此，使得$ \frac{\partial E(\hat{\mathbf{w}})}{\partial \hat{\mathbf{w}}}=0$的$\hat{\mathbf{w}}^*$使得函数取得最小值：</p>
<script type="math/tex; mode=display">\hat{\mathbf{w}}^*=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}</script><p>推导过程：<br><img src="https://wx1.sbimg.cn/2020/08/04/oMyba.jpg" srcset="/img/loading.gif" alt="多元线性回归-最优解"></p>
<h2 id="2-对数几率回归"><a href="#2-对数几率回归" class="headerlink" title="2. 对数几率回归"></a>2. 对数几率回归</h2><p>线性回归方法使用线性模型完成了回归任务。实际上，线性模型也可以用于分类任务。</p>
<h3 id="2-1-广义线性模型"><a href="#2-1-广义线性模型" class="headerlink" title="2.1 广义线性模型"></a>2.1 广义线性模型</h3><p>在上一节，线性模型的学习目标是让预测值$f(x)$逼近真实值$y$。我们也可以让$f(x)$去逼近$y$的衍生物。例如，如果认为$y$是以指数方式变化，就可以让$f(x)$逼近$\ln y$：</p>
<script type="math/tex; mode=display">\ln y=w^Tx+b</script><p>这种线性模型被称作<strong>对数线性回归</strong>，它实际上是在让$e^{w^Tx+b}$逼近$y$。</p>
<p>由此推广，    可以让$w^Tx+b$去逼近$g(y)$，其中$g(·)$是任意单调可微函数，即：</p>
<script type="math/tex; mode=display">g(y)=w^Tx+b</script><p>也可表示为：</p>
<script type="math/tex; mode=display">y=g^{-1}(w^Tx+b)</script><p>这就是<strong>广义线性模型</strong>，其中$g(·)$称为<strong>联系函数</strong>。</p>
<h3 id="2-2-对数几率回归"><a href="#2-2-对数几率回归" class="headerlink" title="2.2 对数几率回归"></a>2.2 对数几率回归</h3><p>由广义线性模型可以想到，如果要用线性模型做分类任务，只需要找到一个单调可微的联系函数，将线性回归的预测值和真实标记值联系起来即可。</p>
<p>从二分类问题开始考虑，<strong>对数几率函数</strong>（Logistic Function）是一个常用的函数：</p>
<script type="math/tex; mode=display">y=\frac{1}{1+e^{-z}}</script><p>它是单位阶跃函数（由于不连续而无法在此使用）的替代：<br><img src="https://wx2.sbimg.cn/2020/08/05/o03PA.png" srcset="/img/loading.gif" alt="联系函数"></p>
<p>将对数几率函数作为$g^{-1}(·)$，得到：</p>
<script type="math/tex; mode=display">y=\frac{1}{1+e^{-(w^Tx+b)}}</script><p>求逆函数，可以得到此时线性回归预测值的目标函数：</p>
<script type="math/tex; mode=display">\ln{\frac{y}{1-y}}=w^Tx+b</script><p>将$y$视作样本$x$是正例的可能性，则$1-y$是反例可能性，两者比值$\frac{y}{1-y}$称为<strong>几率</strong>（odds）。对它取对数则得到<strong>对数几率</strong>（log odds，又称logit）：$\ln{\frac{y}{1-y}}$。</p>
<p>这样，就可以用线性回归的预测结果去逼近真实标记的对数几率，这个模型称作<strong>对数几率回归</strong>，是一种分类学习方法。</p>
<h3 id="2-3-对数几率回归的参数求解"><a href="#2-3-对数几率回归的参数求解" class="headerlink" title="2.3 对数几率回归的参数求解"></a>2.3 对数几率回归的参数求解</h3><p>根据上节内容，对数几率回归可以表示为：</p>
<script type="math/tex; mode=display">\ln{\frac{p(y=1|x)}{p(y=0|x)}}=w^Tx+b</script><p>显然有：</p>
<script type="math/tex; mode=display">p(y=1|x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}</script><script type="math/tex; mode=display">p(y=0|x)=\frac{1}{1+e^{w^Tx+b}}</script><p>接下来，就可以用极大似然法估计$w$和$b$。</p>
<blockquote>
<p><strong>极大似然估计</strong>：设总体的概率密度函数（或分布律）为$f(y,w_1,w_2,…,w_k)$，$y_1,y_2,…,y_m$是从该总体中抽出的样本。因为$y_1,y_2,…,y_m$相互独立且同分布，所以它们的联合概率密度函数（或联合概率）为：</p>
<script type="math/tex; mode=display">L(y_1,y_2,...,y_m;w_1,w_2,...,w_k)=\prod_{i=i}^{m} f(y_i,w_1,w_2,...,w_k)</script><p>其中，$w_1,w_2,…,y_k$是固定但未知的参数。当已经拥有一组样本观测值$y_1,y_2,…,y_m$，要估计未知参数时，一种直观的想法就是，使得当前样本出现概率最大的参数组可能是真实的。这就是极大似然估计。<br>通常记$ L(y_1,y_2,…,y_m;w_1,w_2,…,w_k)=L(w) $，并称其为<strong>似然函数</strong>，于是求$w$的极大似然估计问题就变成了求$L(w)$的最大值点。<br>由于对数函数是单调递增的，所以$\ln{L(w)}$与$L(w)$有相同的最大值点。通过取对数，可以实现如下转换：</p>
<script type="math/tex; mode=display">\ln{L(w)}=\ln{(\prod_{i=i}^{m} f(y_i,w_1,w_2,...,w_k))}=\sum_{i=i}^{m} \ln{f(y_i,w_1,w_2,...,w_k)}</script><p>称$\ln{L(w)}$为<strong>对数似然函数</strong>，在很多情况下，求它的最大值点更简单。</p>
</blockquote>
<p>利用多元线性回归中用到的思想，将$w$和$b$合并为参数$\beta$，令$\hat{x}=(x;1)$，则上面得到的概率表达式可简化为：</p>
<script type="math/tex; mode=display">p_1(\hat{x};\beta)=p(y=1|x)=\frac{e^{\beta^T\hat{x}}}{1+e^{\beta^T\hat{x}}}</script><script type="math/tex; mode=display">p_0(\hat{x};\beta)=p(y=0|x)=\frac{1}{1+e^{\beta^T\hat{x}}}</script><p>由于当$y=1$时，概率为$p_1$；$y=0$时，概率为$p_0$。所以可以从上面两个式子得到随机变量$y$的分布律（这里给出两种满足需要的表达式，后续的推导使用第一种。使用第二种也可得出相同结果）：</p>
<script type="math/tex; mode=display">p(y|x;w,b)=y·p_1(\hat{x};\beta)+(1-y)·p_0(\hat{x};\beta)</script><script type="math/tex; mode=display">p(y|x;w,b)=[p_1(\hat{x};\beta)]^y[p_0(\hat{x};\beta)]^{1-y}</script><p>接下来，就可以将对数似然函数中的概率密度函数替换为$p(y|x;w,b)$：</p>
<script type="math/tex; mode=display">l(w,b):=\ln{L(w)}=\sum_{i=i}^{m} \ln{(y_i·p_1(\hat{x}_i;\beta)+(1-y_i)·p_0(\hat{x}_i;\beta))}</script><p>将$p_1$和$p_0$代入得：</p>
<script type="math/tex; mode=display">l(w,b)=\sum_{i=i}^{m} \ln{(\frac{y_ie^{\beta^T\hat{x}_i}}{1+e^{\beta^T\hat{x}_i}}+\frac{1-y_i}{1+e^{\beta^T\hat{x}_i}})}</script><p>化简得（由于将其作为损失函数，目标是将损失函数最小化，所以对化简结果取负）：</p>
<script type="math/tex; mode=display">l(w,b)=\sum_{i=i}^{m} (-y_i\beta^T\hat{x}_i+\ln{(1+e^{\beta^T\hat{x}_i})})</script><p>推导过程：<br><img src="https://wx1.sbimg.cn/2020/08/05/o0ys6.jpg" srcset="/img/loading.gif" alt="对数几率回归-对数似然函数化简"></p>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    
                  </div>
                
              </div>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2020/ML-2-Dicision-Tree/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">[机器学习理论]决策树</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2020/TF2-11-Dataset/">
                        <span class="hidden-mobile">[TF2]11. Dataset</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "[机器学习理论]线性模型&nbsp;",
      ],
      cursorChar: "〆",
      typeSpeed: 50,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script>

  
















</body>
</html>
