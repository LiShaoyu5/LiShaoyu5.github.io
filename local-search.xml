<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>在Python中使用正则表达式</title>
    <link href="/2020/PythonRe/"/>
    <url>/2020/PythonRe/</url>
    
    <content type="html"><![CDATA[<p>借Python中的<code>re</code>模块复习一下正则表达式的基础。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> reprint(dir(re)text = <span class="hljs-string">'Username is reg445, password is 123456, email is 123456reg445, phone number is 0111-1234567, id is asd-678907'</span>[<span class="hljs-string">'A'</span>, <span class="hljs-string">'ASCII'</span>, <span class="hljs-string">'DEBUG'</span>, <span class="hljs-string">'DOTALL'</span>, <span class="hljs-string">'I'</span>, <span class="hljs-string">'IGNORECASE'</span>, <span class="hljs-string">'L'</span>, <span class="hljs-string">'LOCALE'</span>, <span class="hljs-string">'M'</span>, <span class="hljs-string">'MULTILINE'</span>, <span class="hljs-string">'Match'</span>, <span class="hljs-string">'Pattern'</span>, <span class="hljs-string">'RegexFlag'</span>, <span class="hljs-string">'S'</span>, <span class="hljs-string">'Scanner'</span>, <span class="hljs-string">'T'</span>, <span class="hljs-string">'TEMPLATE'</span>, <span class="hljs-string">'U'</span>, <span class="hljs-string">'UNICODE'</span>, <span class="hljs-string">'VERBOSE'</span>, <span class="hljs-string">'X'</span>, <span class="hljs-string">'_MAXCACHE'</span>, <span class="hljs-string">'__all__'</span>, <span class="hljs-string">'__builtins__'</span>, <span class="hljs-string">'__cached__'</span>, <span class="hljs-string">'__doc__'</span>, <span class="hljs-string">'__file__'</span>, <span class="hljs-string">'__loader__'</span>, <span class="hljs-string">'__name__'</span>, <span class="hljs-string">'__package__'</span>, <span class="hljs-string">'__spec__'</span>, <span class="hljs-string">'__version__'</span>, <span class="hljs-string">'_cache'</span>, <span class="hljs-string">'_compile'</span>, <span class="hljs-string">'_compile_repl'</span>, <span class="hljs-string">'_expand'</span>, <span class="hljs-string">'_locale'</span>, <span class="hljs-string">'_pickle'</span>, <span class="hljs-string">'_special_chars_map'</span>, <span class="hljs-string">'_subx'</span>, <span class="hljs-string">'compile'</span>, <span class="hljs-string">'copyreg'</span>, <span class="hljs-string">'enum'</span>, <span class="hljs-string">'error'</span>, <span class="hljs-string">'escape'</span>, <span class="hljs-string">'findall'</span>, <span class="hljs-string">'finditer'</span>, <span class="hljs-string">'fullmatch'</span>, <span class="hljs-string">'functools'</span>, <span class="hljs-string">'match'</span>, <span class="hljs-string">'purge'</span>, <span class="hljs-string">'search'</span>, <span class="hljs-string">'split'</span>, <span class="hljs-string">'sre_compile'</span>, <span class="hljs-string">'sre_parse'</span>, <span class="hljs-string">'sub'</span>, <span class="hljs-string">'subn'</span>, <span class="hljs-string">'template'</span>]</code></pre></div><p><code>re</code>中最常用的函数是<code>findall()</code>，返回找到目标文本中所有满足条件的字符串组成的列表。如果目标文本较长，也可使用返回迭代器的<code>finditer()</code>。<code>sub()</code>函数可以将对应位置替换为所需字符串。</p><p>查找<strong>固定字符串</strong>是最简单的情况，直接将需要查找的字符串作为参数：<br><div class="hljs"><pre><code class="hljs Python">print(re.findall(<span class="hljs-string">r'123456'</span>, text))[<span class="hljs-string">'123456'</span>, <span class="hljs-string">'123456'</span>, <span class="hljs-string">'123456'</span>]</code></pre></div></p><p>正则表达式中有字符类别码，用于区分<strong>字符类型</strong>：</p><script type="math/tex; mode=display">|  字符类别码   | 说明  ||  :-:  | :-:  || \d  | 0-9中的任意数字 || \D  | \d以外的任意字符 || \w  | 任意字母、数字或下划线 || \W  | \w以外的任意字符 || \s  | 空格、制表符或换行符 || \S  | \s以外的任意字符 |</script><div class="hljs"><pre><code class="hljs Python">print(re.findall(<span class="hljs-string">r'\w'</span>, text))[<span class="hljs-string">'U'</span>, <span class="hljs-string">'s'</span>, <span class="hljs-string">'e'</span>, <span class="hljs-string">'r'</span>, <span class="hljs-string">'n'</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'m'</span>, <span class="hljs-string">'e'</span>, <span class="hljs-string">'i'</span>, <span class="hljs-string">'s'</span>, <span class="hljs-string">'r'</span>, <span class="hljs-string">'e'</span>, <span class="hljs-string">'g'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'p'</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'s'</span>, <span class="hljs-string">'s'</span>, <span class="hljs-string">'w'</span>, <span class="hljs-string">'o'</span>, <span class="hljs-string">'r'</span>, <span class="hljs-string">'d'</span>, <span class="hljs-string">'i'</span>, <span class="hljs-string">'s'</span>, <span class="hljs-string">'1'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'3'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'6'</span>, <span class="hljs-string">'e'</span>, <span class="hljs-string">'m'</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'i'</span>, <span class="hljs-string">'l'</span>, <span class="hljs-string">'i'</span>, <span class="hljs-string">'s'</span>, <span class="hljs-string">'1'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'3'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'6'</span>, <span class="hljs-string">'r'</span>, <span class="hljs-string">'e'</span>, <span class="hljs-string">'g'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'p'</span>, <span class="hljs-string">'h'</span>, <span class="hljs-string">'o'</span>, <span class="hljs-string">'n'</span>, <span class="hljs-string">'e'</span>, <span class="hljs-string">'n'</span>, <span class="hljs-string">'u'</span>, <span class="hljs-string">'m'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'e'</span>, <span class="hljs-string">'r'</span>, <span class="hljs-string">'i'</span>, <span class="hljs-string">'s'</span>, <span class="hljs-string">'0'</span>, <span class="hljs-string">'1'</span>, <span class="hljs-string">'1'</span>, <span class="hljs-string">'1'</span>, <span class="hljs-string">'1'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'3'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'6'</span>, <span class="hljs-string">'7'</span>, <span class="hljs-string">'i'</span>, <span class="hljs-string">'d'</span>, <span class="hljs-string">'i'</span>, <span class="hljs-string">'s'</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'s'</span>, <span class="hljs-string">'d'</span>, <span class="hljs-string">'6'</span>, <span class="hljs-string">'7'</span>, <span class="hljs-string">'8'</span>, <span class="hljs-string">'9'</span>, <span class="hljs-string">'0'</span>, <span class="hljs-string">'7'</span>]</code></pre></div><p>也可以查找<strong>自己定义的字符集</strong>：</p><div class="hljs"><pre><code class="hljs Python">print(re.findall(<span class="hljs-string">r'[U2-6]'</span>, text))  <span class="hljs-comment"># 数字可以是一个范围</span>[<span class="hljs-string">'U'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'3'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'6'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'3'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'6'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'3'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'6'</span>, <span class="hljs-string">'6'</span>]</code></pre></div><p><code>+</code>表示其之前的字符可以<strong>重复</strong>任意次，<code>?</code>表示重复0或1次，<code>*</code>表示重复0或多次，<code>{}</code>可以规定具体次数：</p><div class="hljs"><pre><code class="hljs Python">print(re.findall(<span class="hljs-string">r'\d+4'</span>, text))  <span class="hljs-comment"># 任意个数字，但要以4结尾</span>[<span class="hljs-string">'44'</span>, <span class="hljs-string">'1234'</span>, <span class="hljs-string">'1234'</span>, <span class="hljs-string">'44'</span>, <span class="hljs-string">'1234'</span>]print(re.findall(<span class="hljs-string">r'\d*'</span>, text))[<span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'445'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'123456'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'123456'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'445'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'0111'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'1234567'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'678907'</span>, <span class="hljs-string">''</span>]print(re.findall(<span class="hljs-string">r'\d&#123;4,&#125;'</span>, text))  <span class="hljs-comment"># 长度大于等于4的数字，用,分割最大最小值</span>[<span class="hljs-string">'123456'</span>, <span class="hljs-string">'123456'</span>, <span class="hljs-string">'0111'</span>, <span class="hljs-string">'1234567'</span>, <span class="hljs-string">'678907'</span>]</code></pre></div><p><code>|</code>表示<strong>或</strong>，其前后的模式只要匹配任意一种则计入：<br><div class="hljs"><pre><code class="hljs Python">print(re.findall(<span class="hljs-string">r'\d&#123;4&#125;-\d&#123;7&#125;|\w&#123;3&#125;-\d&#123;6&#125;'</span>, text))[<span class="hljs-string">'0111-1234567'</span>, <span class="hljs-string">'asd-678907'</span>]</code></pre></div></p><p>正则表达式中有以下表示<strong>位置</strong>的方法：</p><script type="math/tex; mode=display">| 锚 | 说明 || :-: | :-: || ^ | 文本开头 || $ | 文本结尾 || \b | 单词边界，即\w与\W之间的位置，也包括\w与开头和结尾之间的位置 || \B | \b的反面 || (?=p) | p模式之前的位置 || (?!p) | p模式之后的位置 |</script><div class="hljs"><pre><code class="hljs Python">text = <span class="hljs-string">'abcd1234 efgh5678 ijkl9012'</span>print(re.findall(<span class="hljs-string">r'^\w&#123;4&#125;\d&#123;4&#125;|\w&#123;4&#125;\d&#123;4&#125;$'</span>, text))  <span class="hljs-comment"># 4字母+4数字的模式，但要求在开头或结尾</span>[<span class="hljs-string">'abcd1234'</span>, <span class="hljs-string">'ijkl9012'</span>]text = <span class="hljs-string">'ab cd ef'</span>print(re.sub(<span class="hljs-string">r'\b'</span>, <span class="hljs-string">'!'</span>, text, count=<span class="hljs-number">3</span>))  <span class="hljs-comment"># 在所有\w与\W之间的位置加入!，最多替换3次（默认0为全部替换）</span>!ab! !cd eftext = <span class="hljs-string">'ab12 ab+2 abcd ac34'</span>print(re.sub(<span class="hljs-string">r'\w(?=\d&#123;2&#125;)'</span>, <span class="hljs-string">'!'</span>, text))  <span class="hljs-comment"># 将所有2数字之前的字符替换为!</span>a!<span class="hljs-number">12</span> ab+<span class="hljs-number">2</span> abcd a!<span class="hljs-number">34</span></code></pre></div>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>正则表达式</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SeqGAN：使用GAN进行文本生成</title>
    <link href="/2020/SeqGAN/"/>
    <url>/2020/SeqGAN/</url>
    
    <content type="html"><![CDATA[<h1 id="SeqGAN：使用GAN进行文本生成"><a href="#SeqGAN：使用GAN进行文本生成" class="headerlink" title="SeqGAN：使用GAN进行文本生成"></a>SeqGAN：使用GAN进行文本生成</h1><blockquote><p>论文：<a href="https://arxiv.org/abs/1609.05473" target="_blank" rel="noopener">SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</a><br>源码：<a href="https://github.com/LantaoYu/SeqGAN" target="_blank" rel="noopener">https://github.com/LantaoYu/SeqGAN</a></p></blockquote><p>用对抗网络实现了离散序列数据的生成模型。解决了对抗生成网络难应用于nlp领域的问题，并且在文本生成任务上有优异表现。</p><h1 id="1-前置知识"><a href="#1-前置知识" class="headerlink" title="1 前置知识"></a>1 前置知识</h1><p>简单先记下几个相关的基本概念，将来单独学习。</p><h2 id="1-1-GAN（Generative-Adversarial-Network）：生成对抗网络"><a href="#1-1-GAN（Generative-Adversarial-Network）：生成对抗网络" class="headerlink" title="1.1 GAN（Generative Adversarial Network）：生成对抗网络"></a>1.1 GAN（Generative Adversarial Network）：生成对抗网络</h2><p>GAN的主要结构包括一个生成器G（Generator）和一个判别器D（Discriminator）。</p><p>GAN的一般训练过程为：</p><ul><li>分别从真实样本、噪声样本、生成样本（由暂时固定的G生成，G的输入一般为满足固定分布的随机向量）中采样一定数量的样本，由D进行判别，训练D使其尽可能好地分辨真实样本和生成样本。</li><li>D训练后，训练G使其生成的样本与真实样本差距尽可能地小（即让D判别错误）。</li><li>循环迭代上面两步，最终的理想状态是使D无法分辨生成样本与真实样本。</li></ul><blockquote><p>这种训练过程是G与D两个网络的博弈，实际训练一般持续到G和D达到纳什均衡，即双方为了自身利益的最大化，没有任何一方愿意单独地改变自身策略，反映到模型即两个网络均不再调整参数，各自达到收敛。</p></blockquote><h2 id="1-2-Reinforcement-Learning：强化学习"><a href="#1-2-Reinforcement-Learning：强化学习" class="headerlink" title="1.2 Reinforcement Learning：强化学习"></a>1.2 Reinforcement Learning：强化学习</h2><p>强化学习的过程：</p><ul><li><strong>主体Agent</strong>在与根据<strong>环境State</strong>采取<strong>行为Action</strong>。换言之，Agent是一个网络，输入State，输出Action。</li><li><strong>State</strong>因<strong>Action</strong>的影响发生变化，通过变化计算<strong>回报值Reward</strong>，Agent通过回报值调整自己的行为以期获得更大的Reward。</li><li>循环迭代上面两步，目的是使Reward尽可能地大。</li></ul><h2 id="1-3-Policy-Gradient"><a href="#1-3-Policy-Gradient" class="headerlink" title="1.3 Policy Gradient"></a>1.3 Policy Gradient</h2><p>强化学习中常用的一种优化方法。</p><h1 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="2 模型结构"></a>2 模型结构</h1><p><img src="https://b1.sbimg.org/file/chevereto-jia/2020/12/03/Q6I4T.png" srcset="/img/loading.gif" alt="SeqGAN"></p><h2 id="2-1-训练过程"><a href="#2-1-训练过程" class="headerlink" title="2.1 训练过程"></a>2.1 训练过程</h2><p><img src="https://b1.sbimg.org/file/chevereto-jia/2020/12/03/Q94Ko.png" srcset="/img/loading.gif" alt="Training"></p><p>总结如下：</p><ul><li>随机初始化G和D。</li><li>使用MLE（最大似然生成训练模型）预训练G，目的是提高搜索效率。</li><li>通过G生成部分负样本预训练D。</li><li>通过G生成样本，使用D评判得到Reward，计算每个Action得到的奖励和累计奖励的期望求导更新网络。</li><li>训练至收敛，目标是最大化识别真实样本的概率，最小化误识别伪造样本的概率。</li></ul><h2 id="2-2-生成器G"><a href="#2-2-生成器G" class="headerlink" title="2.2 生成器G"></a>2.2 生成器G</h2><p>之所以采用强化学习的方法优化G网络，是因为与数值数据、图像数据不同，G网络的输出为文字结果，无法回传数值误差更新参数。因此将<strong>生成数据的置信度</strong>作为强化学习的Reward来更新G网络。</p>]]></content>
    
    
    <categories>
      
      <category>对话系统学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>自然语言处理</tag>
      
      <tag>Python</tag>
      
      <tag>对话系统</tag>
      
      <tag>Seq2Seq</tag>
      
      <tag>对抗生成网络（Generative Adversarial Network）</tag>
      
      <tag>强化学习（Reinforcement Learning）</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Auto-Encoder Matching Model：学习语义依赖性的模型</title>
    <link href="/2020/AMM/"/>
    <url>/2020/AMM/</url>
    
    <content type="html"><![CDATA[<blockquote><p>论文：<a href="https://arxiv.org/abs/1808.08795" target="_blank" rel="noopener">An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation</a><br>代码：<a href="https://github.com/lancopku/AMM" target="_blank" rel="noopener">https://github.com/lancopku/AMM</a></p></blockquote><p>这篇论文着眼于在对话系统中生成语义一致的应答，这需要模型学习<strong>话语级别</strong>的语义依赖。文章提出了一种自动编码器匹配模型（Auto-Encoder Matching Model），包含两个自动编码器和一个映射模块。</p><h2 id="1-模型结构"><a href="#1-模型结构" class="headerlink" title="1 模型结构"></a>1 模型结构</h2><p><img src="https://b1.sbimg.org/file/chevereto-jia/2020/12/01/Qozid.png" srcset="/img/loading.gif" alt="AEM"></p><p>如图所示，与传统的seq2seq模型相比，这个模型中的encoder和decoder都是独立的auto-encoder，分别用于学习输入和回应（目标）的内部表示，通过一个映射模块连接，<strong>学习输入和目标的自编码的映射关系</strong>。</p><h3 id="1-1-encoder"><a href="#1-1-encoder" class="headerlink" title="1.1 encoder"></a>1.1 encoder</h3><p>encoder模块是一个基于LSTM的无监督auto-encoder，实质就是LSTM seq2seq模型，将其encoder和decoder称作source-encoder和source-decoder（套娃）。它将输入句子$x$编码为$h$，再解码为$\tilde{x}$。取隐藏层$h$作为输入的语义表征，输出$\tilde{x}$为输入句的重构建。</p><h3 id="1-2-mapping-module"><a href="#1-2-mapping-module" class="headerlink" title="1.2 mapping module"></a>1.2 mapping module</h3><p>映射模块是一个多层感知机，将$h$映射为$t$，传给decoder。</p><h3 id="1-3-decoder"><a href="#1-3-decoder" class="headerlink" title="1.3 decoder"></a>1.3 decoder</h3><p>与encoder类似，将目标句子$y$编码为语义表征$s$，再解码为文本$\tilde{y}$。</p><h3 id="1-4-训练流程"><a href="#1-4-训练流程" class="headerlink" title="1.4 训练流程"></a>1.4 训练流程</h3><p>训练的方法是将输入和目标分别进行自编码，再使用一个前馈网络进行匹配。训练的loss为encoder、deocder、$s$与$t$的匹配、$x$到$y$的映射，共计4个loss的线性组合：</p><script type="math/tex; mode=display">\begin{aligned}& J_1(\theta)=-\log{P(\tilde{x}\,|\,x;\theta)} \\& J_2(\phi)=-\log{P(\tilde{y}\,|\,y;\phi)} \\& J_3(\gamma)=\frac{1}{2}\left \| t-s \right \|_2^2 \\& J_4(\theta,\phi,\gamma)=-\log{P(y\,|\,x;\theta,\phi,\gamma)}=-\sum_{t=1}^{T}\log{P(y_t\,|\,x,y_{1,...,t-1};\theta,\phi,\gamma)} \\\end{aligned}</script><h2 id="2-使用记录"><a href="#2-使用记录" class="headerlink" title="2 使用记录"></a>2 使用记录</h2><ul><li><code>UnicodeDecodeError: &#39;gbk&#39; codec can&#39;t decode byte 0x99 in position 145: illegal multibyte sequence</code>：给<code>open()</code>函数（主要在./process/process.py里）中添加参数<code>encoding=&#39;utf-8&#39;</code>。</li><li><code>FileNotFoundError: [Errno 2] No such file or directory: &#39;data/processed\\daily\\test_samples.txt&#39;</code>：按路径添加一个空txt即可。</li><li><code>AttributeError: &#39;Bootstrap&#39; object has no attribute &#39;_Bootstrap__model&#39;</code>：mlbootstrap库版本降至0.0.3。</li></ul>]]></content>
    
    
    <categories>
      
      <category>对话系统学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>自然语言处理</tag>
      
      <tag>Python</tag>
      
      <tag>对话系统</tag>
      
      <tag>Seq2Seq</tag>
      
      <tag>Auto-Encoder</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[CS224N]2. Named Entity Recognition</title>
    <link href="/2020/CS224N-2-NER/"/>
    <url>/2020/CS224N-2-NER/</url>
    
    <content type="html"><![CDATA[<blockquote><p>此篇对应<a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/" target="_blank" rel="noopener">CS2224n Winter 2019</a>的第三课（Word Window Classification, Neural Networks, and Matrix Calculus）后半，神经网络部分就跳过了。<br>课程视频：<a href="https://youtu.be/8CWyBNX6eDo" target="_blank" rel="noopener">1</a><br>课程笔记：<a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/readings/cs224n-2019-notes03-neuralnets.pdf" target="_blank" rel="noopener">1</a><br>作业：<a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a2.zip" target="_blank" rel="noopener">Assignment 2</a></p></blockquote><p><strong>命名实体识别</strong>（NER，Named Entity Recognition）是NLP中一项基础但重要的工作，目的是从非结构化文本中识别出“实体”，包括人名、组织、地名等。</p><p>NER从早期基于规则、字典匹配的方法开始，逐渐发展出了基于传统机器学习的方法（HMM、MEMM、CRF等）、基于深度学习的方法（RNN-CRF、CNN-CRF等），近年来新出现的注意力模型、迁移学习等方法也在NER上得到了应用。</p>]]></content>
    
    
    <categories>
      
      <category>CS224N</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>自然语言处理</tag>
      
      <tag>Python</tag>
      
      <tag>命名实体识别</tag>
      
      <tag>Named Entity Recognition(NER)</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[CS224N]1. Word Vectors</title>
    <link href="/2020/CS224N-1-WordVectors/"/>
    <url>/2020/CS224N-1-WordVectors/</url>
    
    <content type="html"><![CDATA[<blockquote><p>此篇对应<a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/" target="_blank" rel="noopener">CS2224n Winter 2019</a>的前两课（Introduction and Word Vectors、Word Vectors 2 and Word Senses）。<br>课程视频：<a href="https://youtu.be/8rXD5-xhemo" target="_blank" rel="noopener">1</a>，<a href="https://youtu.be/kEMJRjEdNzM" target="_blank" rel="noopener">2</a><br>课程笔记：<a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/readings/cs224n-2019-notes01-wordvecs1.pdf" target="_blank" rel="noopener">1</a><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/readings/cs224n-2019-notes02-wordvecs2.pdf" target="_blank" rel="noopener">2</a><br>作业：<a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a1.zip" target="_blank" rel="noopener">Assignment 1</a></p></blockquote><p>在使用计算机处理自然语言时，我们需要将词或字以数字化的形式表示出来。最简单的想法是使用<strong>One-Hot编码</strong>，即对有n个词的语料库，每个词表示为一个n维向量，在其中对应一个独一无二的位置，使该位置为1，其余位置为0。这样就可以将每个词编码。</p><p>但是，这种方法存在着显而易见的问题：</p><ul><li>向量过大，每个词是一个n维向量</li><li>每个词的对应的向量相互正交，无法直接表示词之间的关联、相似度</li></ul><p>因此，需要找到方法，既能缩短每个词的表示，又能体现词之间的关联。</p><h1 id="1-基于SVD的数据降维"><a href="#1-基于SVD的数据降维" class="headerlink" title="1 基于SVD的数据降维"></a>1 基于SVD的数据降维</h1><p>要表示词之间的关联，自然需要考虑相关度较高的词在文章中的表现。在这种方法中，我们假设相关度较高的词通常出现在<strong>同一段文本</strong>中，且<strong>位置较为接近</strong>。</p><p>有了思路，就可以统计词之间的相关度信息了。我们使用一个固定大小的滑动窗口扫描语料库中的所有句子。在每个时刻，对窗口的中心词，统计其两侧处于窗口中的词。完成后，基于统计结果得到词库的共现矩阵（Co-occurrence Matrix）$X$。这里以一个只有三句话的语料库，使用长度为3的窗口进行统计：<br><img src="https://b1.sbimg.org/file/chevereto-jia/2020/10/16/GN0pk.png" srcset="/img/loading.gif" alt="共现矩阵"></p><p>然后对$X$进行奇异值分解，得到$X=USV^T$:<br><img src="https://b1.sbimg.org/file/chevereto-jia/2020/10/16/GNXPD.png" srcset="/img/loading.gif" alt="SVD"></p><p>选择前$k$个奇异值，作为降维后的词向量：<br><img src="https://b1.sbimg.org/file/chevereto-jia/2020/10/16/GNmuN.png" srcset="/img/loading.gif" alt="降维"></p><p>这样。我们可以将一个任意大的语料库降维到我们需要的维度。但是基于SVD的降维方法存在着诸多问题：</p><ul><li>对新词的处理能力弱：<ul><li>无法正确计算新词的词向量</li><li>如果想要在语料库中加入新词，会改变整个矩阵的尺寸和内部信息</li></ul></li><li>计算量大：<ul><li>一般的语料库词汇量很大，而共现矩阵的尺寸是词汇量的平方</li><li>不平衡现象严重，当有很多词出现次数较少时，矩阵会非常稀疏</li><li>SVD的计算复杂度高</li></ul></li></ul><p>虽然有很多方法被提出来改善这些问题，但下一节基于深度学习的方法能够更好地完成这个任务。</p><h1 id="2-基于深度学习的数据降维：Word2Vec"><a href="#2-基于深度学习的数据降维：Word2Vec" class="headerlink" title="2 基于深度学习的数据降维：Word2Vec"></a>2 基于深度学习的数据降维：Word2Vec</h1><p>相比于SVD方法中使用一个矩阵存储全局信息，深度学习能够通过多次迭代计算优化参数。如果将词向量作为神经网络的参数，使用合适的损失函数对其进行优化，也能得到降维的词向量。</p><h2 id="2-1-语言模型"><a href="#2-1-语言模型" class="headerlink" title="2.1 语言模型"></a>2.1 语言模型</h2><p><strong>语言模型</strong>指给定一个句子$s={w_1,w_2,\cdots,w_n}$，预测其出现概率$p(s)$的数学模型。如果通过传统的统计方法，通过每个词在对应位置出现的概率计算它，会出现数据稀疏、计算代价大等各种问题：</p><script type="math/tex; mode=display">p(s) = p(w_1, w_2, \cdots, w_n)</script><p>因此，我们使用<strong>马尔可夫假设</strong>简化语言模型。</p><blockquote><p>马尔可夫假设：时间线上的一系列事件顺序发生，每个事件的发生概率与其未来的事件无关。<br>马尔可夫链：时间和状态均为离散的马尔可夫过程。</p></blockquote><p>在语言模型中，可以认为第t个事件指句子中第t个词出现的概率，则按马尔可夫假设：</p><script type="math/tex; mode=display">\begin{aligned}p(s) &= p(w_1, w_2, \cdots, w_n)\\ &= \prod_{t=1}^{n}p(w_t|w_1,w_2,\cdots,w_{t-1})\end{aligned}</script><p>如果认为每个词都是独立分布的，那这种语言模型就称作<strong>一元语法</strong>；如果假设每个单词的出现概率只与其前面一个单词相关，就能得到<strong>二元语法</strong>。类似地，也可以定义n元语法。</p><h2 id="2-2-Word2Vec中的概率算法模型"><a href="#2-2-Word2Vec中的概率算法模型" class="headerlink" title="2.2 Word2Vec中的概率算法模型"></a>2.2 Word2Vec中的概率算法模型</h2><p>Word2Vec中包含2种算法模型：CBOW和skip-gram。</p><h3 id="2-2-1-CBOW算法"><a href="#2-2-1-CBOW算法" class="headerlink" title="2.2.1 CBOW算法"></a>2.2.1 CBOW算法</h3><p>CBOW（Continuous Bag of Words）：输入上下文，预测中心词。</p><p>首先定义过程中用到的符号：</p><script type="math/tex; mode=display">\begin{aligned}V&:语料库的词典，其大小为|V|。其中第i个词记作w_i。\\n&:自定义的嵌入词向量的长度。\\\mathcal{V}&:输入矩阵，大小为n\times |V|。第i列为v^{(i)}，对应词w_i。\\\mathcal{U}&:输出矩阵，大小为|V|\times n。第i行记作u_i，对应词w_i。\end{aligned}</script><p>CBOW的处理过程如下（方便理解，注明了每步运算涉及变量的尺寸）：</p><ol><li><p>构造输入<br>CBOW的输入是预测目标词的上下文。假设模型输入中心词前m、后m个词，则选择词并以One-Hot表示：$ x^{(c-m)},\cdots,x^{(c-1)},x^{(c+1)},\cdots,x^{(c+m)} $，再输入模型。<br>共输入2m个向量，每个向量$ x^i \in \mathbb{R}^{|V|} $。</p></li><li><p>计算嵌入词向量<br>$v^{(c-m)}=\mathcal{V}x^{(c-m)},\cdots,v^{(c-1)}=\mathcal{V}x^{(c-1)},v^{(c+1)}=\mathcal{V}x^{(c+1)},\cdots,v^{(c+m)}=\mathcal{V}x^{(c+m)}$。$\mathcal{V} \in \mathbb{R}^{n \times |V|}, v^i \in \mathbb{R}^{n}$</p></li><li><p>计算词向量平均值<br>$\hat{v}=\frac{\sum_{j=c-m,j\neq c}^{c+m} v}{2m}$。</p></li><li><p>计算评价向量<br>$z=\mathcal{U}\hat{v}$。$\mathcal{U} \in \mathbb{|V|}^{n}$, $z \in \mathbb{R}^{|V|}$。</p></li><li><p>计算输出<br>$\hat{y}=softmax(z)$。这样得到的是各词的概率（用于计算损失），将其进行转换为One-Hot表示（用于最终输出），即概率最高处为1，其余位置为0，就得到了输出词。</p></li></ol><p>在这个过程中，我们已知的是每个词的位置和其One-Hot表示，而训练对像是$\mathcal{V}、\mathcal{U}$两个矩阵。所以实际上，我们对每个词$w_i$训练了2个词向量：$v^{(i)}$是其作为上下文时的词向量，$u_i$是其作为中心词的词向量。</p><p>从模型结构看，2、3相当于输入层（输入为上下文One-Hot向量，参数为$\mathcal{V}$，输出为$\hat{v}$），4是隐藏层（输入为$\hat{v}$，参数为$\mathcal{U}$，输出为$z$），5是输出层（输入为$z$，输出$\hat{y}$）。</p><p>所以CBOW的原理可以概括如下：计算上下文的平均词向量$\hat{v}$，与中心词向量矩阵$\mathcal{U}$相乘，结果$z$和其softmax: $\hat{y}$中的最大值即对应预测出的中心词（换句话说，$\mathcal{U}$中的词与$\hat{v}$约接近，乘积越大）。优化$\mathcal{V}$和$\mathcal{U}$，就能得到更准确的预测结果。</p><p>要进行优化，就需要评价结果准确性的手段。CBOW中使用了<strong>Cross-Entropy</strong>作为损失函数：</p><script type="math/tex; mode=display">H(\hat(y),y) = -\sum_{j=1}^{|V|}y_j \log{y_j}</script><p>由于词$y_j$是One-Hot向量，损失函数可以简化为：</p><script type="math/tex; mode=display">H(\hat(y),y) = -y_i \log{y_i}</script><p>有了对每次输出的评价方式，就可以导出整体的优化目标：</p><script type="math/tex; mode=display">\begin{aligned}minimize\:  J &= -\log P(w_c \: | \: w_{c-m},\cdots,w_{c-1},w_{c+1},\cdots,w_{c+m}) \\&=-\log P(u_c \: | \: \hat{v}) \\&=-\log \frac{\exp{u_c^T\hat{v}}}{\sum_{j=1}^{|V|}\exp{u_j^T\hat{v}}} \\&=-u_c^T\hat{v} + \log\sum_{j=1}^{|V|}\exp{u_j^T\hat{v}}\end{aligned}</script><h3 id="2-2-2-Skip-Gram算法"><a href="#2-2-2-Skip-Gram算法" class="headerlink" title="2.2.2 Skip-Gram算法"></a>2.2.2 Skip-Gram算法</h3><p>Skip-Gram：输入中心词，输出上下文。</p><p>Skip-Gram的计算过程与CBOW基本相同，可参考上一节，输入变为中心词向量，输出变为2m个上下文词向量，与CBOW正相反。<br><em>注：“上下文”一词并非原笔记中context的翻译。context翻译为“语境”更合适，在CBOW中上下文词是context，而Skip-Gram中中心词就是context。</em></p><p>评价指标也与CBOW相同，但存在一个问题：这里损失函数中的概率$P$是上下文2m个词的联合概率分布，无法直接计算。因此Skip-Gram中使用了<em>朴素贝叶斯假设</em>，即给定中心词的情况下，假设其上下文词均独立分布。这样，就可以将联合分布概率分解为连乘的形式：</p><script type="math/tex; mode=display">\begin{aligned}minimize\:  J &= -\log P(w_{c-m},\cdots,w_{c-1},w_{c+1},\cdots,w_{c+m}\: |\: w_c) \\&=-\log \prod_{j=0,j \neq m}^{2m}P(w_{c-m+j} \: | \: w_c) \\&=-\log \prod_{j=0,j \neq m}^{2m}P(u_{c-m+j} \: | \: v_c) \\&=-\log \prod_{j=0,j \neq m}^{2m} \frac{\exp{(u_{c-m+j}^Tv_c)}}{\sum_{k=1}^{|V|}\exp{(u_k^Tv_c)}} \\&=-\sum_{j=0,j \neq m}^{2m}(u_{c-m+j}^Tv_c) + 2m \log \sum_{k=1}^{|V|} \exp{(u_k^Tv_c)}\end{aligned}</script><h2 id="2-3-Word2Vec中的模型训练方法"><a href="#2-3-Word2Vec中的模型训练方法" class="headerlink" title="2.3 Word2Vec中的模型训练方法"></a>2.3 Word2Vec中的模型训练方法</h2><p>无论是CBOW还是Skip-Gram，在计算梯度时都需要对整个参数矩阵求导，但只更新少量权重，且使用的损失函数$J$中包含了一个有$|V|$项的求和(softmax的分母)，这在实际计算中每次调用都可能会有数百万甚至更多的计算量。因此，需要找到方法，以较低的计算量得到损失和梯度。</p><h3 id="2-3-1-Negative-Sampling"><a href="#2-3-1-Negative-Sampling" class="headerlink" title="2.3.1 Negative Sampling"></a>2.3.1 Negative Sampling</h3><p>负采样的主要思想是只取目标词、上下文词（即窗口内的词）和$K$个的随机采样词（噪声）计算损失函数，以近似上文中的完整损失函数。</p><p>同时，采用sigmoid而非softmax，以避免每次都对所有词进行一次计算。</p><p>CBOW算法中使用负采样的目标函数与原目标函数对比：</p><script type="math/tex; mode=display">regular\; softmax\; loss:\; minimize\;  J=-u_c^T\hat{v} + \log\sum_{j=1}^{|V|}\exp{u_j^T\hat{v}}</script><script type="math/tex; mode=display">negative\; sampling\; loss:\; minimize\;  J=-\log \sigma (u_c^T\hat{v}) - \log\sum_{k=1}^{K}\sigma({-\tilde{u}_j^T\hat{v}})</script><p>skip-gram算法中使用负采样的目标函数与原目标函数对比：</p><script type="math/tex; mode=display">regular\; softmax\; loss:\; minimize\;  J=-\sum_{j=0,j \neq m}^{2m}(u_{c-m+j}^Tv_c) + 2m \log \sum_{k=1}^{|V|} \exp{(u_k^Tv_c)}</script><script type="math/tex; mode=display">negative\; sampling\; loss:\; minimize\;  J=-\sum_{j=0,j \neq m}^{2m}\log \sigma (u_{c-m+j}^Tv_c) - 2m \sum_{k=1}^{K} \log \sigma{(-\tilde{u}_k^Tv_c)}</script><h1 id="3-练习与作业"><a href="#3-练习与作业" class="headerlink" title="3 练习与作业"></a>3 练习与作业</h1><p>之后每节都会将作业附在最后，会有完成的版本，并且注明可能出现的一些坑。</p><h3 id="3-1-Gensim"><a href="#3-1-Gensim" class="headerlink" title="3.1 Gensim"></a>3.1 Gensim</h3><p>第一课提供了Gensim的<a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/materials/Gensim.zip" target="_blank" rel="noopener">使用实例</a>，添加注释的版本：<a href="https://github.com/LiShaoyu5/CS224N-Learning/blob/main/1%20-%20Introduction%20and%20Word%20Vectors/Gensim/Gensim%20word%20vector%20visualization.ipynb" target="_blank" rel="noopener">注释</a>。</p><h3 id="3-2-Homework"><a href="#3-2-Homework" class="headerlink" title="3.2 Homework"></a>3.2 Homework</h3><p>第一课的<a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a1.zip" target="_blank" rel="noopener">作业</a>，完成后的版本：<a href="https://github.com/LiShaoyu5/CS224N-Learning/blob/main/1%20-%20Introduction%20and%20Word%20Vectors/Homework/exploring_word_vectors.ipynb" target="_blank" rel="noopener">notebook</a>。</p>]]></content>
    
    
    <categories>
      
      <category>CS224N</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>自然语言处理</tag>
      
      <tag>Python</tag>
      
      <tag>词向量</tag>
      
      <tag>Word Vectors</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>在Python中显示进度条</title>
    <link href="/2020/Show-Progress-Bar-in-Python/"/>
    <url>/2020/Show-Progress-Bar-in-Python/</url>
    
    <content type="html"><![CDATA[<p>在Python的使用中，有时需要处理大量的数据或执行很长的遍历、循环。这时候如果有一个可视化的进度条，能够更直观地表示程序的执行进度。使用Python中的tqdm库可以方便地实现这一功能，这里介绍tqdm的基本用法。</p><h3 id="可迭代对象"><a href="#可迭代对象" class="headerlink" title="可迭代对象"></a>可迭代对象</h3><p>对于列表、字符串等可迭代对象，可以直接在原有代码上添加tqdm实现遍历：</p><p><img src="https://wx2.sbimg.cn/2020/09/10/9q3ud.png" srcset="/img/loading.gif" alt="iterable"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> sleep<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(range(<span class="hljs-number">10</span>)):    sleep(<span class="hljs-number">0.5</span>)<span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> tqdm(<span class="hljs-string">'abcdefg'</span>):    sleep(<span class="hljs-number">0.5</span>)</code></pre></div><h3 id="自定义进度条"><a href="#自定义进度条" class="headerlink" title="自定义进度条"></a>自定义进度条</h3><p>更自由的方式是自定义一个进度条对象，根据自己的需要进行更新：</p><p><img src="https://wx2.sbimg.cn/2020/09/10/9q684.png" srcset="/img/loading.gif" alt="manual"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<span class="hljs-keyword">import</span> sleep<span class="hljs-keyword">with</span> tqdm(total=<span class="hljs-number">1000</span>) <span class="hljs-keyword">as</span> pbar:  <span class="hljs-comment"># 自定义的进度条对象，范围为0-100</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1000</span>):sleep(<span class="hljs-number">0.1</span>)pbar.update(<span class="hljs-number">1</span>)  <span class="hljs-comment"># 自定义更新的数值</span></code></pre></div>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>实用技巧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从Google Bigquery上下载公开数据集</title>
    <link href="/2020/Download-Table-on-Google-Bigquery/"/>
    <url>/2020/Download-Table-on-Google-Bigquery/</url>
    
    <content type="html"><![CDATA[<p>Google BigQuery平台上有大量的公开数据集，但是没有提供直接的下载选项。另一方面，虽然可以导出到Google Cloud Storage中再下载，但又存在单个文件过大难以导出、平台没有批量下载功能等很多问题。这里整理一份完整的下载流程。</p><p>执行以下操作的前提包括科学上网、激活Google Cloud账号（需要信用卡信息）等，这里不再赘述。</p><h3 id="在Google-BigQuery上搜索数据集"><a href="#在Google-BigQuery上搜索数据集" class="headerlink" title="在Google BigQuery上搜索数据集"></a>在Google BigQuery上搜索数据集</h3><p>登录<a href="https://console.cloud.google.com/bigquery" target="_blank" rel="noopener">Google BigQuery</a>，左侧界面如下：</p><p><img src="https://wx1.sbimg.cn/2020/09/09/9R9PV.png" srcset="/img/loading.gif" alt="public_data"></p><p>可以看到，下方的资源栏中有bigquery-public-data，这就是BigQuery上提供的公开数据集，展开后可以看到大量数据集。本文以etherem_blockchain数据集中的contract表为例进行下载。</p><h3 id="导出表到Google-Cloud-Storage"><a href="#导出表到Google-Cloud-Storage" class="headerlink" title="导出表到Google Cloud Storage"></a>导出表到Google Cloud Storage</h3><p>选择contract表，在右侧的信息栏中点击详细信息，可以看到表的相关信息：</p><p><img src="https://wx2.sbimg.cn/2020/09/09/9RG17.png" srcset="/img/loading.gif" alt="table_info"></p><p>点击右侧的导出，在弹出栏中选择导出到GCS，会弹出导出界面：</p><p><img src="https://wx1.sbimg.cn/2020/09/09/9RQE6.png" srcset="/img/loading.gif" alt="output1"></p><p>首先需要选择目标位置，点击浏览后如果显示没有权限，需要先在网站左上角导航栏中选择一个自己的项目：</p><p><img src="https://wx2.sbimg.cn/2020/09/09/9RcvD.png" srcset="/img/loading.gif" alt="output2"></p><p>重新进入分区选择界面，如果没有分区需要新建一个。新建时有个小坑，如果你了解自己使用的代理的各种信息（主机、端口等）或是不需要科学上网，在“选择如何控制对对象的访问权限”一步可以选择<strong>精细控制</strong>（这是正常的方式，能够更精确地管理数据的存取权限）；如果不清楚这些信息，这一步应该选择<strong>统一</strong>，方便之后下载时的设置：</p><p><img src="https://wx2.sbimg.cn/2020/09/09/9RRhe.png" srcset="/img/loading.gif" alt="output3"></p><p>分区选择完成后需要指定导出的文件名。这个表有21G，无法直接导出到GCS（仅支持1G以下的文件）。但是可以通过在名称中添加*将其导出为多个文件，导出结果后面可以看到：</p><p><img src="https://wx1.sbimg.cn/2020/09/09/9RdfN.png" srcset="/img/loading.gif" alt="output4"></p><p>完成后，点击导出，等待一段时间后在<a href="https://console.cloud.google.com/storage/" target="_blank" rel="noopener">GCS</a>中进入相应的分区即可看到文件（图中是我之前导出的，所以没有扩展名）。通过这种方式导出的分段文件大小不一，数量很多：</p><p><img src="https://wx2.sbimg.cn/2020/09/09/9RhAj.png" srcset="/img/loading.gif" alt="gcs"></p><h3 id="批量下载与后续处理"><a href="#批量下载与后续处理" class="headerlink" title="批量下载与后续处理"></a>批量下载与后续处理</h3><p>GCS的网页UI上没有提供批量下载的方式，需要使用命令行工具gsutil。在 <a href="https://cloud.google.com/storage/docs/gsutil_install#install" target="_blank" rel="noopener">https://cloud.google.com/storage/docs/gsutil_install#install</a> 上根据自己的系统选择合适的安装包下载并安装。</p><h4 id="设置分区公开"><a href="#设置分区公开" class="headerlink" title="设置分区公开"></a>设置分区公开</h4><p>由于我使用的代理有些问题，在gsutil中无法设置自己的账户信息，因此只能将之前的数据分区设置为公开才能下载。代理没有问题的朋友可以不看这部分，但是需要在gsutil中配置自己的帐户信息，才能获取下载的权限。</p><p>首先在GCS上自己的存储分区中选择“权限”：</p><p><img src="https://wx2.sbimg.cn/2020/09/09/9Rjoh.png" srcset="/img/loading.gif" alt="auth1"></p><p>点击下方的“添加”，会弹出添加成员的界面。在“新成员”栏中输入<strong>allUsers</strong>，下方的“角色”选择<strong>Cloud Storage中的Storage Object Viewer</strong>。这样，<strong>任何人</strong>都可以读取、下载这个分区的数据。</p><p><img src="https://wx2.sbimg.cn/2020/09/09/9RJSn.png" srcset="/img/loading.gif" alt="auth2"></p><h4 id="批量下载"><a href="#批量下载" class="headerlink" title="批量下载"></a>批量下载</h4><p>在分区页面点击“配置”，可以看到分区的信息。最下方的<strong>gsutil链接</strong>就是下载需要的，复制下来：</p><p><img src="https://wx1.sbimg.cn/2020/09/09/9RuqM.png" srcset="/img/loading.gif" alt="download1"></p><p>然后就可以在命令行中进行操作：<br><div class="hljs"><pre><code class="hljs profile"># 查看gs://lsydata2目录下的文件gsutil ls gs://lsydata2# 将gs://lsydata2中的文件下载到本地的E:\Download\lsydatagsutil -m cp -R gs://lsydata2 E:\Download\lsydata</code></pre></div></p><p>下载完成后，可以在本地看到下载完成的文件（之前导出时忘记添加扩展名，应当是JSON格式。如果导出时选择了压缩，可以添加.gz扩展名后解压）：</p><p><img src="https://wx2.sbimg.cn/2020/09/09/9R1Ea.png" srcset="/img/loading.gif" alt="download2"></p><h4 id="合并文件"><a href="#合并文件" class="headerlink" title="合并文件"></a>合并文件</h4><p>这一步其实很简单了，我提供个简单的代码段，修改一下文件名、路径和数量判断可以直接使用，运行时间根据文件数量和大小可能较长：</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> osfilelist = os.listdir(<span class="hljs-string">'E:/Download/lsydata/contracts'</span>)  <span class="hljs-comment"># 下载文件的目录</span><span class="hljs-keyword">with</span> open(<span class="hljs-string">'target.json'</span>, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> f1:  <span class="hljs-comment"># 合并后的文件名</span>    <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> filelist:        <span class="hljs-keyword">with</span> open(<span class="hljs-string">'E:/Download/lsydata/contracts/'</span> + file, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f2:            f1.write(f2.read())</code></pre></div><p>合并完成后：</p><p><img src="https://wx1.sbimg.cn/2020/09/09/9RwvK.png" srcset="/img/loading.gif" alt="cat"></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>实用技巧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[机器学习理论]贝叶斯分类器</title>
    <link href="/2020/ML-4-Bayes-Classifier/"/>
    <url>/2020/ML-4-Bayes-Classifier/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>贝叶斯决策论</tag>
      
      <tag>极大似然估计</tag>
      
      <tag>EM算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[机器学习理论]支持向量机</title>
    <link href="/2020/ML-3-Supported-Vector-Machine/"/>
    <url>/2020/ML-3-Supported-Vector-Machine/</url>
    
    <content type="html"><![CDATA[<h1 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h1><h2 id="1-SVM的原始模型"><a href="#1-SVM的原始模型" class="headerlink" title="1. SVM的原始模型"></a>1. SVM的原始模型</h2><p>在分类问题中，最基本的想法就是找到合适的超平面，将不同类别的样本划分来开。但是如下图所示（此图表示一个有2个特征的二分类问题），满足条件的超平面可能有很多，需要有一个标准评价找到的超平面的分类性能：</p><p><img src="https://wx2.sbimg.cn/2020/08/08/ohN17.png" srcset="/img/loading.gif" alt="超平面"></p><p>直观上看，上图中红色的超平面应当具有最好的泛化能力，因为实际的样本可能比训练集中的样本（即图中出现的点）更接近超平面，而离所有样本更“远”的超平面显然是最为鲁棒的。</p><p>超平面可以用一个线性方程表示：</p><script type="math/tex; mode=display">\mathbf{w}^T\mathbf{x}+b=0</script><p>其中，$\mathbf{w}=(w_1;w_2;…;w_d)$为法向量，决定了超平面的方向；$b$为位移项，决定了超平面与原点之间的距离。由于$\mathbf{w}$和$b$决定了一个超平面，所以可以讲该超平面记作$(\mathbf{w},b)$。</p><p>样本空间中任意点$\mathbf{x}$到超平面$(\mathbf{w},b)$的距离为：</p><script type="math/tex; mode=display">r=\frac{|\mathbf{w}^T\mathbf{x}+b|}{||\mathbf{w}||}</script><p>推导过程：<br><img src="https://wx1.sbimg.cn/2020/08/08/osrel.jpg" srcset="/img/loading.gif" alt="距离"></p><p>下面给出关于间隔的两个定义：</p><ul><li><p>函数间隔：定义超平面关于样本$(x_i,y_i)$的函数间隔为$\hat{\gamma}_i=\gamma_i(\mathbf{w}^T\mathbf{x}+b)$，其中$y_i\in{-1,+1}$；超平面关于数据集的样本间隔为$\hat{\gamma}=\underset{i=1,2,…,m}{min}\hat{\gamma}_i$。<br>  可以看出，当且仅当超平面能将所有样本正确分类时，所有$\hat{\gamma}_i&gt;0$恒成立且$\hat{\gamma}&gt;0$；当存在未正确分类的样本时，$\hat{\gamma}$是错误分类的样本中距离当前超平面最远的一个。<br>  函数间隔能够用于检验超平面能否对数据集进行完全正确的分类，但不能做到找到最优的超平面。</p></li><li><p>几何间隔：定义超平面关于样本$(x_i,y_i)$的几何间隔为$\gamma_i=\frac{\hat{\gamma}_i}{||w||}$；超平面关于数据集的集合间隔为$\gamma=\underset{i=1,2,…,m}{min}\gamma_i$。<br>  从函数间隔的公式可知，当分类完全正确时，$\gamma$为距离超平面最近的点与超平面的间距。</p></li></ul><p>通过以上的定义，就可以给出SVM的核心思想：求解一个与已知数据集<strong>几何间隔最大</strong>的超平面。即：</p><script type="math/tex; mode=display">\max \frac{\hat{\gamma}}{||w||} \\ s.t.\; y_i(\mathbf{w}^T\mathbf{x}+b) \geq \hat{\gamma}, i=1,2,\dots,m</script><h2 id="2-求解SVM"><a href="#2-求解SVM" class="headerlink" title="2. 求解SVM"></a>2. 求解SVM</h2><p>这个问题中的优化目标是$\mathbf{w}$和$b$。但是，对于超平面的方程来说，等比例放大$\mathbf{w}$和$b$不会改变超平面。换言之，上面的问题会有无数个解。</p><p>因此，为了保证能求得唯一解，可以对该问题添加一定的约束。利用上段提到的超平面方程的性质，令$\hat{\gamma}=1$：</p><script type="math/tex; mode=display">\max \frac{1}{||w||} \\ s.t.\; y_i(\mathbf{w}^T\mathbf{x}+b) \geq 1, i=1,2,\dots,m</script><p>按照优化问题的一般形式对问题进行变形：</p><script type="math/tex; mode=display">\underset{\mathbf{w},b}{\min} \frac{||w||^2}{2} \\ s.t.\; 1-y_i(\mathbf{w}^T\mathbf{x}+b) \leq 0, i=1,2,\dots,m</script><blockquote><p>能证明这是一个凸优化问题，容易求解。之后证明过程中使用拉格朗日对偶有以下两点原因：</p><ul><li>对偶问题也不难求解，且能引入核函数的概念。</li><li>对偶问题的求解复杂度和样本个数m成正比，主问题的求解复杂度和参数维度成正比。因此当样本个数远小于参数维度时使用拉格朗日对偶求解的效率更高。</li></ul></blockquote><p>由于拉格朗日对偶函数函数恒为凹函数，所以可以将它转换为一个凸优化问题来求解乘子；又因为当问题中不等式约束的乘子均不小于0时，拉格朗日对偶函数构成了优化问题最优值的下界，所以可以通过求解对偶函数的最大值来得到优化问题的最小值。</p><p>首先求解对偶问题的具体形式：<br><img src="https://wx1.sbimg.cn/2020/08/08/osOb1.jpg" srcset="/img/loading.gif" alt="求解-1"><br><img src="https://wx1.sbimg.cn/2020/08/08/osl6o.jpg" srcset="/img/loading.gif" alt="求解-2"></p><p>$\alpha$的求解，《机器学习》中使用了SMO方法，这里给出《统计学习方法（第二版）》中关于SMO方法的讲解：<br><img src="https://wx2.sbimg.cn/2020/08/08/oJ5V4.jpg" srcset="/img/loading.gif" alt="SMO"></p><p>解出$\alpha$后，求出$\mathbf{w}$和$b$即可得到模型：</p><script type="math/tex; mode=display">f(\mathbf{x})=\mathbf{w}^T\mathbf{x}+b=\sum_{i=1}^{m}\alpha_iy_i\mathbf{x}_i^T \mathbf{x}+b</script><h2 id="3-软间隔"><a href="#3-软间隔" class="headerlink" title="3. 软间隔"></a>3. 软间隔</h2><p>前面两节中所有的建模和推导都是建立在数据集中的正负样本<strong>线性可分</strong>（存在超平面可以将它们完美地划分开）的基础上。但是现实中大多数数据集是线性不可分的，或是难以确定这样的超平面，即使找到也可能是过拟合的。</p><p>因此，我们可以允许SVM在一定程度上出错，引入<strong>软间隔</strong>（允许某些样本不满足约束$y_i(\mathbf{w}^T\mathbf{x}+b) \geq 1$，与之相对的，上文中的间隔称作<strong>硬间隔</strong>）的概念，如图所示：<br><img src="https://wx2.sbimg.cn/2020/08/09/oLOh8.png" srcset="/img/loading.gif" alt="软间隔"></p><p>当然，在最大化间隔的同时，也应当让不满足约束的样本尽可能少。因此，优化目标可以改为：</p><script type="math/tex; mode=display">\underset{\mathbf{w},b}{\min} \frac{||w||^2}{2} + C\sum_{i=1}^m l\,(\,y_i(\mathbf{w}^T\mathbf{x}_i+b)-1)</script><p>其中，$C&gt;0$是一个常数；$l$是损失函数，软间隔中一般使用合页(hinge)损失：</p><script type="math/tex; mode=display">l_{hinge}(z)=\max(0,1-z)</script><p>与常用的0/1损失函数相比，合页损失函数是凸函数且连续，而且是0/1损失函数的上界。对于变量$z$，合页损失的惩罚程度取决于它比1小的程度，很适合用于这个问题。</p><p>将优化问题中的损失函数替换为合页损失函数：</p><script type="math/tex; mode=display">\underset{\mathbf{w},b}{\min} \frac{||w||^2}{2} + C\sum_{i=1}^m \max(0,\;\,1-y_i(\mathbf{w}^T\mathbf{x}_i+b))</script><p>令$\xi _i = \max(0,\;\,1-y_i(\mathbf{w}^T\mathbf{x}_i+b))$。可以看出，当样本分类正确时，$\xi_i=0$，意味着没有损失；当样本分类错误时，$\xi_i&gt;0$，存在损失。称$\xi_i$为<strong>松弛变量</strong>。</p><p>改写优化问题：</p><script type="math/tex; mode=display">\underset{\mathbf{w},b,\xi_i}{\min} \frac{||w||^2}{2} + C\sum_{i=1}^m\xi _i \\ s.t. \;y_i(\mathbf{w}^T\mathbf{x}_i+b) \geq 1-\xi_i,\;\xi_i\geq 0,\;i=1,2,\dots,m</script><p>可以看出此优化问题与之前使用硬间隔时十分相似，可以使用同样的方法求解，这里不再重复。</p><p>使用软间隔，既能方便SVM的求解，又能在一定程度上防止过拟合（因为它能避免部分异常样本的影响）。</p><h2 id="4-支持向量回归"><a href="#4-支持向量回归" class="headerlink" title="4. 支持向量回归"></a>4. 支持向量回归</h2><p>对于回归问题，我们希望学得函数$f(\mathbf{x})=\mathbf{w}^T\mathbf{x}_i+b$，使得$f(\mathbf{x})$与真实值$y$尽可能接近。与线性回归模型使用$f(\mathbf{x})$与$y$的差别计算损失不同，支持向量回归（SVR）假设我们能够容忍$f(\mathbf{x})$与$y$有$\epsilon$的偏差，只有当差别绝对值大于$\epsilon$时才计算误差。</p><p>如图所示，这种方式相当于以$f(\mathbf{x})$为中心，构建出了一个宽度为$2\epsilon$的间隔带。若样本落入其中，则认为预测是正确的：<br><img src="https://wx2.sbimg.cn/2020/08/09/ojQ0M.png" srcset="/img/loading.gif" alt="间隔带"></p><p>于是，SVR问题可归结为以下优化问题：</p><script type="math/tex; mode=display">\underset{\mathbf{w},b}{\min} \frac{||w||^2}{2} + C\sum_{i=1}^m l_\epsilon\,(\,f(\mathbf{x}_i)-y_i)</script><p>其中，$C$为正则化常数，$l_\epsilon$为下图所示的ϵ-不敏感损失函数：</p><script type="math/tex; mode=display">l_\epsilon(z)=\left\{\begin{matrix}0,\;\;\;\;\;\;\; & if\;|z|<\epsilon;\;\\|z|-\epsilon, & otherwise.\end{matrix}\right.</script><p><img src="https://wx2.sbimg.cn/2020/08/09/ojeU8.png" srcset="/img/loading.gif" alt="损失函数"></p><p>引入松弛变量，可将优化问题改写为（不知为何识别不出换行，见谅）：</p><script type="math/tex; mode=display">\underset{\mathbf{w},b,\xi_i,\hat{\xi}_i}{\min} \frac{||w||^2}{2} + C\sum_{i=1}^m(\xi_i+\hat{\xi}_i) \\ s.t. \;f(\mathbf{x}_i)-y_i\leq \epsilon+\xi_i;f(\mathbf{x}_i)-y_i \leq \epsilon+\hat{\xi}_i;\xi_i\geq 0;\hat{\xi}_i\geq 0,\;i=1,2,\dots,m</script><p>解法依然与之前类似。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>支持向量机</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[机器学习理论]决策树</title>
    <link href="/2020/ML-2-Dicision-Tree/"/>
    <url>/2020/ML-2-Dicision-Tree/</url>
    
    <content type="html"><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p><strong>决策树</strong>（Decision Tree）是一种在分类问题中常用的模型，包含一个根结点、若干个内部结点和若干个叶结点。其中叶结点对应于决策结果，而其他结点对应于一个属性测试（决策过程中的一次判断）。每个结点中都包含了全体样本中的一部分，而根节点包含样本全集。</p><p>决策树的基本生成算法如下（来自周志华《机器学习》图4.2）：<br><img src="https://wx2.sbimg.cn/2020/08/06/o6n2T.png" srcset="/img/loading.gif" alt="决策树-算法"></p><h2 id="1-划分属性选择"><a href="#1-划分属性选择" class="headerlink" title="1. 划分属性选择"></a>1. 划分属性选择</h2><p>从决策树的生成算法中可知，生成一棵决策树，选择最优划分属性是至关重要的一步。既然有多个可以用来将集合进行划分的属性，那就需要一个评价指标来量化每个属性划分集合的效果（即哪个属性在当前结点中拥有更好的分类能力）。</p><h3 id="1-1-信息增益"><a href="#1-1-信息增益" class="headerlink" title="1.1 信息增益"></a>1.1 信息增益</h3><p><strong>熵</strong>（Entropy）用来表示随机变量的不确定性。在分类问题中，一个集合的分类程度越低，其熵就越大。因此，可以使用<strong>信息熵</strong>来衡量一个集合的纯度：</p><script type="math/tex; mode=display">Ent(D)=-\sum_{k=1}^{|\gamma|} p_k\log_2p_k</script><p>其中，$p_k$表示集合$D$中第$k$类样本所占比例。从定义可知，$ 0\leq Ent(D)k\leq \log \gamma$，其值越小，集合纯度越高。</p><blockquote><p>这里省略了最值的推导过程，仅给出思路：<br>所有类别概率相同时，Ent(D)取得最大值。可以证明-Ent(D)是一个凸函数，且只有等式约束，所以能令其拉格朗日函数的一阶偏导数等于0的点满足KKT条件，使得此函数取得最小值（即Ent(D)的最大值）。<br>仅有一个类别时，Ent(D)取得最小值0。此求和函数中每一项形式均相同，可对任一项求一、二阶导数得到其最小值为0.当求和函数中每一项取最小值时，整体取最小值。考虑约束（概率之和为1），则Ent(D)大于等于0，使任一样本概率取1，其余取0时，Ent(D)取得最小值0。<br>需要注意，在信息熵的计算中，定义$0·log_20=0$。</p></blockquote><p>有了衡量集合纯度的标准，就可以使用它来比较集合划分前后获得的纯度提升。假设使用属性a进行一次划分，产生了$V$个分支结点，其中第$v$个结点记作$D^v$。对每个分支结点计算信息熵，再乘以其权重（$D^v$与$D$的样本数之比），就可以得到划分后的信息熵，也就可以计算这次划分的<strong>信息增益</strong>：</p><script type="math/tex; mode=display">Gain(D,a)=Ent(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)</script><p>其中，记$ H(D|a)=\sum_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v) $，称作<strong>条件熵</strong>，表示集合$D$在已知属性a的样本取值的情况下，集合的不确定性。</p><p>用划分前的信息熵减去划分后的信息熵，得到的信息增益越大，说明当前结点下属性a的分类能力越强。因此，可以以信息增益为标准选择每个结点的划分属性，如<strong>ID3算法</strong>，生成过程与前面图中的生成算法基本一致，区别在于：</p><ul><li>第8行：选择信息增益最大的属性$a_*$。</li><li>第9行开始的循环中：若$a_*$带来的信息增益小于阈值$\epsilon$，则将该结点设为叶结点并返回。</li></ul><p>选择信息增益作为选择划分属性的标准时，<strong>取值较多的特征容易有更大的信息增益</strong>：例如，如果存在一个属性，其在每个样本上的取值都不同（如编号），那此属性的信息增益必然最大（划分后的信息熵为0）。以此类推，取值较多的属性可能会对决策树带来不利影响，降低其泛化能力。</p><h3 id="1-2-增益率"><a href="#1-2-增益率" class="headerlink" title="1.2 增益率"></a>1.2 增益率</h3><p>针对上节末尾提出的问题，应当对信息增益作出改进。因此，在<strong>C4.5算法</strong>中，提出了<strong>增益率</strong>的概念：</p><script type="math/tex; mode=display">Gain\_ratio=\frac{Gain(D,a)}{IV(a)}</script><script type="math/tex; mode=display">IV(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}</script><p>其中，$IV(a)$称为属性a的固有值（intrinsic value）。属性a的取值越多，$IV(a)$的值越大。这样，就限制了取值较多的属性的影响。</p><p>但是，从定义可以看出，与信息增益相反，增益率更倾向于选择取值较少的属性作为划分属性。因此，C4.5算法并没有直接使用它作为标准，而是先选出信息增益高于平均水平的属性，再从中选择增益率更高的属性作为划分属性。</p><h3 id="1-3-基尼指数"><a href="#1-3-基尼指数" class="headerlink" title="1.3 基尼指数"></a>1.3 基尼指数</h3><p><strong>CART算法</strong>（Classification and Regression Tree）中使用了<strong>基尼指数</strong>衡量集合的纯度：</p><script type="math/tex; mode=display">Gini(D)=\sum_{k=1}^{|\gamma|}\sum_{ {k}'\neq k}p_kp_{ {k}'}=\sum_{k=1}^{|\gamma|}p_k(1-p_k)=1- \sum_{k=1}^{|\gamma|}p_k^2</script><p>基尼指数反映了从一个集合中随机抽出两个样本，其类别不一致的概率，因此基尼指数越小，集合纯度越高。</p><p>类似的，属性a的基尼指数定义为：</p><script type="math/tex; mode=display">Gini_index(D,a)=\sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)</script><p>这样，就可以选择使得划分后基尼指数最小的属性作为划分属性。</p><h2 id="2-剪枝"><a href="#2-剪枝" class="headerlink" title="2. 剪枝"></a>2. 剪枝</h2><p>在上面提出的决策树生成算法中，结点停止分裂（成为叶结点）的条件是最优划分属性带来的增益小于某个阈值，或结点已经不能再划分。为了尽可能提高纯度（取决于算法中使用的评价标准），结点划分过程可能会不断重复，甚至将数据集本身的一些特性当成数据的一般特性，导致过拟合现象，使得模型的泛化能力低。</p><p>为了降低过拟合，可以从算法层面主动去掉一些对模型泛化能力提升不利的分支，这就是<strong>剪枝</strong>（pruning），决策树模型处理过拟合的主要方法。</p><h3 id="2-1-预剪枝"><a href="#2-1-预剪枝" class="headerlink" title="2.1 预剪枝"></a>2.1 预剪枝</h3><p><strong>预剪枝</strong>，指在结点划分前进行估计，如果这次划分不能带来泛化能力的提升，就取消划分，将此结点标记为叶结点。</p><blockquote><p>评价泛化能力常用的方法是预留出一部分样本作为<strong>验证集</strong>，在其上验证模型。</p></blockquote><p>可以看出，预剪枝方法不仅能够预防过拟合，而且能够降低训练决策树的时间和资源开销，因为它组织了大量结点的展开和后续计算。</p><p>但是，考虑到情况：有的结点本身展开会导致模型泛化能力的暂时降低，但其后续结点的展开能提升模型性能。预剪枝方法有可能会导致模型欠拟合。</p><h3 id="2-2-后剪枝"><a href="#2-2-后剪枝" class="headerlink" title="2.2 后剪枝"></a>2.2 后剪枝</h3><p>与预剪枝相反，后剪枝方法在完整的决策树生成之后开始运作。此方法会从底层开始，逐个判断每个非叶结点若取消划分是否能提升模型泛化能力，若可以，则将其分裂出的叶结点剪除。</p><p>后剪枝方法同样能降低过拟合风险，但它的开销较大。</p><h2 id="3-连续值与缺失值处理"><a href="#3-连续值与缺失值处理" class="headerlink" title="3. 连续值与缺失值处理"></a>3. 连续值与缺失值处理</h2><h3 id="3-1-连续值处理"><a href="#3-1-连续值处理" class="headerlink" title="3.1 连续值处理"></a>3.1 连续值处理</h3><p>在决策树的基本生成算法中，结点的划分是对最优划分属性的每个取值生成一个子结点，这种方法只适用于属性的取值为离散的情况。当属性取值为连续时，需要对其进行离散化处理。</p><p>离散化处理最简单的方法是二分法，即将连续取值的属性值从小到大排列，将每两个相邻值的中位数设为划分点，然后将每两个相邻划分点组成的区间作为一个类别。</p><h3 id="3-2-缺失值处理"><a href="#3-2-缺失值处理" class="headerlink" title="3.2 缺失值处理"></a>3.2 缺失值处理</h3><p>在实际项目中，常常会出现有缺失属性的样本。如果因为存在属性缺失就放弃整个样本，会造成很大的浪费。因此需要考虑如何在有缺失属性的情况下选择最优划分属性，并对结点进行划分。这里给出C4.5算法中的解决方案。</p><p>对于最优划分属性的选择，算法对信息增益进行了推广：</p><script type="math/tex; mode=display">Gain(D,a)=p(Ent(\tilde{D})-\sum_{v=1}^{V}\tilde{r}_vEnt(\tilde{D^v}))</script><p>其中，在属性a上，$\tilde{D}$表示无缺失值样本组成的子集，$p$表示无缺失值样本所占比例，$\tilde{r}_v$表示在$\tilde{D}$中取值为$a^v$的样本所占比例。</p><p>最优划分属性选择完成后，需要对每个样本根据其取值进行划分。C4.5算法的划分规则为：</p><ul><li>属性a未缺失的样本，划分到与其取值对应的子结点。</li><li>属性a缺失的样本，同时划分到所有子结点，并将其权重调整为$\tilde{r}_v·w_x$。</li></ul><h2 id="4-多变量决策树"><a href="#4-多变量决策树" class="headerlink" title="4. 多变量决策树"></a>4. 多变量决策树</h2><p>上面提到的算法中，非叶结点对样本的划分都是基于某个属性的取值。这种方法映射到坐标系中，相当于使用与坐标轴平行的直线对样本进行划分。这种划分方式不仅不够灵活，而且划分次数很多，开销较大。</p><p>多变量决策树不再使用单个变量的取值，而是使用多个变量的线性组合（这种情况实现了斜线划分）、甚至更复杂的组合函数进行判断与划分。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>决策树</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[机器学习理论]线性模型</title>
    <link href="/2020/ML-1-Linear-Model/"/>
    <url>/2020/ML-1-Linear-Model/</url>
    
    <content type="html"><![CDATA[<h1 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h1><p>对于数据集$D={(x_1, y_1),(x_2, y_2),…,(x_m, y_m)}$，其中$x_i$为$d$维向量，$y_i$为实数。试图求得一个使用属性的线性组合进行预测的函数：</p><script type="math/tex; mode=display">f(x)=w_1x_1+w_2x_2+...w_dx_d+b</script><p>或用向量形式表示：</p><script type="math/tex; mode=display">f(x)=w^Tx+b</script><p>这种模型称作<strong>线性模型</strong>。</p><p>线性模型形式简单、易于建模，且具有较好的可解释性。</p><h2 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1. 线性回归"></a>1. 线性回归</h2><p>对于数据集$D={(x_1, y_1),(x_2, y_2),…,(x_m, y_m)}$，其中$x_i$为$d$维向量，$y_i$为实数。</p><h3 id="1-1-一元线性回归的最优值求解"><a href="#1-1-一元线性回归的最优值求解" class="headerlink" title="1.1 一元线性回归的最优值求解"></a>1.1 一元线性回归的最优值求解</h3><p>从一元线性回归开始考虑（即$xi$为实数），线性回归试图学得：</p><script type="math/tex; mode=display">f(x_i)=wx_i+b，使得f(x_i)\simeq y_i</script><p><strong>均方误差</strong>(Mean-Square Error, MSE)是回归任务中最常用的性能度量。线性回归的目标是让预测值与真实值之间的差别最小，在这里就是让均方差最小化，即：</p><script type="math/tex; mode=display">(w^*, b^*)=\underset{(w, b)}{arg\ min}\sum_{i=1}^{m}(y_i-f(x_i))^2</script><p>基于均方差最小化求解模型的方法称为<strong>最小二乘法</strong>。在几何意义上，最小二乘法就是试图找到一条直线，使得所有样本点到直线的欧氏距离之和最小。</p><p>将$E_{(w,b)}$分别对$w$和$b$求导，得到：</p><script type="math/tex; mode=display">\frac{\partial E_{(w,b)}}{\partial w} = 2(w\sum_{i=1}^{m}{x_i}^2-\sum_{i=1}^{m}x_i(y_i-b))</script><script type="math/tex; mode=display">\frac{\partial E_{(w,b)}}{\partial b} = 2(mb-\sum_{i=1}^{m}(y_i-wx_i))</script><p>推导过程：<br><img src="https://wx1.sbimg.cn/2020/08/04/oM2sD.jpg" srcset="/img/loading.gif" alt="求导"></p><p>求$E_{(w,b)}$的最小值，需要用到以下定理：</p><blockquote><p>设函数$f(x,y)$在区间$D$上具有二阶连续偏导数，记$A=f<em>{xx}^{‘ ‘ }(x,y)$，$B=f</em>{xy}^{‘ ‘ }(x,y)$，$C=f_{yy}^{‘ ‘ }(x,y)$则：<br>在$D$上恒有$A&gt;0$，且$AC-B^2\geq 0$时，$f(x,y)$在$D$上是凸函数。<br>在$D$上恒有$A&lt;0$，且$AC-B^2\geq 0$时，$f(x,y)$在$D$上是凹函数。</p></blockquote><p><blockquote><p>设函数$f(x,y)$是在开区间$D$上具有连续偏导数的凸（凹）函数，$x_0,y_0\in D$且$f_x ‘(x_0,y_0)=0,f_y ‘(x_0,y_0)=0$，则$f(x_0,y_0)$必为$f(x,y)$在$D$上的最小值（最大值）。</p></blockquote><p>首先证明$E_{(w,b)}$是其定义域上的凸函数：<br><img src="https://wx2.sbimg.cn/2020/08/04/oMpEO.jpg" srcset="/img/loading.gif" alt="线性回归-凸函数1"><br><img src="https://wx2.sbimg.cn/2020/08/04/oMXo7.jpg" srcset="/img/loading.gif" alt="线性回归-凸函数2"></p><p>因此，当$E_{(w,b)}$关于$w$和$b$的导数均为0时，$w$和$b$取得最优解。令上面两式等于0，可求得：</p><script type="math/tex; mode=display">w=\frac{ \sum_{i=1}^{m} y_i(x_i-\bar{x})}{\sum_{i=1}^{m}{x_i}^2-\frac{1}{m}(\sum_{i=1}^{m}x_i)^2}</script><script type="math/tex; mode=display">b=\frac{\sum_{i=1}^{m}(y_i-wx_i)}{m}</script><p>推导过程：<br><img src="https://wx2.sbimg.cn/2020/08/04/oMoBk.jpg" srcset="/img/loading.gif" alt="线性回归-最优解"></p><h3 id="1-2-向量化"><a href="#1-2-向量化" class="headerlink" title="1.2 向量化"></a>1.2 向量化</h3><p>上面得到的结果中，使用了连续求和方式进行运算。在编程中，这种计算方式一般需要使用循环结构实现。如果能将结果表示成向量之间的运算，就可以调用<code>numpy</code>等矩阵运算库快速计算。</p><p>首先将$w$转换成可以使用向量表示的形式：</p><script type="math/tex; mode=display">w=\frac{\sum_{i=1}^{m}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{m}(x_i-\bar{x})^2}</script><p>推导过程：<br><img src="https://wx2.sbimg.cn/2020/08/04/oM0ke.jpg" srcset="/img/loading.gif" alt="线性回归-向量化"></p><p>规定向量：</p><script type="math/tex; mode=display">\mathbf{x}=(x_1,x_2,...,x_m)^T,\;\;\;  \mathbf{x}_d=(x_1-\bar{x},x_2-\bar{x},...,x_m-\bar{x})^T</script><script type="math/tex; mode=display">\mathbf{y}=(y_1,y_2,...,y_m)^T,\;\;\;  \mathbf{y}_d=(y_1-\bar{y},y_2-\bar{y},...,y_m-\bar{y})^T</script><p>则$w$可向量化表示为：</p><script type="math/tex; mode=display">w=\frac{\mathbf{x}_d^T\mathbf{y}_d}{\mathbf{x}_d^T\mathbf{x}_d}</script><h3 id="1-3-多元线性回归"><a href="#1-3-多元线性回归" class="headerlink" title="1.3 多元线性回归"></a>1.3 多元线性回归</h3><p>在更一般的情况中，样本$x<em>i$由多个属性表示，即$x_i=(x</em>{i1};x<em>{i2};…;x</em>{id})$。此时线性回归求解的目标是：</p><script type="math/tex; mode=display">f(\mathbf{x_i})=\mathbf{w}^T\mathbf{x_i}+b，使得f(\mathbf{x_i})\simeq y_i</script><p>为了方便以矩阵形式表示和运算，令：</p><script type="math/tex; mode=display">\hat{\mathbf{w}}=(w_1;w_2;...;w_d;b)</script><script type="math/tex; mode=display">\hat{\mathbf{x}}_i=(x_{i1};x_{i2};...;x_{id};1)</script><p>这样，求解目标就可以表示为：</p><script type="math/tex; mode=display">f(\hat{\mathbf{x}}_i)=\hat{\mathbf{w}}^T\hat{\mathbf{x}}_i，使得f(\hat{\mathbf{x}}_i)\simeq y_i</script><p>通过最小二乘法导出损失函数$E(\hat{\mathbf{w}})$：</p><script type="math/tex; mode=display">E(\hat{\mathbf{w}})=\sum_{i=1}^{m}(y_i-f(\hat{\mathbf{x}}_i))^2=\sum_{i=1}^{m}(y_i-\hat{\mathbf{w}}^T\hat{\mathbf{x}}_i)^2</script><p>接下来进行向量化表示。令：</p><script type="math/tex; mode=display">\mathbf{X}=\begin{pmatrix}x_{11} & x_{12} & \cdots & x_{1d} & 1\\x_{21} & x_{22} & \cdots & x_{2d} & 1\\\vdots & \vdots & \ddots & \vdots & 1\\x_{m1} & x_{m2} & \cdots & x_{md} & 1\end{pmatrix}=\begin{pmatrix}\mathbf{\hat{x}_1}\\\mathbf{\hat{x}_2}\\\vdots\\\mathbf{\hat{x}_m}\end{pmatrix}</script><script type="math/tex; mode=display">\mathbf {y}=\begin{pmatrix}y_1\\y_2\\\vdots\\y_m\end{pmatrix}</script><p>则可以将$E(\hat{\mathbf{w}})$向量化表示为：</p><script type="math/tex; mode=display">E(\hat{\mathbf{w}})=(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}})^T(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}})</script><p>推导过程：<br><img src="https://wx2.sbimg.cn/2020/08/04/oMmq6.jpg" srcset="/img/loading.gif" alt="多元线性回归-向量化"></p><p>与一元线性回归类似，通过凸函数的性质来求$E(\hat{\mathbf{w}})$的最优解。需要用到以下定理：</p><blockquote><p><strong>凸集</strong>：设集合$D\in R^n$，如果对任意$x,y\in D$与任意$a\in [0,1]$，有$ax+(1-a)y\in D$，则称集合$D$为凸集。<br>凸集的几何意义是：若两个点属于此集合，则这两点连线上的任意点都属于此集合。</p></blockquote><p><blockquote><p><strong>Hessian矩阵</strong>：设n元函数$f(\mathbf{x})$对自变量$\mathbf{x}=(x_1,x_2,…,x_n)^T$的各分量$x_i$的二阶偏导数$\frac{\partial ^2f(\mathbf{x})}{\partial x_i\partial x_j}, i=1,2,…,n;j=1,2,…,n$都存在，则称$f(\mathbf{x})$在点$\mathbf{x}$处二阶可导，并称矩阵</p><script type="math/tex; mode=display">\triangledown ^2f(\mathbf{x})=\begin{pmatrix}\frac{\partial ^2f(\mathbf{x})}{\partial x_i^2} & \frac{\partial ^2f(\mathbf{x})}{\partial x_1\partial x_2} & \cdots & \frac{\partial ^2f(\mathbf{x})}{\partial x_1\partial x_n}\\\frac{\partial ^2f(\mathbf{x})}{\partial x_2\partial x_1} & \frac{\partial ^2f(\mathbf{x})}{\partial x_2^2} & \cdots & \frac{\partial ^2f(\mathbf{x})}{\partial x_2\partial x_n}\\\vdots & \vdots & \ddots & \vdots\\\frac{\partial ^2f(\mathbf{x})}{\partial x_n\partial x_1} & \frac{\partial ^2f(\mathbf{x})}{\partial x_n\partial x_2} & \cdots & \frac{\partial ^2f(\mathbf{x})}{\partial x_n^2}\end{pmatrix}</script><p>为$f(\mathbf{x})$在点$\mathbf{x}$处的二阶导数或Hessian矩阵，记为$ \triangledown ^2f(\mathbf{x}) $。当$f(\mathbf{x})$对$\mathbf{x}$各分量的所有二阶偏导数都连续时，$ \triangledown ^2f(\mathbf{x}) $为对称矩阵。</p></blockquote><p><blockquote><p><strong>多元实值函数凹凸性判断定理</strong>：设$D\subset R^n$是非空凸开集，$f:D\subset R^n\to R$，且$f(\mathbf{x})$在$D$上二阶连续可微，如果$f(\mathbf{x})$的Hessian矩阵$ \triangledown ^2f(\mathbf{x}) $在$D$上是正定的，则$f(\mathbf{x})$是$D$上的严格凸函数。</p></blockquote><p><blockquote><p><strong>凸充分性定理</strong>：若$f:R^n\to R$是凸函数，且$f(\mathbf{x})$一阶连续可微，则$\mathbf{x}^<em>$是全局解的充分必要条件是$ \triangledown f(\mathbf{x}^</em>)=0$。</p></blockquote><p>要求$E(\hat{\mathbf{w}})$的Hessian矩阵，首先求其一阶偏导数：</p><script type="math/tex; mode=display">\frac{\partial E(\hat{\mathbf{w}})}{\partial \hat{\mathbf{w}}}=2\mathbf{X}^T(\mathbf{X}\hat{\mathbf{w}}-\mathbf{y})</script><p>推导过程：<br><img src="https://wx2.sbimg.cn/2020/08/04/oMIJI.jpg" srcset="/img/loading.gif" alt="多元线性回归-一阶导数"></p><p>再次求导，得到Hessian矩阵：</p><script type="math/tex; mode=display">\frac{\partial ^2 E(\hat{\mathbf{w}})}{\partial \hat{\mathbf{w}}\partial \hat{\mathbf{w}}^T}=2\mathbf{X}^T\mathbf{X}</script><p>推导过程：<br><img src="https://wx1.sbimg.cn/2020/08/04/oMF2R.jpg" srcset="/img/loading.gif" alt="多元线性回归-二阶导数"></p><p>假设$\mathbf{X}^T\mathbf{X}$是正定矩阵（现实中往往不是，此时按照之后的方法可以求得多个解，都可使均方差最小化，因此这里可以假设它是正定的以方便证明），则$E(\hat{\mathbf{w}})$是关于$\hat{\mathbf{w}}$的凸函数。</p><p>因此，使得$ \frac{\partial E(\hat{\mathbf{w}})}{\partial \hat{\mathbf{w}}}=0$的$\hat{\mathbf{w}}^*$使得函数取得最小值：</p><script type="math/tex; mode=display">\hat{\mathbf{w}}^*=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}</script><p>推导过程：<br><img src="https://wx1.sbimg.cn/2020/08/04/oMyba.jpg" srcset="/img/loading.gif" alt="多元线性回归-最优解"></p><h2 id="2-对数几率回归"><a href="#2-对数几率回归" class="headerlink" title="2. 对数几率回归"></a>2. 对数几率回归</h2><p>线性回归方法使用线性模型完成了回归任务。实际上，线性模型也可以用于分类任务。</p><h3 id="2-1-广义线性模型"><a href="#2-1-广义线性模型" class="headerlink" title="2.1 广义线性模型"></a>2.1 广义线性模型</h3><p>在上一节，线性模型的学习目标是让预测值$f(x)$逼近真实值$y$。我们也可以让$f(x)$去逼近$y$的衍生物。例如，如果认为$y$是以指数方式变化，就可以让$f(x)$逼近$\ln y$：</p><script type="math/tex; mode=display">\ln y=w^Tx+b</script><p>这种线性模型被称作<strong>对数线性回归</strong>，它实际上是在让$e^{w^Tx+b}$逼近$y$。</p><p>由此推广，    可以让$w^Tx+b$去逼近$g(y)$，其中$g(·)$是任意单调可微函数，即：</p><script type="math/tex; mode=display">g(y)=w^Tx+b</script><p>也可表示为：</p><script type="math/tex; mode=display">y=g^{-1}(w^Tx+b)</script><p>这就是<strong>广义线性模型</strong>，其中$g(·)$称为<strong>联系函数</strong>。</p><h3 id="2-2-对数几率回归"><a href="#2-2-对数几率回归" class="headerlink" title="2.2 对数几率回归"></a>2.2 对数几率回归</h3><p>由广义线性模型可以想到，如果要用线性模型做分类任务，只需要找到一个单调可微的联系函数，将线性回归的预测值和真实标记值联系起来即可。</p><p>从二分类问题开始考虑，<strong>对数几率函数</strong>（Logistic Function）是一个常用的函数：</p><script type="math/tex; mode=display">y=\frac{1}{1+e^{-z}}</script><p>它是单位阶跃函数（由于不连续而无法在此使用）的替代：<br><img src="https://wx2.sbimg.cn/2020/08/05/o03PA.png" srcset="/img/loading.gif" alt="联系函数"></p><p>将对数几率函数作为$g^{-1}(·)$，得到：</p><script type="math/tex; mode=display">y=\frac{1}{1+e^{-(w^Tx+b)}}</script><p>求逆函数，可以得到此时线性回归预测值的目标函数：</p><script type="math/tex; mode=display">\ln{\frac{y}{1-y}}=w^Tx+b</script><p>将$y$视作样本$x$是正例的可能性，则$1-y$是反例可能性，两者比值$\frac{y}{1-y}$称为<strong>几率</strong>（odds）。对它取对数则得到<strong>对数几率</strong>（log odds，又称logit）：$\ln{\frac{y}{1-y}}$。</p><p>这样，就可以用线性回归的预测结果去逼近真实标记的对数几率，这个模型称作<strong>对数几率回归</strong>，是一种分类学习方法。</p><h3 id="2-3-对数几率回归的参数求解"><a href="#2-3-对数几率回归的参数求解" class="headerlink" title="2.3 对数几率回归的参数求解"></a>2.3 对数几率回归的参数求解</h3><p>根据上节内容，对数几率回归可以表示为：</p><script type="math/tex; mode=display">\ln{\frac{p(y=1|x)}{p(y=0|x)}}=w^Tx+b</script><p>显然有：</p><script type="math/tex; mode=display">p(y=1|x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}</script><script type="math/tex; mode=display">p(y=0|x)=\frac{1}{1+e^{w^Tx+b}}</script><p>接下来，就可以用极大似然法估计$w$和$b$。</p><blockquote><p><strong>极大似然估计</strong>：设总体的概率密度函数（或分布律）为$f(y,w_1,w_2,…,w_k)$，$y_1,y_2,…,y_m$是从该总体中抽出的样本。因为$y_1,y_2,…,y_m$相互独立且同分布，所以它们的联合概率密度函数（或联合概率）为：</p><script type="math/tex; mode=display">L(y_1,y_2,...,y_m;w_1,w_2,...,w_k)=\prod_{i=i}^{m} f(y_i,w_1,w_2,...,w_k)</script><p>其中，$w_1,w_2,…,y_k$是固定但未知的参数。当已经拥有一组样本观测值$y_1,y_2,…,y_m$，要估计未知参数时，一种直观的想法就是，使得当前样本出现概率最大的参数组可能是真实的。这就是极大似然估计。<br>通常记$ L(y_1,y_2,…,y_m;w_1,w_2,…,w_k)=L(w) $，并称其为<strong>似然函数</strong>，于是求$w$的极大似然估计问题就变成了求$L(w)$的最大值点。<br>由于对数函数是单调递增的，所以$\ln{L(w)}$与$L(w)$有相同的最大值点。通过取对数，可以实现如下转换：</p><script type="math/tex; mode=display">\ln{L(w)}=\ln{(\prod_{i=i}^{m} f(y_i,w_1,w_2,...,w_k))}=\sum_{i=i}^{m} \ln{f(y_i,w_1,w_2,...,w_k)}</script><p>称$\ln{L(w)}$为<strong>对数似然函数</strong>，在很多情况下，求它的最大值点更简单。</p></blockquote><p>利用多元线性回归中用到的思想，将$w$和$b$合并为参数$\beta$，令$\hat{x}=(x;1)$，则上面得到的概率表达式可简化为：</p><script type="math/tex; mode=display">p_1(\hat{x};\beta)=p(y=1|x)=\frac{e^{\beta^T\hat{x}}}{1+e^{\beta^T\hat{x}}}</script><script type="math/tex; mode=display">p_0(\hat{x};\beta)=p(y=0|x)=\frac{1}{1+e^{\beta^T\hat{x}}}</script><p>由于当$y=1$时，概率为$p_1$；$y=0$时，概率为$p_0$。所以可以从上面两个式子得到随机变量$y$的分布律（这里给出两种满足需要的表达式，后续的推导使用第一种。使用第二种也可得出相同结果）：</p><script type="math/tex; mode=display">p(y|x;w,b)=y·p_1(\hat{x};\beta)+(1-y)·p_0(\hat{x};\beta)</script><script type="math/tex; mode=display">p(y|x;w,b)=[p_1(\hat{x};\beta)]^y[p_0(\hat{x};\beta)]^{1-y}</script><p>接下来，就可以将对数似然函数中的概率密度函数替换为$p(y|x;w,b)$：</p><script type="math/tex; mode=display">l(w,b):=\ln{L(w)}=\sum_{i=i}^{m} \ln{(y_i·p_1(\hat{x}_i;\beta)+(1-y_i)·p_0(\hat{x}_i;\beta))}</script><p>将$p_1$和$p_0$代入得：</p><script type="math/tex; mode=display">l(w,b)=\sum_{i=i}^{m} \ln{(\frac{y_ie^{\beta^T\hat{x}_i}}{1+e^{\beta^T\hat{x}_i}}+\frac{1-y_i}{1+e^{\beta^T\hat{x}_i}})}</script><p>化简得（由于将其作为损失函数，目标是将损失函数最小化，所以对化简结果取负）：</p><script type="math/tex; mode=display">l(w,b)=\sum_{i=i}^{m} (-y_i\beta^T\hat{x}_i+\ln{(1+e^{\beta^T\hat{x}_i})})</script><p>推导过程：<br><img src="https://wx1.sbimg.cn/2020/08/05/o0ys6.jpg" srcset="/img/loading.gif" alt="对数几率回归-对数似然函数化简"></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>线性模型</tag>
      
      <tag>线性回归</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[TF2]11. Dataset</title>
    <link href="/2020/TF2-11-Dataset/"/>
    <url>/2020/TF2-11-Dataset/</url>
    
    <content type="html"><![CDATA[<div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfprint(tf.__version__)<span class="hljs-comment"># 2.2.0</span></code></pre></div><p><code>tf.data</code>是TensorFlow中用于构建输pipeline的一组<code>Dataset</code>的API。</p><p><code>Dataset</code>：TensorFlow中用于表示数据的类，可以看作是数据的有序列表。它有三个常用子类：</p><ul><li><code>FixedLengthRecordDataset</code>: 用于从二进制文件中读取定长记录的<code>Dataset</code>。</li><li><code>TFRecordDataset</code>: 从一个或多个TFRecord文件中读取并组成记录的<code>Dataset</code>。</li><li><code>TextLineDataset</code>: 从一个或多个文本文件中读取并组成记录的<code>Dataset</code>。</li></ul><h1 id="1-Dataset的基本使用"><a href="#1-Dataset的基本使用" class="headerlink" title="1. Dataset的基本使用"></a>1. <code>Dataset</code>的基本使用</h1><p>建立<code>Dataset</code>，有三种读取数据的方法：</p><ul><li><code>from_tensors</code>: 读取Tensor并合并，创建只有单个元素的<code>Dataset</code>。</li><li><code>from_tensor_slices</code>: 将读入的Tensor按第一维分割，创建有多个元素的<code>Dataset</code>。</li><li><code>from_generator</code>: 读取一个generator（可以调用iter()方法的对象），创建一个使用此generator生成数据的<code>Dataset</code>。</li></ul><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npdata1 = np.zeros(shape=(<span class="hljs-number">3</span>,<span class="hljs-number">5</span>,<span class="hljs-number">2</span>),dtype=np.float32)data2 = tf.constant([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])</code></pre></div><p>从Tensor读取数据，可以看出<code>from_tensors</code>和<code>from_generator</code>的区别：</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 合并元素，建立具有单个元素的Dataset</span>dataset1 = tf.data.Dataset.from_tensors(data1)print(<span class="hljs-string">'dataset1:'</span>)<span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> dataset1:    print(line.shape)  <span class="hljs-comment"># 只有1个元素，形状为3*5*2</span>dataset2 = tf.data.Dataset.from_tensors(data2)print(<span class="hljs-string">'\ndataset2:'</span>)<span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> dataset2:    print(line)  <span class="hljs-comment"># 只有1个元素，形状为2*2</span></code></pre></div><p><img src="https://wx1.sbimg.cn/2020/07/10/CgD8G.png" srcset="/img/loading.gif" alt="from_tensors"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 以输入tensor的第一维分割，建立具有多个元素的Dataset</span>dataset1 = tf.data.Dataset.from_tensor_slices(data1)print(<span class="hljs-string">'dataset1:'</span>)<span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> dataset1:    print(line.shape)  <span class="hljs-comment"># 3*5*2的数据被分割为3个5*2的数据</span>dataset2 = tf.data.Dataset.from_tensor_slices(data2)print(<span class="hljs-string">'\ndataset2:'</span>)<span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> dataset2:    print(line)  <span class="hljs-comment"># 2*2的数据被分割为2个1*2的数据</span></code></pre></div><p><img src="https://wx1.sbimg.cn/2020/07/10/CgCuK.png" srcset="/img/loading.gif" alt="from_tensor_slices"></p><p>因此，读取数据集一般使用<code>from_tensors</code>方法：</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 从numpy读取数据</span>mnist = np.load(<span class="hljs-string">'./data/mnist.npz'</span>)x_train, y_train = mnist[<span class="hljs-string">'x_train'</span>],mnist[<span class="hljs-string">'y_train'</span>]x_train = np.expand_dims(x_train, axis=<span class="hljs-number">-1</span>)print(x_train.shape, y_train.shape)<span class="hljs-comment"># 输入数据为元组，在迭代时也可以成对地读取</span>mnist_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">for</span> image, label <span class="hljs-keyword">in</span> mnist_dataset:    plt.title(label.numpy())    plt.imshow(image.numpy()[:, :,<span class="hljs-number">0</span>])    plt.show()    <span class="hljs-keyword">break</span></code></pre></div><p><img src="https://wx1.sbimg.cn/2020/07/10/CgkOR.png" srcset="/img/loading.gif" alt="mnist"></p><p>可以看出，x_train和y_train均被分割为60000个Tensor，每个表示一张28*28的单通道图像对应一个label。</p><p>从pandas读取数据的方法类似：</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 从pandas读取数据</span><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pddf = pd.read_csv(<span class="hljs-string">'./data/tfdata/heart.csv'</span>)df.head()</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/07/10/Cg2DI.png" srcset="/img/loading.gif" alt="pandas-1"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 处理非数值类型的数据</span>df[<span class="hljs-string">'thal'</span>] = pd.Categorical(df[<span class="hljs-string">'thal'</span>])df[<span class="hljs-string">'thal'</span>] = df.thal.cat.codesdf.head()</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/07/10/Cgpza.png" srcset="/img/loading.gif" alt="pandas-2"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 将数据分为特征和标签</span>value = df.pop(<span class="hljs-string">'target'</span>)print(df.shape, value.shape)</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/10/CgomT.png" srcset="/img/loading.gif" alt="pandas-3"></p><div class="hljs"><pre><code class="hljs Python">dataset = tf.data.Dataset.from_tensor_slices((df.values, value.values))<span class="hljs-keyword">for</span> feature, target <span class="hljs-keyword">in</span> dataset:    print(feature.shape, feature)    print(target.shape, target)    <span class="hljs-keyword">break</span></code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/10/Cg3gw.png" srcset="/img/loading.gif" alt="pandas-4"></p><p>对一个建立好的<code>Dataset</code>，常用到以下处理函数：</p><ul><li><code>Dataset.map(f)</code>: 对其中所有元素应用函数f，得到新的数据集。</li><li><code>Dataset.shuffle(buffer_size)</code>: 打乱数据集。</li><li><code>Dataset.batch(batch_size)</code>: 将数据集分成批次。</li></ul><h1 id="2-TFRecordDataset的使用"><a href="#2-TFRecordDataset的使用" class="headerlink" title="2. TFRecordDataset的使用"></a>2. <code>TFRecordDataset</code>的使用</h1><p><code>TFRecordDataset</code>常用于处理体积过大而无法一次性读入内存的数据集，TensorFlow能高效、快速地处理<code>.tfrecords</code>格式的文件。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 定义Feature结构，告诉解码器每个Feature的类型是什么</span>feature_description = &#123;    <span class="hljs-string">'image'</span>: tf.io.FixedLenFeature([], tf.string),    <span class="hljs-string">'label'</span>: tf.io.FixedLenFeature([], tf.int64),&#125;<span class="hljs-comment"># 将TFRecord文件中的每一个序列化的tf.train.Example解码</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_parse_example</span><span class="hljs-params">(example_string)</span>:</span>    feature_dict = tf.io.parse_single_example(example_string, feature_description)    feature_dict[<span class="hljs-string">'image'</span>] = tf.io.decode_jpeg(feature_dict[<span class="hljs-string">'image'</span>])    <span class="hljs-comment"># 解码JPEG图片</span>    feature_dict[<span class="hljs-string">'image'</span>] = tf.image.resize(feature_dict[<span class="hljs-string">'image'</span>], [<span class="hljs-number">256</span>, <span class="hljs-number">256</span>]) / <span class="hljs-number">255.0</span>    <span class="hljs-keyword">return</span> feature_dict[<span class="hljs-string">'image'</span>], feature_dict[<span class="hljs-string">'label'</span>]train_dataset = tf.data.TFRecordDataset(<span class="hljs-string">"./data/tfdata/sub_train.tfrecords"</span>)    <span class="hljs-comment"># 读取 TFRecord 文件</span>train_dataset = train_dataset.map(_parse_example)<span class="hljs-keyword">for</span> image,label <span class="hljs-keyword">in</span> train_dataset:    print(image)    print(label)    <span class="hljs-keyword">break</span></code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/10/Cg6ao.png" srcset="/img/loading.gif" alt="TFRecordDataset"></p><h1 id="3-TextLineDataset类的使用"><a href="#3-TextLineDataset类的使用" class="headerlink" title="3. TextLineDataset类的使用"></a>3. <code>TextLineDataset</code>类的使用</h1><p><code>TextLineDataset</code>提供了从文本文件读取生成数据集的简单方法，它将文本文件的一行视为一个Tensor。</p><div class="hljs"><pre><code class="hljs Python">titanic_lines = tf.data.TextLineDataset([<span class="hljs-string">'./data/tfdata/train.csv'</span>,<span class="hljs-string">'./data/tfdata/eval.csv'</span>])<span class="hljs-comment"># 将一行string按，分开</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">data_func</span><span class="hljs-params">(line)</span>:</span>    line = tf.strings.split(line, sep = <span class="hljs-string">","</span>)    <span class="hljs-keyword">return</span> linetitanic_data = titanic_lines.skip(<span class="hljs-number">1</span>).map(data_func)<span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> titanic_data:    print(line)    <span class="hljs-keyword">break</span></code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/10/Cg9Il.png" srcset="/img/loading.gif" alt="TextLineDataset"></p><h1 id="4-Dataset中的常用操作"><a href="#4-Dataset中的常用操作" class="headerlink" title="4. Dataset中的常用操作"></a>4. <code>Dataset</code>中的常用操作</h1><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># flat_map(f)：将f映射到数据集中的每个元素，再将数据集展平</span>dataset = tf.data.Dataset.from_tensor_slices([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])print(<span class="hljs-string">'dataset:'</span>)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> dataset:    print(i)print(<span class="hljs-string">'datasetq:'</span>)dataset2 = dataset.flat_map(<span class="hljs-keyword">lambda</span> x: tf.data.Dataset.from_tensor_slices(x))<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> dataset2:    print(i)</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/10/CgQQ1.png" srcset="/img/loading.gif" alt="flat_map"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># interleave(f, cycle_length, block_length)：每次取出cycle_length个元素，应用f，然后每次放回block_length个直到此次结果全部放回</span>dataset = tf.data.Dataset.range(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>)  <span class="hljs-comment"># [1, 2, 3, 4, 5]</span>dataset = dataset.interleave(    <span class="hljs-keyword">lambda</span> x: tf.data.Dataset.from_tensors(x).repeat(<span class="hljs-number">3</span>),    cycle_length=<span class="hljs-number">2</span>, block_length=<span class="hljs-number">2</span>)list(dataset.as_numpy_iterator())</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/07/10/CgA72.png" srcset="/img/loading.gif" alt="interleave"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># zip：将两个数据集纵向连接</span>a = tf.data.Dataset.range(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>)b = tf.data.Dataset.range(<span class="hljs-number">4</span>, <span class="hljs-number">7</span>)print(<span class="hljs-string">'dataset1:'</span>)dataset1 = tf.data.Dataset.zip((a, b))<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> dataset1:    print(i)print(<span class="hljs-string">'dataset2:'</span>)dataset2 = tf.data.Dataset.zip((b, a))<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> dataset2:    print(i)</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/10/CgjfA.png" srcset="/img/loading.gif" alt="zip"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># concatenate：将两个数据集横向连接</span>a = tf.data.Dataset.range(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>)b = tf.data.Dataset.range(<span class="hljs-number">4</span>, <span class="hljs-number">7</span>)print(<span class="hljs-string">'dataset1:'</span>)dataset1 = tf.data.Dataset.concatenate(a, b)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> dataset1:    print(i)print(<span class="hljs-string">'dataset2:'</span>)dataset2 = tf.data.Dataset.concatenate(b, a)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> dataset2:    print(i)</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/10/CgJh4.png" srcset="/img/loading.gif" alt="concatenate"></p><h1 id="5-Dataset提升性能的方法"><a href="#5-Dataset提升性能的方法" class="headerlink" title="5. Dataset提升性能的方法"></a>5. <code>Dataset</code>提升性能的方法</h1><ul><li><code>prefech()</code>：将准备数据与参数迭代同时执行。</li><li><code>interleave()</code>：可以并行化读取数据。</li><li><code>map()</code>：在函数中指定<code>num_parallel_calls</code>参数，使其中函数的应用并行执行。</li><li><code>cache</code>：将数据集缓存，避免某些操作在每个epoch都执行。一般在<code>map()</code>后添加。</li></ul><h1 id="6-实例：Dogs-vs-Cats"><a href="#6-实例：Dogs-vs-Cats" class="headerlink" title="6. 实例：Dogs vs Cats"></a>6. 实例：Dogs vs Cats</h1><p>原比赛网址： <a href="https://www.kaggle.com/c/dogs-vs-cats" target="_blank" rel="noopener">https://www.kaggle.com/c/dogs-vs-cats</a> （已关闭提交，数据从此处下载）</p><h2 id="6-1-准备数据"><a href="#6-1-准备数据" class="headerlink" title="6.1 准备数据"></a>6.1 准备数据</h2><p>为了方便读取并建立标签，手工将train和test文件夹内的文件分别分为dogs和cats两个文件夹。</p><p>由于官网上提供的test数据没有标签，所以从train中取出5000个数据（dogs和cats各取后2500个）作为验证集。</p><div class="hljs"><pre><code class="hljs Python">data_dir = <span class="hljs-string">'./data/dogs-vs-cats'</span>train_dogs_dir = data_dir + <span class="hljs-string">'/train/dogs/'</span>train_cats_dir = data_dir + <span class="hljs-string">'/train/cats/'</span>test_dogs_dir = data_dir + <span class="hljs-string">'/valid/dogs/'</span>test_cats_dir = data_dir + <span class="hljs-string">'/valid/cats/'</span></code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> osprint(os.listdir(train_dogs_dir)[:<span class="hljs-number">5</span>], len(os.listdir(train_dogs_dir)))print(os.listdir(train_cats_dir)[:<span class="hljs-number">5</span>], len(os.listdir(train_cats_dir)))print(os.listdir(test_dogs_dir)[:<span class="hljs-number">5</span>], len(os.listdir(test_dogs_dir)))print(os.listdir(test_cats_dir)[:<span class="hljs-number">5</span>], len(os.listdir(test_cats_dir)))</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/10/CgcPJ.png" srcset="/img/loading.gif" alt="dir"></p><div class="hljs"><pre><code class="hljs Python">train_dogs_filename = tf.constant([train_dogs_dir + filename <span class="hljs-keyword">for</span> filename <span class="hljs-keyword">in</span> os.listdir(train_dogs_dir)])train_cats_filename = tf.constant([train_cats_dir + filename <span class="hljs-keyword">for</span> filename <span class="hljs-keyword">in</span> os.listdir(train_cats_dir)])train_filenames = tf.concat([train_dogs_filename, train_cats_filename], axis=<span class="hljs-number">-1</span>)train_labels = tf.concat([tf.zeros(<span class="hljs-number">10000</span>), tf.ones(<span class="hljs-number">10000</span>)], axis=<span class="hljs-number">-1</span>)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">9995</span>, <span class="hljs-number">10005</span>):    print(train_filenames[i].numpy(), train_labels[i].numpy())print(len(train_filenames), len(train_labels))</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/10/CgBO8.png" srcset="/img/loading.gif" alt="train"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 根据文件名读取图片，并resize为256*256</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ReadImage</span><span class="hljs-params">(filename, label)</span>:</span>    img = tf.io.read_file(filename)    img = tf.image.decode_jpeg(img)    img = tf.image.resize(img, [<span class="hljs-number">256</span>, <span class="hljs-number">256</span>]) / <span class="hljs-number">255.0</span>    <span class="hljs-keyword">return</span> img, label<span class="hljs-comment"># 测试</span>img, label = ReadImage(tf.constant(<span class="hljs-string">'./data/dogs-vs-cats/train/cats/cat.10.jpg'</span>), tf.constant(<span class="hljs-number">0</span>))<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> pltplt.imshow(img.numpy())</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/10/Cgd1m.png" srcset="/img/loading.gif" alt="train_img"></p><div class="hljs"><pre><code class="hljs Python">train_dataset = tf.data.Dataset.from_tensor_slices((train_filenames, train_labels))train_dataset = train_dataset.map(ReadImage)<span class="hljs-keyword">for</span> img, label <span class="hljs-keyword">in</span> train_dataset:    print(img.shape, label)    plt.imshow(img.numpy())    <span class="hljs-keyword">break</span></code></pre></div><p><img src="https://wx1.sbimg.cn/2020/07/10/Cgspd.png" srcset="/img/loading.gif" alt="train_dataset_img"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># train_dataset = train_dataset.shuffle(buffer_size=20000)</span>train_dataset = train_dataset.batch(<span class="hljs-number">32</span>)</code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 使用同样的方法构建测试集</span>test_dogs_filename = tf.constant([test_dogs_dir + filename <span class="hljs-keyword">for</span> filename <span class="hljs-keyword">in</span> os.listdir(test_dogs_dir)])test_cats_filename = tf.constant([test_cats_dir + filename <span class="hljs-keyword">for</span> filename <span class="hljs-keyword">in</span> os.listdir(test_cats_dir)])test_filenames = tf.concat([test_dogs_filename, test_cats_filename], axis=<span class="hljs-number">-1</span>)test_labels = tf.concat([tf.zeros(<span class="hljs-number">2500</span>), tf.ones(<span class="hljs-number">2500</span>)], axis=<span class="hljs-number">-1</span>)test_dataset = tf.data.Dataset.from_tensor_slices((test_filenames, test_labels))test_dataset = test_dataset.map(ReadImage)test_dataset = test_dataset.shuffle(<span class="hljs-number">5000</span>)test_dataset = test_dataset.batch(<span class="hljs-number">32</span>)```    <span class="hljs-comment">## 6.2 构建模型</span>```Python<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CNNModel</span><span class="hljs-params">(tf.keras.models.Model)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        super(CNNModel, self).__init__()        self.conv1 = tf.keras.layers.Conv2D(<span class="hljs-number">32</span>, <span class="hljs-number">3</span>, activation=<span class="hljs-string">'relu'</span>)        self.maxpool1 = tf.keras.layers.MaxPooling2D()        self.conv2 = tf.keras.layers.Conv2D(<span class="hljs-number">32</span>, <span class="hljs-number">1</span>, activation=<span class="hljs-string">'relu'</span>)        self.maxpool2 = tf.keras.layers.MaxPooling2D()        self.flatten = tf.keras.layers.Flatten()        self.d1 = tf.keras.layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)        self.d2 = tf.keras.layers.Dense(<span class="hljs-number">2</span>, activation=<span class="hljs-string">'softmax'</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, x)</span>:</span>        x = self.conv1(x)        x = self.maxpool1(x)        x = self.conv2(x)        x = self.maxpool2(x)               x = self.flatten(x)        x = self.d1(x)        x = self.d2(x)        <span class="hljs-keyword">return</span> xmodel = CNNModel()loss_object = tf.keras.losses.SparseCategoricalCrossentropy()  <span class="hljs-comment">#label 没有one-hot</span>optimizer = tf.keras.optimizers.Adam(learning_rate=<span class="hljs-number">0.001</span>)train_loss = tf.keras.metrics.Mean(name=<span class="hljs-string">'train_loss'</span>)train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="hljs-string">'train_accuracy'</span>)test_loss = tf.keras.metrics.Mean(name=<span class="hljs-string">'test_loss'</span>)test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="hljs-string">'test_accuracy'</span>)<span class="hljs-meta">@tf.function</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_step</span><span class="hljs-params">(images, labels)</span>:</span>    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:        predictions = model(images)        loss = loss_object(labels, predictions)    gradients = tape.gradient(loss, model.trainable_variables)    optimizer.apply_gradients(zip(gradients, model.trainable_variables))    train_loss(loss)    train_accuracy(labels, predictions)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_step</span><span class="hljs-params">(images, labels)</span>:</span>    predictions = model(images)    t_loss = loss_object(labels, predictions)    test_loss(t_loss)    test_accuracy(labels, predictions)</code></pre></div><h2 id="6-3-训练模型"><a href="#6-3-训练模型" class="headerlink" title="6.3 训练模型"></a>6.3 训练模型</h2><div class="hljs"><pre><code class="hljs Python">EPOCHS = <span class="hljs-number">5</span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(EPOCHS):    <span class="hljs-comment"># 在下一个epoch开始时，重置评估指标</span>    train_loss.reset_states()    train_accuracy.reset_states()    test_loss.reset_states()    test_accuracy.reset_states()    <span class="hljs-keyword">for</span> images, labels <span class="hljs-keyword">in</span> train_dataset:        train_step(images, labels)    <span class="hljs-keyword">for</span> images, labels <span class="hljs-keyword">in</span> test_dataset:        test_step(images, labels)    template = <span class="hljs-string">'Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;'</span>    print(template.format(epoch + <span class="hljs-number">1</span>,                          train_loss.result(),                          train_accuracy.result() * <span class="hljs-number">100</span>,                          test_loss.result(),                          test_accuracy.result() * <span class="hljs-number">100</span>                         ))</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/10/CgLaY.png" srcset="/img/loading.gif" alt="train"></p>]]></content>
    
    
    <categories>
      
      <category>TensorFlow2学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Python</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[TF2]10. TensorBoard</title>
    <link href="/2020/TF2-10-TensorBoard/"/>
    <url>/2020/TF2-10-TensorBoard/</url>
    
    <content type="html"><![CDATA[<div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfprint(tf.__version__)<span class="hljs-comment"># 2.1.1</span></code></pre></div><h1 id="1-TensorBoard参数"><a href="#1-TensorBoard参数" class="headerlink" title="1. TensorBoard参数"></a>1. TensorBoard参数</h1><ul><li>log_dir：保存TensorBoard要解析的日志文件的目录的路径。</li><li>histogram_freq：频率（在epoch中），计算模型层的激活和权重直方图。如果设置为0，则不会计算直方图。必须为直方图可视化指定验证：数据（或拆分）。</li><li>write_graph：是否在TensorBoard中可视化图像。当write_graph设置为True时，日志文件可能会变得非常大。</li><li>write_images：是否在TensorBoard中编写模型权重以显示为图像。</li><li>update_freq：‘batch’或’epoch’或整数。使用‘batch’时，在每个batch后将损失和指标(评估函数)写入TensorBoard。这同样适用’epoch’。如果使用整数，比方说1000，回调将会在每1000个样本后将指标和损失写入TensorBoard。请注意，过于频繁地写入TensorBoard会降低您的训练速度。</li><li>profile_batch：分析批次以采样计算特征。profile_batch必须是非负整数或正整数对的逗号分隔字符串。一对正整数表示要分析的批次范围。默认情况下，它将配置第二批。将profile_batch = 0设置为禁用性能分析。必须在TensorFlow eager模式下运行。</li><li>embeddings_freq：可视化嵌入层的频率（以epoch为单位）。如果设置为0，则嵌入将不可见。</li><li>embeddings_metadata：字典，它将层名称映射到文件名，该嵌入层的元数据保存在该文件名中。</li></ul><h1 id="2-在Keras模型训练中使用TensorBoard"><a href="#2-在Keras模型训练中使用TensorBoard" class="headerlink" title="2. 在Keras模型训练中使用TensorBoard"></a>2. 在Keras模型训练中使用TensorBoard</h1><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> absolute_import, division, print_function, unicode_literals<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense, Flatten, Conv2D<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> Model<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> datetime</code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 准备数据</span>mnist = np.load(<span class="hljs-string">'data/mnist.npz'</span>)x_train, y_train, x_test, y_test = mnist[<span class="hljs-string">'x_train'</span>],mnist[<span class="hljs-string">'y_train'</span>],mnist[<span class="hljs-string">'x_test'</span>],mnist[<span class="hljs-string">'y_test'</span>]x_train, x_test = x_train / <span class="hljs-number">255.0</span>, x_test / <span class="hljs-number">255.0</span><span class="hljs-comment"># Add a channels dimension</span>x_train = x_train[..., tf.newaxis]x_test = x_test[..., tf.newaxis]</code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 构建模型</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModel</span><span class="hljs-params">(Model)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        super(MyModel, self).__init__()        self.conv2d = Conv2D(<span class="hljs-number">32</span>, <span class="hljs-number">3</span>, activation=<span class="hljs-string">'relu'</span>)        self.flatten = Flatten()        self.dense1 = Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">'relu'</span>)        self.dense2 = Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>)<span class="hljs-meta">    @tf.function</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, x)</span>:</span>        x = self.conv2d(x)        x = self.flatten(x)        x = self.dense1(x)        x = self.dense2(x)        <span class="hljs-keyword">return</span> xmodel = MyModel()model.compile(    optimizer=<span class="hljs-string">'adam'</span>,    loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>,    metrics=[<span class="hljs-string">'accuracy'</span>])</code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 训练</span><span class="hljs-comment"># 定义TensorBoard</span>tb = tf.keras.callbacks.TensorBoard(    log_dir=<span class="hljs-string">'log_keras'</span>,    histogram_freq=<span class="hljs-number">1</span>,    profile_batch=<span class="hljs-number">100000000</span>)<span class="hljs-comment"># 训练</span>model.fit(    x=x_train,    y=y_train,    epochs=<span class="hljs-number">10</span>,    validation_data=(x_test, y_test),    callbacks=[tb])</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/02/2YT6I.png" srcset="/img/loading.gif" alt="Training"></p><h1 id="3-查看TensorBoard"><a href="#3-查看TensorBoard" class="headerlink" title="3. 查看TensorBoard"></a>3. 查看TensorBoard</h1><p>训练完成后，在命令行中进入之前指定的log_dir（即log_keras）的上一级目录中，执行命令：<br><div class="hljs"><pre><code class="hljs ada">tensorboard <span class="hljs-comment">--logdir log_keras</span></code></pre></div><br>启动了TensorBoard：</p><p><img src="https://wx2.sbimg.cn/2020/07/02/2YrWa.png" srcset="/img/loading.gif" alt="CMD"></p><p>在浏览器中打开给出的链接，可以查看TensorBoard：</p><p><img src="https://wx2.sbimg.cn/2020/07/02/2YOVR.png" srcset="/img/loading.gif" alt="TensorBoard"></p>]]></content>
    
    
    <categories>
      
      <category>TensorFlow2学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Python</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Install COCOAPI on Windows 10</title>
    <link href="/2020/Install-COCOAPI-on-Windows-10/"/>
    <url>/2020/Install-COCOAPI-on-Windows-10/</url>
    
    <content type="html"><![CDATA[<p>在Window 10上安装Python用的COCO API时，直接使用<code>pip install pycocotools</code>经常会报错，在这里总结一下遇到的各种问题。</p><h1 id="1-安装步骤"><a href="#1-安装步骤" class="headerlink" title="1. 安装步骤"></a>1. 安装步骤</h1><p>采用下载源码再编译安装的方式。在命令行中（我是在Anaconda Prompt中，且激活了目标环境）依次执行以下命令：<br><div class="hljs"><pre><code class="hljs vim">git clone http<span class="hljs-variable">s:</span>//github.<span class="hljs-keyword">com</span>/pdollar/coco.git<span class="hljs-keyword">cd</span> coco/PythonAPI<span class="hljs-keyword">python</span> setup.<span class="hljs-keyword">py</span> build_ext --inplace<span class="hljs-keyword">python</span> setup.<span class="hljs-keyword">py</span> build_ext install</code></pre></div></p><h1 id="2-报错解决"><a href="#2-报错解决" class="headerlink" title="2. 报错解决"></a>2. 报错解决</h1><h2 id="2-1-Error-Unable-to-find-vcvarsall-bat"><a href="#2-1-Error-Unable-to-find-vcvarsall-bat" class="headerlink" title="2.1 Error:Unable to find vcvarsall.bat"></a>2.1 Error:Unable to find vcvarsall.bat</h2><p>需要安装对应的Window 10 SDK。</p><p>首先右键开始，选择系统，查看自己的Window 10版本：</p><p><img src="https://wx1.sbimg.cn/2020/07/02/2Wnv1.png" srcset="/img/loading.gif" alt="Windows-1"><br><img src="https://wx1.sbimg.cn/2020/07/02/2WNHJ.png" srcset="/img/loading.gif" alt="Windows-2"></p><p>然后在 <a href="https://developer.microsoft.com/zh-cn/windows/downloads/sdk-archive/" target="_blank" rel="noopener">https://developer.microsoft.com/zh-cn/windows/downloads/sdk-archive/</a> 中找到对应版本的SDK下载。推荐下载ISO文件，避免网络问题安装失败：</p><p><img src="https://wx1.sbimg.cn/2020/07/02/2W842.png" srcset="/img/loading.gif" alt="Download-1"></p><p>下载完成后，装载ISO，运行其中的Win10SDKSetup.exe安装。</p><p>安装完成后问题解决。</p><h2 id="2-2-LINK-fatal-error-LNK1158-cannot-run-‘rc-exe’"><a href="#2-2-LINK-fatal-error-LNK1158-cannot-run-‘rc-exe’" class="headerlink" title="2.2 LINK : fatal error LNK1158: cannot run ‘rc.exe’"></a>2.2 LINK : fatal error LNK1158: cannot run ‘rc.exe’</h2><p>将<code>C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x86</code>（10.0.18362.0可能需要替换为自己系统的版本对应的文件夹，或同目录下的x86文件夹）中的rc.exe和rcdll.dll两个文件复制到<code>C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin</code>中，即可解决问题。</p><h2 id="2-3-cl-命令行-error-D8021-无效的数值参数“-Wno-cpp”"><a href="#2-3-cl-命令行-error-D8021-无效的数值参数“-Wno-cpp”" class="headerlink" title="2.3 cl: 命令行 error D8021 :无效的数值参数“/Wno-cpp”"></a>2.3 cl: 命令行 error D8021 :无效的数值参数“/Wno-cpp”</h2><p>打开 <code>coco/PythonAPI/setup.py</code> ，将参数 <code>extra_compile_args</code> 中的 <code>-Wno-cpp</code> 和 <code>-Wno-unused-function</code> 删除：<br><div class="hljs"><pre><code class="hljs prolog">ext_modules = [    <span class="hljs-symbol">Extension</span>(        <span class="hljs-string">'pycocotools._mask'</span>,        sources=[<span class="hljs-string">'../common/maskApi.c'</span>, <span class="hljs-string">'pycocotools/_mask.pyx'</span>],        include_dirs = [np.get_include(), <span class="hljs-string">'../common'</span>],        # extra_compile_args=[<span class="hljs-string">'-Wno-cpp'</span>, <span class="hljs-string">'-Wno-unused-function'</span>, <span class="hljs-string">'-std=c99'</span>],  # 修改前        extra_compile_args=[<span class="hljs-string">'-std=c99'</span>],  # 修改后    )]</code></pre></div></p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Python</tag>
      
      <tag>环境设置</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[PyTorch]1. Installation</title>
    <link href="/2020/PyTorch-1-Installation/"/>
    <url>/2020/PyTorch-1-Installation/</url>
    
    <content type="html"><![CDATA[<p>此篇笔记主要针对国内在Windows10上使用<code>pip install</code>或<code>conda install</code>安装torch时，即使使用了镜像源或VPN，也时常有中断、超时发生的情况而写。</p><p>首先在需要安装PyTorch的Python环境中执行以下代码：</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> wheel.pep425tags <span class="hljs-keyword">as</span> wprint(w.get_supported(<span class="hljs-string">"win_amd64"</span>))  <span class="hljs-comment"># 64位</span></code></pre></div><p>得到输出：</p><p><img src="https://wx1.sbimg.cn/2020/07/01/27PzN.png" srcset="/img/loading.gif" alt="whl"></p><p>内容为这个Python环境支持的whl安装包的版本。</p><p>然后前往 <a href="https://download.pytorch.org/whl/torch_stable.html" target="_blank" rel="noopener">https://download.pytorch.org/whl/torch_stable.html</a> ，寻找需要安装的库。</p><p>选择时需要注意以下几点：</p><ul><li>库的版本：在库名后标出，如torch-1.5.1。选择自己需要的版本。</li><li>CPU/GPU版：在文件名开头，CPU开头即为CPU，GPU开头为CUDA版本，如cu101。根据自己电脑的CUDA版本选择。</li><li>whl版本：在文件名结尾，如cp37-cp37m-win_amd64。根据上一步的输出选择（一般选择环境能支持的最高的版本）。</li></ul><p>这里选择了GPU版、CUDA 10.1、cp37-cp37m-win_amd64的torch 1.5.1和torchvision 0.6.1：<br><img src="https://wx2.sbimg.cn/2020/07/01/279un.png" srcset="/img/loading.gif" alt="torch"><br><img src="https://wx2.sbimg.cn/2020/07/01/27ApM.png" srcset="/img/loading.gif" alt="torchvision"></p><p>可以使用IDM、迅雷等下载工具下载。完成后，使用<code>pip install 文件路径</code>的方式安装对应的库。</p><p><img src="https://wx1.sbimg.cn/2020/07/01/27Bga.png" srcset="/img/loading.gif" alt="Install"></p><p>如果有依赖库不满足且直接使用pip安装有困难，可以参考 <a href="https://aiart.red/2020/pip-tips/" target="_blank" rel="noopener">https://aiart.red/2020/pip-tips/</a> 中的方法单独安装。</p><p>安装CUDA和CUDNN的方法在 <a href="https://aiart.red/2020/TF2-1-Installation/" target="_blank" rel="noopener">https://aiart.red/2020/TF2-1-Installation/</a> 中。</p><p>安装完成后测试torch、CUDA、CUDNN是否正常：</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch, torchvision<span class="hljs-keyword">from</span> torch.backends <span class="hljs-keyword">import</span> cudnnprint(<span class="hljs-string">'torch version:       '</span>, torch.__version__)print(<span class="hljs-string">'torchvision version: '</span>, torchvision.__version__)print(<span class="hljs-string">'Support CUDA:        '</span>, torch.cuda.is_available())print(<span class="hljs-string">'Support CUDNN:       '</span>, cudnn.is_acceptable(torch.tensor([<span class="hljs-number">1.0</span>])))  <span class="hljs-comment"># 此函数需要一个tensor作为参数</span></code></pre></div><p><img src="https://wx2.sbimg.cn/2020/09/10/9qgOe.png" srcset="/img/loading.gif" alt="Check"></p>]]></content>
    
    
    <categories>
      
      <category>PyTorch学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Python</tag>
      
      <tag>PyTorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[TF2]9. Custom Metric Function</title>
    <link href="/2020/TF2-9-Custom-Metric-Function/"/>
    <url>/2020/TF2-9-Custom-Metric-Function/</url>
    
    <content type="html"><![CDATA[<div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfprint(tf.__version__)<span class="hljs-comment"># 2.1.1</span></code></pre></div><h1 id="1-常用评估函数"><a href="#1-常用评估函数" class="headerlink" title="1. 常用评估函数"></a>1. 常用评估函数</h1><p>常用于回归的评估函数：</p><ul><li>tf.keras.metrics.MeanAbsoluteError （平方差误差，用于回归，可以简写为MSE，函数形式为mse）</li><li>tf.keras.metrics.MeanAbsoluteError (绝对值误差，用于回归，可以简写为MAE，函数形式为mae)</li><li>tf.keras.metrics.MeanAbsolutePercentageError (平均百分比误差，用于回归，可以简写为MAPE，函数形式为mape)</li><li>tf.keras.metrics.RootMeanSquaredError (均方根误差，用于回归)</li></ul><p>常用于分类的评估函数：</p><ul><li>tf.keras.metrics.Accuracy (准确率，用于分类，可以用字符串”Accuracy”表示，Accuracy=(TP+TN)/(TP+TN+FP+FN)，要求y_true和y_pred都为类别序号编码)</li><li>tf.keras.metrics.AUC (ROC曲线(TPR vs FPR)下的面积，用于二分类，直观解释为随机抽取一个正样本和一个负样本，正样本的预测值大于负样本的概率)</li><li>tf.keras.metrics.Precision (精确率，用于二分类，Precision = TP/(TP+FP))</li><li>tf.keras.metrics.Recall (召回率，用于二分类，Recall = TP/(TP+FN))</li><li>tf.keras.metrics.TopKCategoricalAccuracy(多分类TopK准确率，要求y_true(label)为onehot编码形式)</li></ul><h1 id="2-自定义评估函数"><a href="#2-自定义评估函数" class="headerlink" title="2. 自定义评估函数"></a>2. 自定义评估函数</h1><p>在训练mnist数据集的过程中，使用自定义的评估函数。</p><h1 id="2-1-准备数据"><a href="#2-1-准备数据" class="headerlink" title="2.1 准备数据"></a>2.1 准备数据</h1><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> pltmnist = np.load(<span class="hljs-string">'./data/mnist.npz'</span>)x_train, y_train, x_test, y_test = mnist[<span class="hljs-string">'x_train'</span>] / <span class="hljs-number">255.0</span>, mnist[<span class="hljs-string">'y_train'</span>], mnist[<span class="hljs-string">'x_test'</span>] / <span class="hljs-number">255.0</span>, mnist[<span class="hljs-string">'y_test'</span>]print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/01/2zsLV.png" srcset="/img/loading.gif" alt="Shape-1"></p><div class="hljs"><pre><code class="hljs Python">fig, ax = plt.subplots(    nrows=<span class="hljs-number">2</span>,    ncols=<span class="hljs-number">5</span>,    sharex=<span class="hljs-literal">True</span>,    sharey=<span class="hljs-literal">True</span>, )ax = ax.flatten()<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):    img = x_train[y_train == i][<span class="hljs-number">0</span>].reshape(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)    ax[i].imshow(img, cmap=<span class="hljs-string">'Greys'</span>, interpolation=<span class="hljs-string">'nearest'</span>)ax[<span class="hljs-number">0</span>].set_xticks([])ax[<span class="hljs-number">0</span>].set_yticks([])plt.tight_layout()plt.show()</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/01/2zhCA.png" srcset="/img/loading.gif" alt="Plot"></p><div class="hljs"><pre><code class="hljs Python">x_train = x_train[..., tf.newaxis]x_test = x_test[..., tf.newaxis]train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(<span class="hljs-number">10000</span>).batch(<span class="hljs-number">32</span>)test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="hljs-number">32</span>)print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/01/2zcw4.png" srcset="/img/loading.gif" alt="Shape-2"></p><h1 id="2-2-自定义评估函数"><a href="#2-2-自定义评估函数" class="headerlink" title="2.2 自定义评估函数"></a>2.2 自定义评估函数</h1><p>基于类的方式自定义评估函数，需要继承<code>tf.keras.metrics.Metric</code>类，并在其中重写以下方法：</p><ul><li><code>__init__()</code>：初始化，通过使用<code>add_weigt()</code>方法添加状态变量。</li><li><code>update_state()</code>：对状态变量进行更新。</li><li><code>result()</code>：根据状态变量计算并返回指标值。</li><li><code>reset_states()</code>：重置状态变量。</li></ul><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 类方式</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Accuracy_cls</span><span class="hljs-params">(tf.keras.metrics.Metric)</span>:</span>    <span class="hljs-comment"># 初始化</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, name=<span class="hljs-string">'SparseCategoricalAccuracy_cls'</span>, **kwargs)</span>:</span>        super(Accuracy_cls, self).__init__()        self.total = self.add_weight(name=<span class="hljs-string">'total'</span>, dtype=tf.int32, initializer=tf.zeros_initializer())  <span class="hljs-comment"># 总样本个数</span>        self.count = self.add_weight(name=<span class="hljs-string">'count'</span>, dtype=tf.int32, initializer=tf.zeros_initializer())  <span class="hljs-comment"># 预测正确个数</span>    <span class="hljs-comment"># 更新状态变量</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_state</span><span class="hljs-params">(self, y_true, y_pred, sample_weight=None)</span>:</span>        y_true = tf.cast(y_true, tf.int32)        values = tf.equal(y_true, tf.argmax(y_pred, axis=<span class="hljs-number">-1</span>, output_type=tf.int32))  <span class="hljs-comment"># 返回一个布尔型tensor，相等位置为True，反之为False</span>        values = tf.cast(values, tf.int32)  <span class="hljs-comment"># 转为0和1，求和即为正确个数</span>        self.total.assign_add(tf.shape(y_true)[<span class="hljs-number">0</span>])  <span class="hljs-comment"># 总样本个数</span>        self.count.assign_add(tf.reduce_sum(values))  <span class="hljs-comment"># 预测正确的个数</span>    <span class="hljs-comment"># 返回评价指标</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">result</span><span class="hljs-params">(self)</span>:</span>        acc = self.count / self.total        <span class="hljs-keyword">return</span> acc    <span class="hljs-comment"># 重置状态变量，此方法会在每个epoch开始时执行</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reset_states</span><span class="hljs-params">(self)</span>:</span>        self.total.assign(<span class="hljs-number">0</span>)        self.count.assign(<span class="hljs-number">0</span>)</code></pre></div><h1 id="2-3-训练模型"><a href="#2-3-训练模型" class="headerlink" title="2.3 训练模型"></a>2.3 训练模型</h1><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Conv2D, Flatten, Dense<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModel</span><span class="hljs-params">(tf.keras.Model)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        super(MyModel, self).__init__()        self.conv  =Conv2D(<span class="hljs-number">32</span>, <span class="hljs-number">3</span>, activation=<span class="hljs-string">'relu'</span>)        self.flatten = Flatten()        self.dense1 = Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">'relu'</span>)        self.dense2 = Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, x)</span>:</span>        x = self.conv(x)        x = self.flatten(x)        x = self.dense1(x)        x = self.dense2(x)        <span class="hljs-keyword">return</span> x</code></pre></div><div class="hljs"><pre><code class="hljs Python">model = MyModel()model.compile(    optimizer=tf.keras.optimizers.Adam(<span class="hljs-number">0.001</span>),    loss=tf.keras.losses.CategoricalCrossentropy(),    metrics=[Accuracy_cls()])model.fit(train_ds, epochs=<span class="hljs-number">5</span>, validation_data=test_ds)</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/07/01/2zdlY.png" srcset="/img/loading.gif" alt="Train"></p>]]></content>
    
    
    <categories>
      
      <category>TensorFlow2学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Python</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[TF2]8. Custom Loss Function</title>
    <link href="/2020/TF2-8-Custom-Loss-Function/"/>
    <url>/2020/TF2-8-Custom-Loss-Function/</url>
    
    <content type="html"><![CDATA[<div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfprint(tf.__version__)<span class="hljs-comment"># 2.1.1</span></code></pre></div><h1 id="1-常用损失函数"><a href="#1-常用损失函数" class="headerlink" title="1. 常用损失函数"></a>1. 常用损失函数</h1><ul><li>mean_square_error：平方差误差，一般用于回归问题。</li><li>binary_crossentropy：二元交叉熵，用于二元分类问题。</li><li>categorical_crossentropy：类别交叉熵，用于<strong>label为onehot编码</strong>的多分类问题。</li><li>sparse_categorical_crossentropy：稀疏类别交叉熵，用于<strong>label为序号编码</strong>的多分类问题。</li></ul><h1 id="2-自定义损失函数"><a href="#2-自定义损失函数" class="headerlink" title="2. 自定义损失函数"></a>2. 自定义损失函数</h1><p>在训练mnist数据集的过程中，使用自定义的FocalLoss损失函数。</p><h2 id="2-1-准备数据"><a href="#2-1-准备数据" class="headerlink" title="2.1 准备数据"></a>2.1 准备数据</h2><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> pltmnist = np.load(<span class="hljs-string">'./data/mnist.npz'</span>)x_train, y_train, x_test, y_test = mnist[<span class="hljs-string">'x_train'</span>] / <span class="hljs-number">255.0</span>, mnist[<span class="hljs-string">'y_train'</span>], mnist[<span class="hljs-string">'x_test'</span>] / <span class="hljs-number">255.0</span>, mnist[<span class="hljs-string">'y_test'</span>]print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/30/2uHC6.png" srcset="/img/loading.gif" alt="Shape-1"></p><p>可以看到，训练集中有60000条数据，每条数据是一个28*28的图像，对应一个标签（0-9）；测试集有10000条数据。</p><div class="hljs"><pre><code class="hljs Python">fig, ax = plt.subplots(    nrows=<span class="hljs-number">2</span>,    ncols=<span class="hljs-number">5</span>,    sharex=<span class="hljs-literal">True</span>,    sharey=<span class="hljs-literal">True</span>, )ax = ax.flatten()<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):    img = x_train[y_train == i][<span class="hljs-number">0</span>].reshape(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)    ax[i].imshow(img, cmap=<span class="hljs-string">'Greys'</span>, interpolation=<span class="hljs-string">'nearest'</span>)ax[<span class="hljs-number">0</span>].set_xticks([])ax[<span class="hljs-number">0</span>].set_yticks([])plt.tight_layout()plt.show()</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/30/2uKLO.png" srcset="/img/loading.gif" alt="Plot"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 加入新的轴</span>x_train = x_train[..., tf.newaxis]x_test = x_test[..., tf.newaxis]<span class="hljs-comment"># 将标签转换为独热变量</span>y_train = tf.one_hot(y_train,depth=<span class="hljs-number">10</span>)y_test = tf.one_hot(y_test,depth=<span class="hljs-number">10</span>)<span class="hljs-comment"># 建立Dataset</span>train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(<span class="hljs-number">10000</span>).batch(<span class="hljs-number">32</span>)test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="hljs-number">32</span>)print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/30/2ut9A.png" srcset="/img/loading.gif" alt="Shape-2"></p><h2 id="2-2-自定义FocalLoss损失函数"><a href="#2-2-自定义FocalLoss损失函数" class="headerlink" title="2.2 自定义FocalLoss损失函数"></a>2.2 自定义FocalLoss损失函数</h2><p>多分类的FocalLoss损失函数：</p><script type="math/tex; mode=display">FL(p_t) = \sum_{c=1}^{m} -(1-p_t)^{\gamma} * y_c * \log(p_t)</script><p>使用类的方式自定义损失函数，需继承<code>tf.keras.losses.Loss</code>类：<br><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 类方式</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FocalLoss_cls</span><span class="hljs-params">(tf.keras.losses.Loss)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, gamma=<span class="hljs-number">2.0</span>, alpha=<span class="hljs-number">0.25</span>)</span>:</span>        self.gamma = gamma        self.alpha = alpha        super(FocalLoss_cls, self).__init__()    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, y_true, y_pred)</span>:</span>        y_pred = tf.nn.softmax(y_pred, axis=<span class="hljs-number">-1</span>)  <span class="hljs-comment"># 转换为概率</span>        epsilon = tf.keras.backend.epsilon()  <span class="hljs-comment"># 防止值为0导致log计算出错</span>        y_pred = tf.clip_by_value(y_pred, epsilon, <span class="hljs-number">1.0</span>)  <span class="hljs-comment"># 值的下限为epsilon，上限为1</span>        y_true = tf.cast(y_true, tf.float32)        loss = -y_true * tf.math.pow(<span class="hljs-number">1</span> - y_pred, self.gamma) * tf.math.log(y_pred)        loss = tf.math.reduce_sum(loss)        <span class="hljs-keyword">return</span> loss</code></pre></div></p><p>使用函数方式自定义损失函数：<br><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 函数方式</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">FocalLoss_func</span><span class="hljs-params">(gamma=<span class="hljs-number">2.0</span>, alpha=<span class="hljs-number">0.25</span>)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">focal_loss_fixed</span><span class="hljs-params">(y_true, y_pred)</span>:</span>        y_pred = tf.nn.softmax(y_pred, axis=<span class="hljs-number">-1</span>)  <span class="hljs-comment"># 转换为概率</span>        epsilon = tf.keras.backend.epsilon()  <span class="hljs-comment"># 防止值为0导致log计算出错</span>        y_pred = tf.clip_by_value(y_pred, epsilon, <span class="hljs-number">1.0</span>)  <span class="hljs-comment"># 值的下限为epsilon，上限为1</span>        y_true = tf.cast(y_true, tf.float32)        loss = -y_true * tf.math.pow(<span class="hljs-number">1</span> - y_pred, gamma) * tf.math.log(y_pred)        loss = tf.math.reduce_sum(loss)        <span class="hljs-keyword">return</span> loss    <span class="hljs-keyword">return</span> focal_loss_fixed</code></pre></div></p><h1 id="3-训练模型"><a href="#3-训练模型" class="headerlink" title="3. 训练模型"></a>3. 训练模型</h1><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Conv2D, Flatten, Dense<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModel</span><span class="hljs-params">(tf.keras.Model)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        super(MyModel, self).__init__()        self.conv  =Conv2D(<span class="hljs-number">32</span>, <span class="hljs-number">3</span>, activation=<span class="hljs-string">'relu'</span>)        self.flatten = Flatten()        self.dense1 = Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">'relu'</span>)        self.dense2 = Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, x)</span>:</span>        x = self.conv(x)        x = self.flatten(x)        x = self.dense1(x)        x = self.dense2(x)        <span class="hljs-keyword">return</span> x</code></pre></div><div class="hljs"><pre><code class="hljs Python">model = MyModel()model.compile(    optimizer=tf.keras.optimizers.Adam(<span class="hljs-number">0.001</span>),    loss=FocalLoss_cls(gamma=<span class="hljs-number">2.0</span>, alpha=<span class="hljs-number">0.25</span>),    metrics=[tf.keras.metrics.CategoricalAccuracy()])model.fit(train_ds, epochs=<span class="hljs-number">5</span>, validation_data=test_ds)</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/30/2uwtV.png" srcset="/img/loading.gif" alt="Train-1"></p><div class="hljs"><pre><code class="hljs Python">model = MyModel()model.compile(    optimizer=tf.keras.optimizers.Adam(<span class="hljs-number">0.001</span>),    loss=FocalLoss_func(gamma=<span class="hljs-number">2.0</span>, alpha=<span class="hljs-number">0.25</span>),    metrics=[tf.keras.metrics.CategoricalAccuracy()])model.fit(train_ds, epochs=<span class="hljs-number">5</span>, validation_data=test_ds)</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/30/2uzl7.png" srcset="/img/loading.gif" alt="Train-2"></p>]]></content>
    
    
    <categories>
      
      <category>TensorFlow2学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Python</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Grammer Analyzer Based On Python and Qt</title>
    <link href="/2020/Grammer-Analyzer/"/>
    <url>/2020/Grammer-Analyzer/</url>
    
    <content type="html"><![CDATA[<p>编译原理课程的大作业，要求为实现一个带有图形界面的语法分析器，能够求First集、Follow集、LL分析表和LR分析表。</p><p>项目地址：<a href="https://github.com/LiShaoyu5/LL-LR-Parser" target="_blank" rel="noopener">https://github.com/LiShaoyu5/LL-LR-Parser</a></p><p>测试不多，还有很多问题待修改。</p><h1 id="1-环境要求"><a href="#1-环境要求" class="headerlink" title="1. 环境要求"></a>1. 环境要求</h1><p>开发环境：Python 3.7.7, PyQt 5.13.0</p><p>使用PyInstaller 3.6打包为可执行程序。</p><h1 id="2-使用说明"><a href="#2-使用说明" class="headerlink" title="2. 使用说明"></a>2. 使用说明</h1><h2 id="2-1-可执行程序"><a href="#2-1-可执行程序" class="headerlink" title="2.1 可执行程序"></a>2.1 可执行程序</h2><p>由于单个文件25MB的大小限制，可执行程序没有上传到Github。在源码目录下，具有上节环境的命令行中执行命令：</p><div class="hljs"><pre><code class="hljs reasonml">pyinstaller -F -w <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">MainWindow</span>.</span></span>py</code></pre></div><p>生成可执行程序。</p><h2 id="2-2-输入说明"><a href="#2-2-输入说明" class="headerlink" title="2.2 输入说明"></a>2.2 输入说明</h2><p>一条规范的推导式示例：A-&gt;c|Bc|ε</p><ul><li>推出符号为-&gt;，空符号为ε，候选式分隔符为|。</li><li>推导式中不应包含空格。</li><li>每条推导式占一行。</li><li>符号集（包括终结符和非终结符）中不应包含#、-、&gt;、|、’、”、\、·，符号集只接受单个字符，如E’会被识别为E和’。</li><li>同一非终结符可推出的候选式不应有重复。</li><li>同一非终结符可推出的候选式应写在同一行。错误的写法： A-&gt;a A-&gt;b；正确的写法：A-&gt;a|b。</li><li>此条不影响程序运行，但推荐使用大写字母表示非终结符、小写字母或数字表示终结符。</li></ul><h1 id="3-运行截图"><a href="#3-运行截图" class="headerlink" title="3. 运行截图"></a>3. 运行截图</h1><p><img src="https://wx1.sbimg.cn/2020/06/26/2ApXM.png" srcset="/img/loading.gif" alt="test1"></p><p><img src="https://wx1.sbimg.cn/2020/06/26/2AXnh.png" srcset="/img/loading.gif" alt="test2"></p><p><img src="https://wx1.sbimg.cn/2020/06/26/2AMLn.png" srcset="/img/loading.gif" alt="test3"></p><p><img src="https://wx1.sbimg.cn/2020/06/26/2Akca.png" srcset="/img/loading.gif" alt="test4"></p><h1 id="4-源代码"><a href="#4-源代码" class="headerlink" title="4. 源代码"></a>4. 源代码</h1><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># MainWindow.py</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-keyword">import</span> os<span class="hljs-keyword">import</span> sys<span class="hljs-keyword">from</span> PyQt5 <span class="hljs-keyword">import</span> QtCore, QtGui, QtWidgets<span class="hljs-keyword">from</span> PyQt5.QtWidgets <span class="hljs-keyword">import</span> QFileDialog, QTableWidgetItem<span class="hljs-comment"># 状态集</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">State</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, root, content, isroot, parent, idx)</span>:</span>        self.root = root  <span class="hljs-comment"># 基本推导式集</span>        self.content = content  <span class="hljs-comment"># 此状态内的推导式集</span>        self.isroot = isroot  <span class="hljs-comment"># 是否为IO</span>        self.parent = parent  <span class="hljs-comment"># 父节点</span>        self.next = &#123;&#125;  <span class="hljs-comment"># 子节点</span>        self.idx = idx  <span class="hljs-comment"># 状态集编号</span>        self.VN = []  <span class="hljs-comment"># 非终结符集</span>        self.Symbol = []  <span class="hljs-comment"># 符号集</span>        self.EndNum = &#123;&#125;  <span class="hljs-comment"># 文法中已推导至结束的式子数和式子总数</span>        <span class="hljs-keyword">if</span> root:            self.GetSymbol()            self.Generate()            self.FindNext()    <span class="hljs-comment"># 自己是否为终结态</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">End</span><span class="hljs-params">(self)</span>:</span>        cnt = <span class="hljs-number">0</span>        <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> self.content:            <span class="hljs-keyword">if</span> c[<span class="hljs-number">-1</span>] == <span class="hljs-string">'·'</span>:                cnt += <span class="hljs-number">1</span>        self.EndNum = &#123;<span class="hljs-string">'end'</span>: cnt, <span class="hljs-string">'all'</span>: len(self.content)&#125;    <span class="hljs-comment"># 得到符号集</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">GetSymbol</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-comment"># 非终结符</span>        <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> self.root:            self.VN.append(c[<span class="hljs-number">0</span>])        self.VN = list(set(self.VN))        <span class="hljs-comment"># 所有符号</span>        <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> self.root:            <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> c:                <span class="hljs-keyword">if</span> r <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.Symbol <span class="hljs-keyword">and</span> r <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">'·'</span>, <span class="hljs-string">'-'</span>, <span class="hljs-string">'&gt;'</span>]:                    self.Symbol.append(r)    <span class="hljs-comment"># 生成此状态集中的推导式</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Generate</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-comment"># 根节点内的推导式集就是基本推导式集</span>        <span class="hljs-keyword">if</span> self.isroot:            self.content = self.root        <span class="hljs-comment"># 非根节点的推导式集</span>        <span class="hljs-keyword">else</span>:            new_content = []            <span class="hljs-comment"># 将父节点传来的推导式中·后移一位</span>            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> self.content:                c = c.split(<span class="hljs-string">'·'</span>)                c = c[<span class="hljs-number">0</span>] + c[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>] + <span class="hljs-string">'·'</span> + c[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>:]                new_content.append(c)            <span class="hljs-comment"># 若存在·VN的形式，则将所有VN-&gt;*的推导式加入</span>            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> new_content:                <span class="hljs-keyword">if</span> c.index(<span class="hljs-string">'·'</span>) == len(c) - <span class="hljs-number">2</span> <span class="hljs-keyword">and</span> c[<span class="hljs-number">-1</span>] <span class="hljs-keyword">in</span> self.VN:                    <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> self.root:                        <span class="hljs-keyword">if</span> r[<span class="hljs-number">0</span>] == c[<span class="hljs-number">-1</span>]:                            new_content.append(r)            new_content = list(set(new_content))            self.content = new_content    <span class="hljs-comment"># 得到每个符号对应的下一个状态中的推导式</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">FindNext</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> self.Symbol:            self.next[s] = []        <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> self.content:            pos = c.index(<span class="hljs-string">'·'</span>)            <span class="hljs-keyword">if</span> pos != len(c) - <span class="hljs-number">1</span>:                <span class="hljs-comment"># ·之后符号对应的next里加入这个式子</span>                self.next[c[pos + <span class="hljs-number">1</span>]].append(c)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Ui_MainWindow</span><span class="hljs-params">(object)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">setupUi</span><span class="hljs-params">(self, MainWindow)</span>:</span>        MainWindow.setObjectName(<span class="hljs-string">"Analyzer"</span>)        MainWindow.resize(<span class="hljs-number">1200</span>, <span class="hljs-number">750</span>)        MainWindow.setFixedSize(<span class="hljs-number">1200</span>, <span class="hljs-number">750</span>)        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Fixed, QtWidgets.QSizePolicy.Fixed)        sizePolicy.setHorizontalStretch(<span class="hljs-number">0</span>)        sizePolicy.setVerticalStretch(<span class="hljs-number">0</span>)        sizePolicy.setHeightForWidth(MainWindow.sizePolicy().hasHeightForWidth())        MainWindow.setSizePolicy(sizePolicy)        font = QtGui.QFont()        font.setFamily(<span class="hljs-string">"微软雅黑"</span>)        font.setPointSize(<span class="hljs-number">12</span>)        MainWindow.setFont(font)        self.Base = QtWidgets.QWidget(MainWindow)        self.Base.setObjectName(<span class="hljs-string">"Base"</span>)        self.InputArea = QtWidgets.QTextEdit(self.Base)        self.InputArea.setGeometry(QtCore.QRect(<span class="hljs-number">50</span>, <span class="hljs-number">40</span>, <span class="hljs-number">520</span>, <span class="hljs-number">300</span>))        self.InputArea.setObjectName(<span class="hljs-string">"InputArea"</span>)        self.FirstSet = QtWidgets.QTextEdit(self.Base)        self.FirstSet.setGeometry(QtCore.QRect(<span class="hljs-number">50</span>, <span class="hljs-number">375</span>, <span class="hljs-number">520</span>, <span class="hljs-number">150</span>))        self.FirstSet.setObjectName(<span class="hljs-string">"FirstSet"</span>)        self.FollowSet = QtWidgets.QTextEdit(self.Base)        self.FollowSet.setGeometry(QtCore.QRect(<span class="hljs-number">50</span>, <span class="hljs-number">560</span>, <span class="hljs-number">520</span>, <span class="hljs-number">150</span>))        self.FollowSet.setObjectName(<span class="hljs-string">"FollowSet"</span>)        self.AnalyticalTable = QtWidgets.QTableWidget(self.Base)        self.AnalyticalTable.setGeometry(QtCore.QRect(<span class="hljs-number">630</span>, <span class="hljs-number">40</span>, <span class="hljs-number">520</span>, <span class="hljs-number">485</span>))        self.AnalyticalTable.setObjectName(<span class="hljs-string">"AnalyticalTable"</span>)        self.AnalyticalTable.setColumnCount(<span class="hljs-number">50</span>)        self.AnalyticalTable.setRowCount(<span class="hljs-number">50</span>)        self.RunButton = QtWidgets.QPushButton(self.Base)        self.RunButton.setGeometry(QtCore.QRect(<span class="hljs-number">670</span>, <span class="hljs-number">570</span>, <span class="hljs-number">170</span>, <span class="hljs-number">45</span>))        self.RunButton.setObjectName(<span class="hljs-string">"RunButton"</span>)        self.ClearButton = QtWidgets.QPushButton(self.Base)        self.ClearButton.setGeometry(QtCore.QRect(<span class="hljs-number">940</span>, <span class="hljs-number">570</span>, <span class="hljs-number">170</span>, <span class="hljs-number">45</span>))        self.ClearButton.setObjectName(<span class="hljs-string">"ClearButton"</span>)        self.HelpButton = QtWidgets.QPushButton(self.Base)        self.HelpButton.setGeometry(QtCore.QRect(<span class="hljs-number">940</span>, <span class="hljs-number">655</span>, <span class="hljs-number">170</span>, <span class="hljs-number">45</span>))        self.HelpButton.setObjectName(<span class="hljs-string">"HelpButton"</span>)        self.OpenButton = QtWidgets.QPushButton(self.Base)        self.OpenButton.setGeometry(QtCore.QRect(<span class="hljs-number">670</span>, <span class="hljs-number">655</span>, <span class="hljs-number">170</span>, <span class="hljs-number">45</span>))        self.OpenButton.setObjectName(<span class="hljs-string">"OpenButton"</span>)        self.label_1 = QtWidgets.QLabel(self.Base)        self.label_1.setGeometry(QtCore.QRect(<span class="hljs-number">50</span>, <span class="hljs-number">15</span>, <span class="hljs-number">121</span>, <span class="hljs-number">21</span>))        self.label_1.setObjectName(<span class="hljs-string">"label_1"</span>)        self.label_2 = QtWidgets.QLabel(self.Base)        self.label_2.setGeometry(QtCore.QRect(<span class="hljs-number">50</span>, <span class="hljs-number">350</span>, <span class="hljs-number">121</span>, <span class="hljs-number">21</span>))        self.label_2.setObjectName(<span class="hljs-string">"label_2"</span>)        self.label_3 = QtWidgets.QLabel(self.Base)        self.label_3.setGeometry(QtCore.QRect(<span class="hljs-number">50</span>, <span class="hljs-number">535</span>, <span class="hljs-number">121</span>, <span class="hljs-number">21</span>))        self.label_3.setObjectName(<span class="hljs-string">"label_3"</span>)        self.label_4 = QtWidgets.QLabel(self.Base)        self.label_4.setGeometry(QtCore.QRect(<span class="hljs-number">630</span>, <span class="hljs-number">15</span>, <span class="hljs-number">121</span>, <span class="hljs-number">21</span>))        self.label_4.setObjectName(<span class="hljs-string">"label_4"</span>)        MainWindow.setCentralWidget(self.Base)        self.retranslateUi(MainWindow)        QtCore.QMetaObject.connectSlotsByName(MainWindow)        <span class="hljs-comment"># 自定义变量</span>        self.SymbolSet = []  <span class="hljs-comment"># 符号集，保存每一个符号</span>        self.Derivation = &#123;&#125;  <span class="hljs-comment"># 推导式集，以字典形式保存，key为str型左侧非终结符，value为list型右侧候选式</span>        self.First = &#123;&#125;  <span class="hljs-comment"># First集，</span>        self.Follow = &#123;&#125;  <span class="hljs-comment"># Follow集</span>        self.VT = []  <span class="hljs-comment"># 终结符集</span>        self.VN = []  <span class="hljs-comment"># 非终结符集</span>        self.e_list = [<span class="hljs-string">'ε'</span>]  <span class="hljs-comment"># 可推导出ε的符号集</span>        self.I = []  <span class="hljs-comment"># 状态集</span>        <span class="hljs-comment"># 自定义槽函数</span>        self.OpenButton.clicked.connect(self.OpenFile)        self.RunButton.clicked.connect(self.Analyze)        self.ClearButton.clicked.connect(self.Clear)        self.HelpButton.clicked.connect(self.Help)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">retranslateUi</span><span class="hljs-params">(self, MainWindow)</span>:</span>        _translate = QtCore.QCoreApplication.translate        MainWindow.setWindowTitle(_translate(<span class="hljs-string">"MainWindow"</span>, <span class="hljs-string">"Analyzer"</span>))        self.InputArea.setPlaceholderText(_translate(<span class="hljs-string">"MainWindow"</span>, <span class="hljs-string">"输入文法前请先确认符合同目录下操作手册.docx中的规范！"</span>))        self.RunButton.setText(_translate(<span class="hljs-string">"MainWindow"</span>, <span class="hljs-string">"分析"</span>))        self.ClearButton.setText(_translate(<span class="hljs-string">"MainWindow"</span>, <span class="hljs-string">"清空"</span>))        self.HelpButton.setText(_translate(<span class="hljs-string">"MainWindow"</span>, <span class="hljs-string">"帮助"</span>))        self.OpenButton.setText(_translate(<span class="hljs-string">"MainWindow"</span>, <span class="hljs-string">"从文件打开.."</span>))        self.label_1.setText(_translate(<span class="hljs-string">"MainWindow"</span>, <span class="hljs-string">"输入区"</span>))        self.label_2.setText(_translate(<span class="hljs-string">"MainWindow"</span>, <span class="hljs-string">"First集"</span>))        self.label_3.setText(_translate(<span class="hljs-string">"MainWindow"</span>, <span class="hljs-string">"Follow集"</span>))        self.label_4.setText(_translate(<span class="hljs-string">"MainWindow"</span>, <span class="hljs-string">"分析表"</span>))    <span class="hljs-comment"># 从文件读入文法</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">OpenFile</span><span class="hljs-params">(self)</span>:</span>        self.Clear()        fname = QFileDialog.getOpenFileName(self.OpenButton, <span class="hljs-string">'打开文件'</span>, <span class="hljs-string">'.'</span>)        <span class="hljs-keyword">if</span> fname[<span class="hljs-number">0</span>]:            f = open(fname[<span class="hljs-number">0</span>], <span class="hljs-string">'r'</span>, encoding=<span class="hljs-string">'utf-8'</span>)            <span class="hljs-keyword">with</span> f:                data = f.read()                self.InputArea.setText(data)    <span class="hljs-comment"># 分析文法</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Analyze</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">if</span> self.SymbolSet:            <span class="hljs-keyword">return</span>        content = self.InputArea.toPlainText()        <span class="hljs-keyword">if</span> content == <span class="hljs-string">''</span>:            self.FirstSet.setText(<span class="hljs-string">'没有输入！'</span>)            self.FollowSet.setText(<span class="hljs-string">'没有输入！'</span>)        <span class="hljs-keyword">else</span>:            data = self.InputArea.toPlainText()            <span class="hljs-comment"># 获取符号集</span>            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> data:                <span class="hljs-keyword">if</span> c != <span class="hljs-string">'|'</span> <span class="hljs-keyword">and</span> c != <span class="hljs-string">'-'</span> <span class="hljs-keyword">and</span> c != <span class="hljs-string">'&gt;'</span> <span class="hljs-keyword">and</span> c != <span class="hljs-string">'\n'</span> <span class="hljs-keyword">and</span> c != <span class="hljs-string">' '</span> <span class="hljs-keyword">and</span> c <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.SymbolSet:                    self.SymbolSet.append(c)            <span class="hljs-comment"># 按行拆分，读取推导规则，保存为字典格式</span>            data = data.split(<span class="hljs-string">'\n'</span>)            <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> data:                <span class="hljs-keyword">if</span> line == <span class="hljs-string">''</span>:                    <span class="hljs-keyword">continue</span>                line = line.split(<span class="hljs-string">'-&gt;'</span>)                self.Derivation[line[<span class="hljs-number">0</span>]] = line[<span class="hljs-number">1</span>].split(<span class="hljs-string">'|'</span>)  <span class="hljs-comment"># 左侧为key，右侧为value（list格式）</span>            <span class="hljs-comment"># 建立终结符集和非终结符集</span>            <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> self.SymbolSet:                <span class="hljs-keyword">if</span> s <span class="hljs-keyword">in</span> self.Derivation.keys():                    self.VN.append(s)                <span class="hljs-keyword">else</span>:                    self.VT.append(s)            <span class="hljs-comment"># 建立First集</span>            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> self.SymbolSet:  <span class="hljs-comment"># 初始化</span>                self.First[c] = []            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> self.SymbolSet:  <span class="hljs-comment"># 建立First集</span>                self.FindFirst(c)            <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> self.First.keys():  <span class="hljs-comment"># 去重</span>                self.First[key] = list(set(self.First[key]))            <span class="hljs-comment"># 设置文本显示，将self.First（字典）转换为字符串，添加换行，去掉空格和单引号</span>            text = <span class="hljs-string">'符号的First集：\n'</span>            <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> self.First.items():                <span class="hljs-comment"># 去掉引号，将list的[]换成集合的&#123;&#125;</span>                text += <span class="hljs-string">'First('</span> + str(item[<span class="hljs-number">0</span>]).replace(<span class="hljs-string">'\''</span>, <span class="hljs-string">''</span>) + <span class="hljs-string">'): '</span> \                        + str(item[<span class="hljs-number">1</span>]).replace(<span class="hljs-string">'\''</span>, <span class="hljs-string">''</span>).replace(<span class="hljs-string">'['</span>, <span class="hljs-string">'&#123;'</span>).replace(<span class="hljs-string">']'</span>, <span class="hljs-string">'&#125;'</span>) \                        + <span class="hljs-string">'\n'</span>            text += <span class="hljs-string">'\n候选式的First集：\n'</span>            c_list = []            <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> self.Derivation.values():                <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> value:                    <span class="hljs-keyword">if</span> c <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> c_list <span class="hljs-keyword">and</span> c != <span class="hljs-string">'ε'</span>:                        c_list.append(c)            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> c_list:                f = self.cFirst(c)                f = str(f).replace(<span class="hljs-string">'\''</span>, <span class="hljs-string">''</span>).replace(<span class="hljs-string">'['</span>, <span class="hljs-string">'&#123;'</span>).replace(<span class="hljs-string">']'</span>, <span class="hljs-string">'&#125;'</span>) + <span class="hljs-string">'\n'</span>                text += <span class="hljs-string">'First('</span> + str(c).replace(<span class="hljs-string">'\''</span>, <span class="hljs-string">''</span>) + <span class="hljs-string">'): '</span> + f            self.FirstSet.setText(text)            <span class="hljs-comment"># 统计能推出ε的符号</span>            self.FindE()            <span class="hljs-comment"># 建立Follow集</span>            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> self.VN:  <span class="hljs-comment"># Follow集只考虑非终结符</span>                self.Follow[c] = []            self.FindFollow()            <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> self.Follow.keys():  <span class="hljs-comment"># 去重</span>                self.Follow[key] = list(set(self.Follow[key]))            <span class="hljs-comment"># 设置文本显示，将self.Follow（字典）转换为字符串，添加换行，去掉空格和单引号</span>            text = <span class="hljs-string">''</span>            <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> self.Follow.items():                <span class="hljs-comment"># 去掉引号，将list的[]换成集合的&#123;&#125;</span>                text += <span class="hljs-string">'Follow('</span> + str(item[<span class="hljs-number">0</span>]).replace(<span class="hljs-string">'\''</span>, <span class="hljs-string">''</span>) + <span class="hljs-string">'): '</span> \                        + str(item[<span class="hljs-number">1</span>]).replace(<span class="hljs-string">'\''</span>, <span class="hljs-string">''</span>).replace(<span class="hljs-string">'['</span>, <span class="hljs-string">'&#123;'</span>).replace(<span class="hljs-string">']'</span>, <span class="hljs-string">'&#125;'</span>) \                        + <span class="hljs-string">'\n'</span>            self.FollowSet.setText(text)            <span class="hljs-comment"># 构建分析表</span>            self.VT.append(<span class="hljs-string">'#'</span>)            self.LL1()            self.LR0()    <span class="hljs-comment"># 寻找非终结符c的First集</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">FindFirst</span><span class="hljs-params">(self, c)</span>:</span>        <span class="hljs-comment"># 终结符的First集为自己</span>        <span class="hljs-keyword">if</span> c <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.VN:            self.First[c] = [c]            <span class="hljs-keyword">return</span> self.First[c]        <span class="hljs-comment"># 非终结符遍历其候选式的首字母：</span>        <span class="hljs-keyword">else</span>:            <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> self.Derivation[c]:                <span class="hljs-comment"># 终结符或ε直接加入左侧符号的First集中</span>                <span class="hljs-keyword">if</span> s[<span class="hljs-number">0</span>] <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.VN:                    self.First[c].append(s[<span class="hljs-number">0</span>])                <span class="hljs-comment"># 非终结符</span>                <span class="hljs-keyword">else</span>:                    <span class="hljs-comment"># 将其First集加入左侧符号的First集中</span>                    <span class="hljs-keyword">if</span> c != s[<span class="hljs-number">0</span>]:                        extend = self.FindFirst(s[<span class="hljs-number">0</span>])                        <span class="hljs-comment"># 若开头的非终结符能推出ε但此候选式长度不为1，其First集中ε不会加入左侧符号的First集中</span>                        <span class="hljs-keyword">if</span> len(s) != <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> <span class="hljs-string">'ε'</span> <span class="hljs-keyword">in</span> extend:                            extend.remove(<span class="hljs-string">'ε'</span>)                        self.First[c].extend(extend)                        <span class="hljs-comment"># 若开头的非终结符能推出ε，则将其下一个符号的First集加入左侧符号的First集中</span>                        <span class="hljs-keyword">if</span> <span class="hljs-string">'ε'</span> <span class="hljs-keyword">in</span> self.Derivation[s[<span class="hljs-number">0</span>]] <span class="hljs-keyword">and</span> len(s) != <span class="hljs-number">1</span>:                            extend = self.FindFirst(s[<span class="hljs-number">1</span>])                            self.First[c].extend(extend)            <span class="hljs-keyword">return</span> self.First[c]    <span class="hljs-comment"># 建立文法的Follow集</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">FindFollow</span><span class="hljs-params">(self)</span>:</span>        self.Follow[self.SymbolSet[<span class="hljs-number">0</span>]].append(<span class="hljs-string">'#'</span>)        <span class="hljs-comment"># while中的过程直至所有的Follow集不再增大</span>        isChange = <span class="hljs-literal">True</span>        <span class="hljs-keyword">while</span> isChange:            isChange = <span class="hljs-literal">False</span>            <span class="hljs-comment"># 遍历每一条推导式</span>            <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> self.Derivation.items():  <span class="hljs-comment"># 每条推导式</span>                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(value)):  <span class="hljs-comment"># 每条候选式value[i]</span>                    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(len(value[i])):  <span class="hljs-comment"># 候选式中的每个符号value[i][j]</span>                        <span class="hljs-keyword">if</span> value[i][j] <span class="hljs-keyword">in</span> self.VN:  <span class="hljs-comment"># 只考虑非终结符</span>                            l = len(value[i])                            <span class="hljs-comment"># a-&gt;*b或a-&gt;*bc且c-&gt;ε，将Follow(a)加入Follow(b)</span>                            <span class="hljs-comment"># value[i][j]是结尾字符</span>                            <span class="hljs-keyword">if</span> j == l - <span class="hljs-number">1</span>:                                l_o = len(self.Follow[value[i][j]])  <span class="hljs-comment"># 原本的长度</span>                                self.Follow[value[i][j]].extend(self.Follow[key])  <span class="hljs-comment"># 将Follow(key)加入Follow(value[i][j])</span>                                self.Follow[value[i][j]] = list(set(self.Follow[value[i][j]]))  <span class="hljs-comment"># 去重</span>                                <span class="hljs-keyword">if</span> len(self.Follow[value[i][j]]) != l_o:  <span class="hljs-comment"># 检查是否发生变化</span>                                    isChange = <span class="hljs-literal">True</span>                            <span class="hljs-comment"># a-&gt;*bc且c-&gt;ε，将Follow(a)加入Follow(b)</span>                            <span class="hljs-comment"># value[i][j]是倒数第二个字符，且value[i][j+1]是非终结符</span>                            <span class="hljs-keyword">elif</span> (j == l - <span class="hljs-number">2</span>) <span class="hljs-keyword">and</span> (value[i][j + <span class="hljs-number">1</span>] <span class="hljs-keyword">in</span> self.VN):                                l_o = len(self.Follow[value[i][j]])  <span class="hljs-comment"># 原本的长度</span>                                extend = self.First[value[i][j + <span class="hljs-number">1</span>]]                                <span class="hljs-keyword">if</span> <span class="hljs-string">'ε'</span> <span class="hljs-keyword">in</span> extend:                                    extend.remove(<span class="hljs-string">'ε'</span>)                                self.Follow[value[i][j]].extend(extend)  <span class="hljs-comment"># 将First(value[i][j+1])加入Follow(value[i][j])</span>                                self.Follow[value[i][j]] = list(set(self.Follow[value[i][j]]))  <span class="hljs-comment"># 去重</span>                                <span class="hljs-keyword">if</span> len(self.Follow[value[i][j]]) != l_o:  <span class="hljs-comment"># 检查是否发生变化</span>                                    isChange = <span class="hljs-literal">True</span>                                <span class="hljs-comment"># value[i][j+1]能推出ε</span>                                <span class="hljs-keyword">if</span> <span class="hljs-string">'ε'</span> <span class="hljs-keyword">in</span> self.Derivation[value[i][j + <span class="hljs-number">1</span>]]:                                    l_o = len(self.Follow[value[i][j]])  <span class="hljs-comment"># 原本的长度</span>                                    self.Follow[value[i][j]].extend(                                        self.Follow[key])  <span class="hljs-comment"># 将Follow(key)加入Follow(value[i][j])</span>                                    self.Follow[value[i][j]] = list(set(self.Follow[value[i][j]]))  <span class="hljs-comment"># 去重</span>                                    <span class="hljs-keyword">if</span> len(self.Follow[value[i][j]]) != l_o:  <span class="hljs-comment"># 检查是否发生变化</span>                                        isChange = <span class="hljs-literal">True</span>                            <span class="hljs-comment"># a-&gt;*bc*，将First(c)-&#123;ε&#125;加入Follow(b)</span>                            <span class="hljs-keyword">else</span>:                                l_o = len(self.Follow[value[i][j]])  <span class="hljs-comment"># 原本的长度</span>                                extend = self.First[value[i][j + <span class="hljs-number">1</span>]]                                <span class="hljs-keyword">if</span> <span class="hljs-string">'ε'</span> <span class="hljs-keyword">in</span> extend:                                    extend.remove(<span class="hljs-string">'ε'</span>)                                self.Follow[value[i][j]].extend(extend)  <span class="hljs-comment"># 将First(value[i][j+1])加入Follow(value[i][j])</span>                                self.Follow[value[i][j]] = list(set(self.Follow[value[i][j]]))  <span class="hljs-comment"># 去重</span>                                <span class="hljs-keyword">if</span> len(self.Follow[value[i][j]]) != l_o:  <span class="hljs-comment"># 检查是否发生变化</span>                                    isChange = <span class="hljs-literal">True</span>    <span class="hljs-comment"># 判断是否为LL(1)文法</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">isLL1</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-comment"># 遍历每个推导式</span>        <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> self.Derivation.items():            <span class="hljs-comment"># 同一个符号的候选式至多只能有一个能推出ε</span>            cnt = <span class="hljs-number">0</span>            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> value:                <span class="hljs-keyword">if</span> self.isE(c):                    cnt += <span class="hljs-number">1</span>                <span class="hljs-keyword">if</span> cnt &gt; <span class="hljs-number">1</span>:                    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>            c_e = []  <span class="hljs-comment"># 能推出ε的候选式</span>            c_none = []  <span class="hljs-comment"># 不能推出ε的候选式</span>            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> value:                <span class="hljs-comment"># 存在左递归则不是LL(1)文法</span>                <span class="hljs-keyword">if</span> key == c[<span class="hljs-number">0</span>]:                    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>                <span class="hljs-comment"># 候选式按能否推出ε分类</span>                <span class="hljs-keyword">if</span> self.isE(c):                    c_e.append(c)                <span class="hljs-keyword">else</span>:                    c_none.append(c)            <span class="hljs-comment"># 对不能推出ε的候选式，两两First交集为空</span>            <span class="hljs-keyword">if</span> len(c_none) &gt; <span class="hljs-number">1</span>:  <span class="hljs-comment"># 至少得有两个候选式</span>                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> c_none:  <span class="hljs-comment"># 两两比较</span>                    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> c_none:                        <span class="hljs-keyword">if</span> i == j:  <span class="hljs-comment"># 不能自己跟自己比</span>                            <span class="hljs-keyword">continue</span>                        <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> self.Derivation.values():  <span class="hljs-comment"># 要是同一个符号的候选式</span>                            <span class="hljs-keyword">if</span> i <span class="hljs-keyword">in</span> v <span class="hljs-keyword">and</span> j <span class="hljs-keyword">in</span> v:                                i_first = set(self.cFirst(i))                                j_first = set(self.cFirst(j))                                <span class="hljs-keyword">if</span> i_first &amp; j_first == set():  <span class="hljs-comment"># 交集为空集</span>                                    <span class="hljs-keyword">continue</span>                                <span class="hljs-keyword">else</span>:                                    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>            <span class="hljs-comment"># 对能推出ε的候选式，First(候选式)与Follow(c)交集为空</span>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> c_e:                i_first = set(self.cFirst(i))                key_first = set(self.Follow[key])                <span class="hljs-keyword">if</span> i_first &amp; key_first == set():  <span class="hljs-comment"># 交集为空集</span>                    <span class="hljs-keyword">continue</span>                <span class="hljs-keyword">else</span>:                    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>    <span class="hljs-comment"># 建立LL分析表</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">LL1</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">if</span> self.isLL1():            <span class="hljs-comment"># 表头部分</span>            self.AnalyticalTable.setItem(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, QTableWidgetItem(<span class="hljs-string">'LL Parser'</span>))            self.AnalyticalTable.setSpan(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, len(self.VT) + <span class="hljs-number">1</span>)            column = <span class="hljs-number">1</span>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> self.VT:                <span class="hljs-keyword">if</span> i != <span class="hljs-string">'ε'</span>:                    self.AnalyticalTable.setItem(<span class="hljs-number">1</span>, column, QTableWidgetItem(i))                    column += <span class="hljs-number">1</span>            row = <span class="hljs-number">2</span>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> self.VN:                self.AnalyticalTable.setItem(row, <span class="hljs-number">0</span>, QTableWidgetItem(i))                row += <span class="hljs-number">1</span>            <span class="hljs-comment"># 坐标：</span>            <span class="hljs-comment"># 行：在self.VN中的坐标+2</span>            <span class="hljs-comment"># 列：在self.VT中的坐标+1</span>            <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> self.Derivation.items():                <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> value:                    c_first = self.cFirst(c)                    <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> c_first:                        <span class="hljs-keyword">if</span> a != <span class="hljs-string">'ε'</span>:                            x = self.VN.index(key) + <span class="hljs-number">2</span>                            y = self.VT.index(a)                            <span class="hljs-keyword">if</span> <span class="hljs-string">'ε'</span> <span class="hljs-keyword">in</span> self.VT:                                <span class="hljs-keyword">if</span> y &lt; self.VT.index(<span class="hljs-string">'ε'</span>):                                    y += <span class="hljs-number">1</span>                            <span class="hljs-keyword">else</span>:                                y += <span class="hljs-number">1</span>                            content = QTableWidgetItem(key + <span class="hljs-string">'-&gt;'</span> + c)                            self.AnalyticalTable.setItem(x, y, content)                            <span class="hljs-keyword">if</span> <span class="hljs-string">'ε'</span> <span class="hljs-keyword">in</span> self.First[a]:                                <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> self.Follow[key]:                                    x = self.VN.index(key) + <span class="hljs-number">2</span>                                    y = self.VT.index(b) + <span class="hljs-number">1</span>                                    content = QTableWidgetItem(key + <span class="hljs-string">'-&gt;'</span> + c)                                    self.AnalyticalTable.setItem(x, y, content)        <span class="hljs-comment"># 不是LL(1)文法</span>        <span class="hljs-keyword">else</span>:            self.AnalyticalTable.setItem(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, QTableWidgetItem(<span class="hljs-string">'This is not a LL grammer.'</span>))            self.AnalyticalTable.setColumnWidth(<span class="hljs-number">0</span>, <span class="hljs-number">200</span>)    <span class="hljs-comment"># 建立LR分析表</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">LR0</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-comment"># 建立状态集</span>        root = []        <span class="hljs-comment"># 每个结果不是ε的推导式，在候选式前加·，放进root</span>        <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> self.Derivation.items():            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> value:                root.append(key + <span class="hljs-string">'-&gt;·'</span> + c)        idx = <span class="hljs-number">0</span>        I0 = State(root, [], <span class="hljs-literal">True</span>, <span class="hljs-literal">None</span>, idx)        idx += <span class="hljs-number">1</span>        I = [I0]  <span class="hljs-comment"># 状态集</span>        isChange = <span class="hljs-literal">True</span>        <span class="hljs-keyword">while</span> isChange:            isChange = <span class="hljs-literal">False</span>            <span class="hljs-comment"># 查看每个状态集是否还能推导</span>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> I:                <span class="hljs-comment"># 检查next中的每个对象</span>                <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> i.next.items():                    <span class="hljs-comment"># 若value为列表，则要将其转换为State</span>                    <span class="hljs-keyword">if</span> type(value) == list <span class="hljs-keyword">and</span> value != []:                        isExist = <span class="hljs-literal">False</span>                        <span class="hljs-comment"># 若value对应的State已经存在于I中，则直接建立引用</span>                        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> I:                            value_new = []                            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> value:                                c = c.split(<span class="hljs-string">'·'</span>)                                c = c[<span class="hljs-number">0</span>] + c[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>] + <span class="hljs-string">'·'</span> + c[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>:]                                value_new.append(c)                            <span class="hljs-keyword">if</span> j.content == value_new:                                i.next[key] = j                                isExist = <span class="hljs-literal">True</span>                                isChange = <span class="hljs-literal">True</span>                                <span class="hljs-keyword">break</span>                        <span class="hljs-comment"># 若不存在，则建立新的State</span>                        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> isExist:                            i.next[key] = State(root, value, <span class="hljs-literal">False</span>, i, idx)                            I.append(i.next[key])                            idx += <span class="hljs-number">1</span>                            isChange = <span class="hljs-literal">True</span>        I.insert(<span class="hljs-number">1</span>, State([], [], <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>))  <span class="hljs-comment"># 增广</span>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> I:            i.End()  <span class="hljs-comment"># 得到其中·在最后的式子的数量</span>        <span class="hljs-comment"># 表头部分</span>        row = len(self.VN) + <span class="hljs-number">6</span>        column = <span class="hljs-number">1</span>        self.AnalyticalTable.setItem(row - <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, QTableWidgetItem(<span class="hljs-string">'LR Parser'</span>))        self.AnalyticalTable.setSpan(row - <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, len(self.VT) + len(self.VN) + <span class="hljs-number">1</span>)        self.AnalyticalTable.setItem(row - <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, QTableWidgetItem(<span class="hljs-string">'Action'</span>))        self.AnalyticalTable.setSpan(row - <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, len(self.VT))        self.AnalyticalTable.setItem(row - <span class="hljs-number">2</span>, len(self.VT) + <span class="hljs-number">1</span>, QTableWidgetItem(<span class="hljs-string">'Goto'</span>))        self.AnalyticalTable.setSpan(row - <span class="hljs-number">2</span>, len(self.VT) + <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, len(self.VN))        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> self.VT:            <span class="hljs-keyword">if</span> i != <span class="hljs-string">'ε'</span>:                self.AnalyticalTable.setItem(row - <span class="hljs-number">1</span>, column, QTableWidgetItem(i))                column += <span class="hljs-number">1</span>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> self.VN:            self.AnalyticalTable.setItem(row - <span class="hljs-number">1</span>, column+<span class="hljs-number">1</span>, QTableWidgetItem(i))            column += <span class="hljs-number">1</span>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(I)):            self.AnalyticalTable.setItem(row, <span class="hljs-number">0</span>, QTableWidgetItem(str(i)))            row += <span class="hljs-number">1</span>        <span class="hljs-comment"># 填表</span>        <span class="hljs-comment"># Ia通过key到Ib，则(a, key) = sb/b，取决于key是终结符还是非终结符</span>        <span class="hljs-comment"># key∈VT, key = self.VT.index(key)+1</span>        <span class="hljs-comment"># key∈VN, key = self.VN.index(key)+1+len(self.VT)</span>        <span class="hljs-comment"># 遍历I：</span>        <span class="hljs-comment"># 若其中只有一个推导式且·在末尾，则全填rx</span>        <span class="hljs-comment"># 若有多个推导式，一部位·在末尾，则这几个式子对应的格子填rx，key为从这个式子到终结态的key</span>        isLR = <span class="hljs-literal">True</span>        <span class="hljs-comment"># sb/b</span>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(I)):            <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> I[i].next.items():                <span class="hljs-keyword">if</span> value:  <span class="hljs-comment"># 每一对Ia-&gt;Ib</span>                    x = len(self.VN) + <span class="hljs-number">6</span> + i                    <span class="hljs-comment"># key为终结符，填sb</span>                    <span class="hljs-keyword">if</span> key <span class="hljs-keyword">in</span> self.VT:                        y = self.VT.index(key) + <span class="hljs-number">1</span>                        source = <span class="hljs-string">'s'</span> + str(I.index(value))                        <span class="hljs-keyword">if</span> self.AnalyticalTable.item(x, y):                            isLR = <span class="hljs-literal">False</span>                        <span class="hljs-keyword">else</span>:                            self.AnalyticalTable.setItem(x, y, QTableWidgetItem(source))                    <span class="hljs-comment"># key为非终结符，填b</span>                    <span class="hljs-keyword">else</span>:                        y = self.VN.index(key) + <span class="hljs-number">1</span> + len(self.VT)                        source = str(I.index(value))                        <span class="hljs-keyword">if</span> self.AnalyticalTable.item(x, y):                            isLR = <span class="hljs-literal">False</span>                        <span class="hljs-keyword">else</span>:                            self.AnalyticalTable.setItem(x, y, QTableWidgetItem(source))        <span class="hljs-comment"># rb</span>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(I)):            x = len(self.VN) + <span class="hljs-number">6</span> + i            <span class="hljs-comment"># I[i]中只有一个式子且·在末尾，则在这一行的Action下全填rx，x为该式子在root对应式子的编号</span>            <span class="hljs-keyword">if</span> I[i].EndNum[<span class="hljs-string">'end'</span>] == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> I[i].EndNum[<span class="hljs-string">'all'</span>] == <span class="hljs-number">1</span>:                c = I[i].content[<span class="hljs-number">0</span>][:<span class="hljs-number">-1</span>].split(<span class="hljs-string">'-&gt;'</span>)                c = c[<span class="hljs-number">0</span>] + <span class="hljs-string">'-&gt;·'</span> + c[<span class="hljs-number">1</span>]                source = root.index(c) + <span class="hljs-number">1</span>                source = <span class="hljs-string">'r'</span> + str(source)                <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, len(self.VT)):                    <span class="hljs-keyword">if</span> self.AnalyticalTable.item(x, y):                        isLR = <span class="hljs-literal">False</span>                    <span class="hljs-keyword">else</span>:                        self.AnalyticalTable.setItem(x, y, QTableWidgetItem(source))            <span class="hljs-comment"># I[i]中一部分式子·在末尾，则在#列下填rx</span>            <span class="hljs-keyword">else</span>:                <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> I[i].content:                    <span class="hljs-keyword">if</span> c[<span class="hljs-number">-1</span>] == <span class="hljs-string">'·'</span>:                        c = c[:<span class="hljs-number">-1</span>].split(<span class="hljs-string">'-&gt;'</span>)                        c = c[<span class="hljs-number">0</span>] + <span class="hljs-string">'-&gt;·'</span> + c[<span class="hljs-number">1</span>]                        source = root.index(c) + <span class="hljs-number">1</span>                        source = <span class="hljs-string">'r'</span> + str(source)                        y = len(self.VT)                        <span class="hljs-keyword">if</span> self.AnalyticalTable.item(x, y):                            isLR = <span class="hljs-literal">False</span>                        <span class="hljs-keyword">else</span>:                            self.AnalyticalTable.setItem(x, y, QTableWidgetItem(source))        self.AnalyticalTable.setItem(len(self.VN) + <span class="hljs-number">6</span> + <span class="hljs-number">1</span>, self.VT.index(<span class="hljs-string">'#'</span>) + <span class="hljs-number">1</span>, QTableWidgetItem(<span class="hljs-string">'acc'</span>))        <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> self.Derivation.items():            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> value:                <span class="hljs-comment"># 存在左递归则不是LR文法</span>                <span class="hljs-keyword">if</span> key == c[<span class="hljs-number">0</span>]:                    isLR = <span class="hljs-literal">False</span>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> isLR:            <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> range(len(self.VN) + <span class="hljs-number">6</span>, len(self.VN) + <span class="hljs-number">7</span> + len(I)):                <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, len(self.VT) + len(self.VN) + <span class="hljs-number">1</span>):                    self.AnalyticalTable.setItem(x, y, QTableWidgetItem(<span class="hljs-string">''</span>))            self.AnalyticalTable.setItem(len(self.VN) + <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, QTableWidgetItem(<span class="hljs-string">'This is not a LR grammer.'</span>))    <span class="hljs-comment"># 判断候选式是否能推导出ε</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">isE</span><span class="hljs-params">(self, c)</span>:</span>        <span class="hljs-keyword">if</span> c == <span class="hljs-string">'ε'</span>:            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>        <span class="hljs-comment"># 有终结符存在必不可能推导出ε</span>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> c:            <span class="hljs-keyword">if</span> i <span class="hljs-keyword">in</span> self.VT:                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>        <span class="hljs-comment"># 无终结符，则遍历每个符号，若全都能推出ε，则此候选式能推出ε</span>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> c:            <span class="hljs-keyword">if</span> i <span class="hljs-keyword">in</span> self.e_list:                <span class="hljs-keyword">continue</span>            <span class="hljs-keyword">else</span>:                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>    <span class="hljs-comment"># 建立ε集</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">FindE</span><span class="hljs-params">(self)</span>:</span>        isChange = <span class="hljs-literal">True</span>        <span class="hljs-keyword">while</span> isChange:            isChange = <span class="hljs-literal">False</span>            <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> self.Derivation.items():                <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> value:                    <span class="hljs-keyword">if</span> c <span class="hljs-keyword">in</span> self.e_list <span class="hljs-keyword">and</span> key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.e_list:  <span class="hljs-comment"># 如果c可以推出ε，那key也可以</span>                        self.e_list.append(key)                        isChange = <span class="hljs-literal">True</span>    <span class="hljs-comment"># 求候选式的First集</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cFirst</span><span class="hljs-params">(self, c)</span>:</span>        <span class="hljs-comment"># 终结符开头，则其First集为这个终结符</span>        <span class="hljs-keyword">if</span> c[<span class="hljs-number">0</span>] <span class="hljs-keyword">in</span> self.VT:            <span class="hljs-keyword">return</span> [c[<span class="hljs-number">0</span>]]        <span class="hljs-comment"># 非终结符，则返回其First集</span>        <span class="hljs-keyword">else</span>:            result = self.First[c[<span class="hljs-number">0</span>]]            <span class="hljs-comment"># 若c[0]能推出ε，则还要加上其后符号的First集</span>            <span class="hljs-keyword">if</span> <span class="hljs-string">'ε'</span> <span class="hljs-keyword">in</span> self.Derivation[c[<span class="hljs-number">0</span>]] <span class="hljs-keyword">and</span> len(c) != <span class="hljs-number">1</span>:                result.extend(self.First[c[<span class="hljs-number">1</span>]])            <span class="hljs-keyword">return</span> list(set(result))    <span class="hljs-comment"># 重置为初始状态</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Clear</span><span class="hljs-params">(self)</span>:</span>        self.InputArea.clear()        self.FirstSet.clear()        self.FollowSet.clear()        self.AnalyticalTable.clear()        self.AnalyticalTable.setColumnCount(<span class="hljs-number">0</span>)        self.AnalyticalTable.setRowCount(<span class="hljs-number">0</span>)        self.AnalyticalTable.setColumnCount(<span class="hljs-number">50</span>)        self.AnalyticalTable.setRowCount(<span class="hljs-number">50</span>)        self.SymbolSet = []  <span class="hljs-comment"># 符号集，保存每一个符号</span>        self.Derivation = &#123;&#125;  <span class="hljs-comment"># 推导式集，以字典形式保存，key为str型左侧非终结符，value为list型右侧候选式</span>        self.First = &#123;&#125;  <span class="hljs-comment"># First集</span>        self.Follow = &#123;&#125;  <span class="hljs-comment"># Follow集</span>        self.VT = []  <span class="hljs-comment"># 终结符集</span>        self.VN = []  <span class="hljs-comment"># 非终结符集</span>        self.e_list = [<span class="hljs-string">'ε'</span>]  <span class="hljs-comment"># 可推导出ε的符号集</span>        self.I = []  <span class="hljs-comment"># 状态集</span>    <span class="hljs-comment"># 帮助</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Help</span><span class="hljs-params">(self)</span>:</span>        os.startfile(<span class="hljs-string">'操作手册.docx'</span>)<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:    app = QtWidgets.QApplication(sys.argv)    gui = QtWidgets.QMainWindow()    ui = Ui_MainWindow()    ui.setupUi(gui)    gui.show()    sys.exit(app.exec_())</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>Qt</tag>
      
      <tag>编译原理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[TF2]7. Custom Layer</title>
    <link href="/2020/TF2-7-Custom-Layer/"/>
    <url>/2020/TF2-7-Custom-Layer/</url>
    
    <content type="html"><![CDATA[<div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfprint(tf.__version__)<span class="hljs-comment"># 2.1.1</span></code></pre></div><p>自定义层一般依赖于TensorFlow中的<code>tf.keras.layers.Layer</code>类，通过继承它并实现：</p><ul><li><code>__init__</code>: 进行所有与输入无关的初始化，定义需要的层</li><li><code>build</code>: 定义输入张量的格式并进行其余的初始化，可以放在<code>__init__</code>中完成</li><li><code>call</code>: 定义前向传播</li></ul><p>以iris数据集为例，构建一个全连接层。</p><p>首先加载数据：</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasetsiris = datasets.load_iris()data = iris.datatarget = iris.targetprint(data.shape)print(target.shape)</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/12/7-1.png" srcset="/img/loading.gif" alt="DataShape"></p><p>对于一个线性层，其计算方式是 $y = xw + b$ 。对于iris数据集：$ x \in R^{(150, 4)} , w \in R^{(4, 1)} , b \in R^{(1)} , y \in R^{(150, 1)} $。</p><h1 id="1-自定义层的基本方法"><a href="#1-自定义层的基本方法" class="headerlink" title="1. 自定义层的基本方法"></a>1. 自定义层的基本方法</h1><h2 id="1-在-init-中使用Iitializer初始化参数"><a href="#1-在-init-中使用Iitializer初始化参数" class="headerlink" title="(1) 在__init__()中使用Iitializer初始化参数"></a>(1) 在<code>__init__()</code>中使用<code>Iitializer</code>初始化参数</h2><p><code>tf.keras.initializers.Initializer</code>类能够生成指定形状、格式、分布的Tensor。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Linear</span><span class="hljs-params">(tf.keras.layers.Layer)</span>:</span>    <span class="hljs-comment"># units为神经元个数，即每个神经元有一个input_dim*1的w</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, units=<span class="hljs-number">1</span>, input_dim=<span class="hljs-number">4</span>)</span>:</span>        super(Linear, self).__init__()        w_init = tf.random_normal_initializer()  <span class="hljs-comment"># 正态分布</span>        self.w = tf.Variable(            initial_value=w_init(shape=(input_dim, units), dtype=tf.float32),            trainable=<span class="hljs-literal">True</span>        )        b_init = tf.zeros_initializer()        self.b = tf.Variable(            initial_value=b_init(shape=(units,), dtype=tf.float32),  <span class="hljs-comment"># 每个神经元有一个偏置量b</span>            trainable=<span class="hljs-literal">True</span>        )    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, inputs)</span>:</span>        <span class="hljs-keyword">return</span> tf.matmul(inputs, self.w) + self.b  <span class="hljs-comment"># y=xw+b</span></code></pre></div><div class="hljs"><pre><code class="hljs Python">linear = Linear(units=<span class="hljs-number">1</span>, input_dim=<span class="hljs-number">4</span>)y = linear(data)print(y[:<span class="hljs-number">5</span>])  <span class="hljs-comment"># 查看前5个预测结果</span></code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/12/7-2.png" srcset="/img/loading.gif" alt="Custom1"></p><h2 id="2-在-init-中使用add-weight-初始化参数"><a href="#2-在-init-中使用add-weight-初始化参数" class="headerlink" title="(2) 在__init__()中使用add_weight()初始化参数"></a>(2) 在<code>__init__()</code>中使用<code>add_weight()</code>初始化参数</h2><p>方法基本与(1)相同，只是不需要使用单独的Initializer，而是直接在<code>add_weight()</code>的参数中指定。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Linear</span><span class="hljs-params">(tf.keras.layers.Layer)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, units=<span class="hljs-number">1</span>, input_dim=<span class="hljs-number">4</span>)</span>:</span>        super(Linear, self).__init__()        self.w = self.add_weight(            shape=(input_dim, units),            initializer=<span class="hljs-string">'random_normal'</span>,            trainable=<span class="hljs-literal">True</span>        )        self.b = self.add_weight(            shape=(units,),            initializer=<span class="hljs-string">'zeros'</span>,            trainable=<span class="hljs-literal">True</span>        )    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, inputs)</span>:</span>        <span class="hljs-keyword">return</span> tf.matmul(inputs, self.w) + self.b</code></pre></div><div class="hljs"><pre><code class="hljs Python">linear = Linear(units=<span class="hljs-number">1</span>, input_dim=<span class="hljs-number">4</span>)y = linear(data)print(y[:<span class="hljs-number">5</span>])</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/06/12/7-3.png" srcset="/img/loading.gif" alt="Custom2"></p><h2 id="3-在build-中初始化参数"><a href="#3-在build-中初始化参数" class="headerlink" title="(3) 在build()中初始化参数"></a>(3) 在<code>build()</code>中初始化参数</h2><p>使用这种方法，就不需要再在实例化层时显式地指定输入维度，而是可以从输入中读取。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Linear</span><span class="hljs-params">(tf.keras.layers.Layer)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, units=<span class="hljs-number">1</span>)</span>:</span>        super(Linear, self).__init__()        self.units = units    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build</span><span class="hljs-params">(self, input_shape)</span>:</span>        self.w = self.add_weight(            shape=(input_shape[<span class="hljs-number">-1</span>], self.units),  <span class="hljs-comment"># 从输入中读取维度</span>            initializer=<span class="hljs-string">'random_normal'</span>,            trainable=<span class="hljs-literal">True</span>        )        self.b = self.add_weight(            shape=(self.units,),            initializer=<span class="hljs-string">'zeros'</span>,            trainable=<span class="hljs-literal">True</span>        )        super(Linear, self).build(input_shape)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, inputs)</span>:</span>        <span class="hljs-keyword">return</span> tf.matmul(inputs, self.w) + self.b</code></pre></div><div class="hljs"><pre><code class="hljs Python">linear = Linear(units=<span class="hljs-number">1</span>)y = linear(data)print(y[:<span class="hljs-number">5</span>])</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/06/12/7-4.png" srcset="/img/loading.gif" alt="Custom3"></p><p>在定义层的参数时，可以通过设置参数的<code>trainable</code>属性来指定参数是否参与训练。通过<code>layer.weights</code> / <code>layer.trainable_weights</code> / <code>layer.non_trainable_weights</code>查看层中的各类参数。</p><h1 id="2-在模型中使用自定义层"><a href="#2-在模型中使用自定义层" class="headerlink" title="2. 在模型中使用自定义层"></a>2. 在模型中使用自定义层</h1><p>实际使用自定义层时，有以下注意事项：</p><p>(1) 在模型中使用自定义层时，若需要保存模型：</p><ul><li>需要在自定义层中重写<code>get_config()</code>方法，以便保存时能读取参数。</li><li>模型的参数需要命名（指定<code>name</code>参数）。</li><li>若加载模型时出现’Unknown layer’错误，需要在<code>load_model()</code>中指定参数<code>custom_object={&#39;层名&#39;: 层名}</code>。</li></ul><p>(2) 自定义层的名字不应与<code>tf.keras.layers</code>中的层同名。</p><p>(3) 在<code>__init__()</code>中添加可变参数<code>**kwargs</code>。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 准备数据</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasetsiris = datasets.load_iris()data = iris.datatarget = iris.target<span class="hljs-comment"># 打乱数据</span>concat = np.concatenate((data, target.reshape(<span class="hljs-number">150</span>, <span class="hljs-number">1</span>)), axis=<span class="hljs-number">-1</span>)  <span class="hljs-comment"># 绑定特征和对应标签</span>np.random.shuffle(concat)data = concat[:, :<span class="hljs-number">4</span>]  <span class="hljs-comment"># 前四列为特征</span>target = concat[:, <span class="hljs-number">-1</span>]  <span class="hljs-comment"># 最后一列为标签</span></code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 自定义层</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyLinear</span><span class="hljs-params">(tf.keras.layers.Layer)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, units=<span class="hljs-number">1</span>)</span>:</span>        super(MyLinear, self).__init__()        self.units = units    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build</span><span class="hljs-params">(self, input_shape)</span>:</span>        self.w = self.add_weight(            shape=(input_shape[<span class="hljs-number">-1</span>], self.units),            initializer=<span class="hljs-string">'random_normal'</span>,            trainable=<span class="hljs-literal">True</span>,            name=<span class="hljs-string">'w'</span>  <span class="hljs-comment"># 参数需指定name</span>        )        self.b = self.add_weight(            shape=(self.units,),            initializer=<span class="hljs-string">'zeros'</span>,            trainable=<span class="hljs-literal">True</span>,            name=<span class="hljs-string">'b'</span>        )        super(MyLinear, self).build(input_shape)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, inputs)</span>:</span>        <span class="hljs-keyword">return</span> tf.matmul(inputs, self.w) + self.b    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_config</span><span class="hljs-params">(self)</span>:</span>  <span class="hljs-comment"># 保存模型用</span>        config = super(MyLinear, self).get_config()        config.update(&#123;<span class="hljs-string">'units:'</span>: self.units&#125;)        <span class="hljs-keyword">return</span> config</code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 训练模型</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModel</span><span class="hljs-params">(tf.keras.Model)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, num_classes=<span class="hljs-number">3</span>)</span>:</span>        super(MyModel, self).__init__()        self.num_classes = num_classes        <span class="hljs-comment"># 使用自定义的层</span>        self.Dense1 = MyLinear(units=<span class="hljs-number">15</span>)        self.Dense2 = MyLinear(units=<span class="hljs-number">3</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, inputs)</span>:</span>        x = self.Dense1(inputs)        x = tf.nn.tanh(x)        x = self.Dense2(inputs)        x = tf.nn.softmax(x)        <span class="hljs-keyword">return</span> xmodel = MyModel(num_classes=<span class="hljs-number">3</span>)model.compile(    optimizer=tf.keras.optimizers.Adam(<span class="hljs-number">0.001</span>),    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])model.fit(data, target, batch_size=<span class="hljs-number">15</span>, epochs=<span class="hljs-number">15</span>)</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/06/12/7-5.png" srcset="/img/loading.gif" alt="Training"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 随机取几个样本预测结果</span>print(<span class="hljs-string">'预测结果：'</span>, model(data[:<span class="hljs-number">-1</span>:<span class="hljs-number">10</span>]).numpy().argmax(axis=<span class="hljs-number">1</span>))print(<span class="hljs-string">'正确结果：'</span>, target[:<span class="hljs-number">-1</span>:<span class="hljs-number">10</span>].astype(int))</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/12/7-6.png" srcset="/img/loading.gif" alt="Prediction"></p><div class="hljs"><pre><code class="hljs Python">model.summary()</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/12/7-7.png" srcset="/img/loading.gif" alt="Summary"></p><div class="hljs"><pre><code class="hljs Python">model.save(<span class="hljs-string">'model/MyLinear'</span>)</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/12/7-8.png" srcset="/img/loading.gif" alt="Saving"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 读取模型并预测</span>model_load = tf.keras.models.load_model(<span class="hljs-string">'model/MyLinear'</span>)model_load(data[:<span class="hljs-number">-1</span>:<span class="hljs-number">10</span>]).numpy().argmax(axis=<span class="hljs-number">1</span>)</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/06/12/7-9.png" srcset="/img/loading.gif" alt="Load"></p>]]></content>
    
    
    <categories>
      
      <category>TensorFlow2学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Python</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[TF2]6. Model Saving and Loading</title>
    <link href="/2020/TF2-6-Model-Saving-and-Loading/"/>
    <url>/2020/TF2-6-Model-Saving-and-Loading/</url>
    
    <content type="html"><![CDATA[<div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfprint(tf.__version__)<span class="hljs-comment"># 2.1.1</span></code></pre></div><p>当一个模型创建、进行训练后，我们可以将其保存至本地，以便继续训练、快速重新使用、部署到别的平台等。</p><h1 id="1-Keras模型的保存与加载"><a href="#1-Keras模型的保存与加载" class="headerlink" title="1. Keras模型的保存与加载"></a>1. Keras模型的保存与加载</h1><p>首先定义一个Keras模型并训练：</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npiris = datasets.load_iris()data = iris.datalabels = iris.targetdata = np.concatenate((data, labels.reshape(<span class="hljs-number">150</span>,<span class="hljs-number">1</span>)), axis=<span class="hljs-number">-1</span>)np.random.shuffle(data)X = data[:, :<span class="hljs-number">4</span>]  <span class="hljs-comment"># 前四列为特征</span>Y = data[:, <span class="hljs-number">-1</span>]  <span class="hljs-comment"># 最后一列为标签</span>print(X.shape)  <span class="hljs-comment"># 4个特征，150个样本</span>print(Y.shape)  <span class="hljs-comment"># 150个样本</span>print(np.unique(Y))  <span class="hljs-comment"># 3类</span></code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">import</span> tensorflow.keras.layers <span class="hljs-keyword">as</span> layers<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModel</span><span class="hljs-params">(tf.keras.Model)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, num_classes=<span class="hljs-number">3</span>)</span>:</span>        super(MyModel, self).__init__()        self.num_classes = num_classes        self.Dense1 = layers.Dense(<span class="hljs-number">32</span>, activation=<span class="hljs-string">'relu'</span>, input_shape=(<span class="hljs-number">4</span>,))        self.Dense2 = layers.Dense(<span class="hljs-number">3</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, inputs)</span>:</span>        x = self.Dense1(inputs)        x = self.Dense2(x)        <span class="hljs-keyword">return</span> xmodel = MyModel(num_classes=<span class="hljs-number">3</span>)model.compile(optimizer=tf.keras.optimizers.Adam(<span class="hljs-number">0.001</span>),              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])</code></pre></div><div class="hljs"><pre><code class="hljs Python">model.fit(X, Y, batch_size=<span class="hljs-number">10</span>, epochs=<span class="hljs-number">20</span>, shuffle=<span class="hljs-literal">True</span>)</code></pre></div><div class="hljs"><pre><code class="hljs Python">model.summary()</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/11/6-9.png" srcset="/img/loading.gif" alt="ModelSummary"><br><div class="hljs"><pre><code class="hljs Python">print(model.predict(X[<span class="hljs-number">140</span>:]).argmax(axis=<span class="hljs-number">1</span>))print(Y[<span class="hljs-number">140</span>:])</code></pre></div><br><img src="https://wx1.sbimg.cn/2020/06/11/6-10.png" srcset="/img/loading.gif" alt="OriginalPrediction"></p><blockquote><p>若保存过程中发生OSError错误，先创建相应的文件夹再保存。</p></blockquote><h2 id="（1）-model-save-weights"><a href="#（1）-model-save-weights" class="headerlink" title="（1） model.save_weights()"></a>（1） <code>model.save_weights()</code></h2><p>这种方法仅保存了模型的权重，重新读取、使用依赖于原来的代码。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 保存权重</span>model.save_weights(<span class="hljs-string">'model/test/weight_saving/weight_saving.h5'</span>)<span class="hljs-comment"># 读取权重需要通过相同类的模型实例</span>model_load_1 = MyModel(num_classes=<span class="hljs-number">3</span>)model_load_1.compile(optimizer=tf.keras.optimizers.Adam(<span class="hljs-number">0.001</span>),              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])model_load_1.fit(X, Y, epochs=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 建立层</span><span class="hljs-comment"># 读取权重</span>model_load_1.load_weights(<span class="hljs-string">'model/test/weight_saving/weight_saving.h5'</span>)print(model_load_1.predict(X[<span class="hljs-number">140</span>:]).argmax(axis=<span class="hljs-number">1</span>))print(Y[<span class="hljs-number">140</span>:])</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/06/11/6-1.png" srcset="/img/loading.gif" alt="1-Prediction"></p><h2 id="2-model-save"><a href="#2-model-save" class="headerlink" title="(2) model.save()"></a>(2) <code>model.save()</code></h2><p><code>model.save()</code>能保存模型的完整信息（包括权重、优化器等模型配置），加载模型不依赖原来的代码。</p><p>参数<code>save_format</code>在tf 2.x中默认为<code>&#39;tf&#39;</code>，也可手动指定为<code>&#39;h5&#39;</code>，但是<strong>HDF5格式不应用于保存子类式的模型</strong>。<br><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 保存整个模型</span>model.save(<span class="hljs-string">'model/test/model_saving/'</span>, save_format=<span class="hljs-string">'tf'</span>)<span class="hljs-comment"># save_format在tf2.x版本中默认为'tf'，也可选择'h5'</span><span class="hljs-comment"># model.save('./model/./model/test/model_saving.h5', save_format='h5')</span><span class="hljs-comment"># 读取模型，不需要先构建模型实例</span>model_load_2 = tf.keras.models.load_model(<span class="hljs-string">'model/test/model_saving/'</span>)print(model_load_2.predict(X[<span class="hljs-number">140</span>:]).argmax(axis=<span class="hljs-number">1</span>))print(Y[<span class="hljs-number">140</span>:])</code></pre></div><br><img src="https://wx2.sbimg.cn/2020/06/11/6-2.png" srcset="/img/loading.gif" alt="2-Prediction"></p><h2 id="3-tf-saved-model-save"><a href="#3-tf-saved-model-save" class="headerlink" title="(3) tf.saved_model.save()"></a>(3) <code>tf.saved_model.save()</code></h2><p><code>tf.saved_model.save()</code>保存的模型可以直接预测，但不保存优化器等配置，适用于部署的场景。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 保存没有优化器等配置的模型</span>tf.saved_model.save(model, <span class="hljs-string">'model/test/model_saving_noconf/'</span>)<span class="hljs-comment"># 读取不需要依赖已有实例，但是只能用于预测</span>model_load_3 = tf.saved_model.load(<span class="hljs-string">'model/test/model_saving_noconf/'</span>)<span class="hljs-comment"># 读取的模型不是keras模型，需要通过签名指定输入来得到预测结果</span>f = model_load_3.signatures[<span class="hljs-string">"serving_default"</span>]pred = f(input_1 = tf.constant(X[<span class="hljs-number">140</span>:].tolist()))print(pred)</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/06/11/6-3.png" srcset="/img/loading.gif" alt="3-Load"><br><div class="hljs"><pre><code class="hljs Python">print(pred[<span class="hljs-string">'output_1'</span>].numpy().argmax(axis=<span class="hljs-number">1</span>))print(Y[<span class="hljs-number">140</span>:])</code></pre></div><br><img src="https://wx2.sbimg.cn/2020/06/11/6-4.png" srcset="/img/loading.gif" alt="3-Prediction"></p><h1 id="2-自定义模型的保存与加载"><a href="#2-自定义模型的保存与加载" class="headerlink" title="2. 自定义模型的保存与加载"></a>2. 自定义模型的保存与加载</h1><p>自定义模型保存时注意模型中<code>call()</code>函数需要用<code>@tf.function</code>修饰使其成为静态图。</p><p>保存、读取后的使用细节与Keras模型稍有不同。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModel</span><span class="hljs-params">(tf.keras.Model)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, num_classes=<span class="hljs-number">10</span>)</span>:</span>        super(MyModel, self).__init__()        self.num_classes = num_classes        self.dense1 = Dense(<span class="hljs-number">32</span>, activation=<span class="hljs-string">'relu'</span>)        self.dense2 = Dense(<span class="hljs-number">10</span>)    <span class="hljs-comment"># 修饰时指定输入形状和类型</span><span class="hljs-meta">    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 32], dtype=tf.float32)])</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, x)</span>:</span>        x = self.dense1(x)        x = self.dense2(x)        <span class="hljs-keyword">return</span> xmodel = MyModel(num_classes=<span class="hljs-number">10</span>)<span class="hljs-comment"># 定义优化器、损失函数和评价指标</span>optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="hljs-number">0.001</span>)loss = tf.keras.losses.CategoricalCrossentropy()train_metric = tf.keras.metrics.CategoricalAccuracy()val_metric = tf.keras.metrics.CategoricalAccuracy()</code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 准备数据</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npx_train = np.random.random((<span class="hljs-number">6400</span>, <span class="hljs-number">32</span>))  <span class="hljs-comment"># 32个输入特征</span>y_train = np.random.random((<span class="hljs-number">6400</span>, <span class="hljs-number">10</span>))  <span class="hljs-comment"># 10个输出特征</span>train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="hljs-number">64</span>)x_val = np.random.random((<span class="hljs-number">640</span>, <span class="hljs-number">32</span>))y_val = np.random.random((<span class="hljs-number">640</span>, <span class="hljs-number">10</span>))val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(<span class="hljs-number">64</span>)</code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 自定义训练</span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">5</span>):  <span class="hljs-comment"># 5个epoch</span>    loss_epoch = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> step, (x_batch, y_batch) <span class="hljs-keyword">in</span> enumerate(train_dataset):  <span class="hljs-comment"># 遍历每个batch</span>        <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:  <span class="hljs-comment"># 使用GradientTape记录模型的前向传播以便求导</span>            prediction = model(x_batch, training=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># 运行前向传播，得到预测值</span>            loss_value = loss(y_batch, prediction)  <span class="hljs-comment"># 计算损失</span>        grad = tape.gradient(loss_value, model.trainable_weights)  <span class="hljs-comment"># 得到梯度</span>        optimizer.apply_gradients(zip(grad, model.trainable_weights))  <span class="hljs-comment"># 优化器使用梯度更新权重参数</span>        loss_epoch += float(loss_value)        train_metric(y_batch, prediction)  <span class="hljs-comment"># 每个batch更新一次metric</span>    print(<span class="hljs-string">'Epoch &#123;&#125;/5: \nLoss=&#123;&#125;, Accuracy=&#123;&#125;'</span>.format(epoch + <span class="hljs-number">1</span>, loss_epoch / <span class="hljs-number">100</span>, float(train_metric.result())))    train_metric.reset_states()  <span class="hljs-comment"># 每个epoch结束后重置metric</span>    <span class="hljs-comment"># 在评价集上计算metric</span>    <span class="hljs-keyword">for</span> x_batch, y_vatch <span class="hljs-keyword">in</span> val_dataset:        validation = model(x_batch)        val_metric(y_vatch, validation)    print(<span class="hljs-string">'Accuracy on validation set: &#123;&#125;\n'</span>.format(float(val_metric.result())))    val_metric.reset_states()</code></pre></div><h2 id="（1）-model-save-weights-1"><a href="#（1）-model-save-weights-1" class="headerlink" title="（1） model.save_weights()"></a>（1） <code>model.save_weights()</code></h2><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 保存权重</span>model.save_weights(<span class="hljs-string">'model/test/weight_saving/weight_saving.h5'</span>)<span class="hljs-comment"># 读取权重需要通过相同类的模型实例</span>model_load_1 = MyModel(num_classes=<span class="hljs-number">3</span>)model_load_1.compile(optimizer=optimizer,              loss=loss,              metrics=[train_metric, val_metric])model_load_1.fit(x_train, y_train, epochs=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 建立层</span><span class="hljs-comment"># 读取权重</span>model_load_1.load_weights(<span class="hljs-string">'model/test/weight_saving/weight_saving.h5'</span>)print(model_load_1(x_val[:<span class="hljs-number">1</span>]))print(y_val[:<span class="hljs-number">1</span>])</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/11/6-5.png" srcset="/img/loading.gif" alt="1-Prediction"></p><h2 id="2-model-save-1"><a href="#2-model-save-1" class="headerlink" title="(2) model.save()"></a>(2) <code>model.save()</code></h2><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 保存整个模型</span>model._set_inputs(tf.TensorSpec(shape=[<span class="hljs-literal">None</span>, <span class="hljs-number">32</span>], dtype=tf.float32))  <span class="hljs-comment"># 要首先指定输入格式，与call的装饰器中保持相同</span>model.save(<span class="hljs-string">'model/test/model_saving/'</span>, save_format=<span class="hljs-string">'tf'</span>)<span class="hljs-comment"># 读取模型，不需要先构建模型实例</span>model_load_2 = tf.keras.models.load_model(<span class="hljs-string">'model/test/model_saving/'</span>)print(model_load_1(x_val[:<span class="hljs-number">1</span>]))print(y_val[:<span class="hljs-number">1</span>])</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/11/6-6.png" srcset="/img/loading.gif" alt="2-Prediction"></p><h2 id="3-tf-saved-model-save-1"><a href="#3-tf-saved-model-save-1" class="headerlink" title="(3) tf.saved_model.save()"></a>(3) <code>tf.saved_model.save()</code></h2><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 保存没有优化器等配置的模型</span>tf.saved_model.save(model, <span class="hljs-string">'model/test/model_saving_noconf/'</span>)<span class="hljs-comment"># 读取不需要依赖已有实例，但是只能用于预测</span>model_load_3 = tf.saved_model.load(<span class="hljs-string">'model/test/model_saving_noconf/'</span>)<span class="hljs-comment"># 读取的模型不是keras模型，需要通过签名指定输入来得到预测结果</span>f = model_load_3.signatures[<span class="hljs-string">"serving_default"</span>]pred = f(args_0 = tf.constant(x_val[:<span class="hljs-number">1</span>].tolist()))print(pred)</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/11/6-7.png" srcset="/img/loading.gif" alt="3-Load"></p><div class="hljs"><pre><code class="hljs Python">print(pred[<span class="hljs-string">'output_1'</span>].numpy())print(y_val[:<span class="hljs-number">1</span>])</code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/11/6-8.png" srcset="/img/loading.gif" alt="3-Prediction"></p>]]></content>
    
    
    <categories>
      
      <category>TensorFlow2学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Python</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[TF2]5. AutoGraph</title>
    <link href="/2020/TF2-5-Autograph/"/>
    <url>/2020/TF2-5-Autograph/</url>
    
    <content type="html"><![CDATA[<div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfprint(tf.__version__)<span class="hljs-comment"># 2.1.1</span></code></pre></div><p>Tensorflow用计算图（Computional Graph）来表示计算逻辑。计算图一般有三种构建方式：</p><ul><li>静态图：首先创建图的结构，然后开启一个会话（Session），显式地执行图。这种方式编译器对网络的优化程度高，便于模型的复用。</li><li>动态图：按照代码的顺序执行。这种方式更利于调试和开发</li><li>AutoGraph：将动态图转换为静态图。</li></ul><h1 id="1-AutoGraph的机制原理与使用规范"><a href="#1-AutoGraph的机制原理与使用规范" class="headerlink" title="1. AutoGraph的机制原理与使用规范"></a>1. AutoGraph的机制原理与使用规范</h1><p>使用 <code>@tf.function</code> 修饰函数以使用AutoGraph，即将函数创建静态图。<br>AutoGraph使用中一般需要注意以下三点：</p><h2 id="1-优先使用TensorFlow中提供的函数"><a href="#1-优先使用TensorFlow中提供的函数" class="headerlink" title="(1) 优先使用TensorFlow中提供的函数"></a>(1) 优先使用TensorFlow中提供的函数</h2><p>在被 <code>@tf.function</code> 修饰的函数第一次执行时，会首先创建计算图，然后执行计算图。但是由于非TensorFlow的函数无法嵌入静态图，所以再次执行时可能会出现不同的结果。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 非TensorFlow的函数无法嵌入静态图的例子</span><span class="hljs-meta">@tf.function</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">myadd</span><span class="hljs-params">(a, b)</span>:</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tf.range(<span class="hljs-number">3</span>):        tf.print(i)    c = a + b    print(<span class="hljs-string">'Python function'</span>)    <span class="hljs-keyword">return</span> c</code></pre></div><p>第一次执行时，TensorFlow中的函数被加入了静态图，而Python函数直接执行，没有加入静态图；图创建完成后被执行，只执行了TensorFlow中的函数。</p><div class="hljs"><pre><code class="hljs Python">myadd(tf.constant(<span class="hljs-string">'aaa'</span>), tf.constant(<span class="hljs-string">'bbb'</span>))  <span class="hljs-comment"># 第一次执行较慢</span></code></pre></div><p><img src="https://wx1.sbimg.cn/2020/06/10/5-1.png" srcset="/img/loading.gif" alt="AutoGraph-1"></p><p>再次执行时，只会执行创建完成的计算图，所以Python函数没有执行。<br><div class="hljs"><pre><code class="hljs Python">myadd(tf.constant(<span class="hljs-string">'aaa'</span>), tf.constant(<span class="hljs-string">'bbb'</span>))  <span class="hljs-comment"># 已创建的计算图执行很快</span></code></pre></div><br><img src="https://wx1.sbimg.cn/2020/06/10/5-2.png" srcset="/img/loading.gif" alt="AutoGraph-2"></p><p>如果传入不同类型的参数，则会创建新的静态图。<br><div class="hljs"><pre><code class="hljs Python">myadd(tf.constant(<span class="hljs-number">1</span>), tf.constant(<span class="hljs-number">3</span>))</code></pre></div><br><img src="https://wx1.sbimg.cn/2020/06/10/5-3.png" srcset="/img/loading.gif" alt="AutoGraph-3"></p><p>若不使用TensorFlow中的数据类型，则每次都会创建新的图。<br><div class="hljs"><pre><code class="hljs Python">myadd(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)myadd(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)</code></pre></div><br><img src="https://wx1.sbimg.cn/2020/06/10/5-4.png" srcset="/img/loading.gif" alt="AutoGraph-4"></p><p>使用非TensorFlow函数还会导致一些其他的问题。<br><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-meta">@tf.function</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">np_random</span><span class="hljs-params">()</span>:</span>    a = np.random.randn(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)    tf.print(a)<span class="hljs-meta">@tf.function</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tf_random</span><span class="hljs-params">()</span>:</span>    a = tf.random.normal((<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))    tf.print(a)<span class="hljs-comment"># 使用@tf.function将函数添加入静态图后，numpy中的随机函数每次都输出相同的结果</span>np_random()np_random()<span class="hljs-comment"># 使用tensorflow中的随机函数，运行正常</span>tf_random()tf_random()</code></pre></div><br><img src="https://wx1.sbimg.cn/2020/06/10/5-5.png" srcset="/img/loading.gif" alt="AutoGraph-Random"></p><h2 id="2-避免在被修饰的函数内部定义Variable"><a href="#2-避免在被修饰的函数内部定义Variable" class="headerlink" title="(2) 避免在被修饰的函数内部定义Variable"></a>(2) 避免在被修饰的函数内部定义Variable</h2><p>在动态图中,  <code>tf.Variable</code> 是一个普通的Python变量, 超出了其作用域范围就会被销毁；而在静态图中,  <code>tf.Variable</code> 则是计算图中一个持续存在的节点, 不受Python的作用域的影响。在转换成图的过程中， <code>tf.function</code> 可能会将一个函数执行多次，如果在函数内部有创建Variable的语句，就会因为多次创建相同的Variable而出错。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 正确的方式：在外部定义变量</span>x = tf.Variable(<span class="hljs-number">1</span>, dtype=tf.float32)<span class="hljs-meta">@tf.function</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">outer_var</span><span class="hljs-params">()</span>:</span>    x.assign_add(<span class="hljs-number">1</span>)    tf.print(x)    <span class="hljs-keyword">return</span> xouter_var()outer_var()</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/06/10/5-6.png" srcset="/img/loading.gif" alt="Outer_Var"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 错误的方式：在内部定义变量</span><span class="hljs-meta">@tf.function</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">inner_var</span><span class="hljs-params">()</span>:</span>    x = tf.Variable(<span class="hljs-number">1</span>, dtype=tf.float32)    x.assign_add(<span class="hljs-number">1</span>)    tf.print(x)    <span class="hljs-keyword">return</span> xinner_var()inner_var()</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/06/10/5-7.png" srcset="/img/loading.gif" alt="Inner_Var"></p><h2 id="3-被修饰的函数无法修改外部Python列表或字典等结构类型变量"><a href="#3-被修饰的函数无法修改外部Python列表或字典等结构类型变量" class="headerlink" title="(3) 被修饰的函数无法修改外部Python列表或字典等结构类型变量"></a>(3) 被修饰的函数无法修改外部Python列表或字典等结构类型变量</h2><p>与Python函数类似，列表、字典等Python内置的结构类型变量也无法嵌入计算图，因此无法被静态图修改。<br><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 未被修饰的函数</span>tensor_list = []<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">append_tensor</span><span class="hljs-params">(x)</span>:</span>    tensor_list.append(x)    <span class="hljs-keyword">return</span> tensor_listappend_tensor(tf.constant(<span class="hljs-number">5.0</span>))append_tensor(tf.constant(<span class="hljs-number">6.0</span>))print(tensor_list)  <span class="hljs-comment"># 里面有两个tensor</span></code></pre></div><br><img src="https://wx2.sbimg.cn/2020/06/10/5-8.png" srcset="/img/loading.gif" alt="Undecorated"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 被修饰的函数</span>tensor_list = []<span class="hljs-meta">@tf.function</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">append_tensor</span><span class="hljs-params">(x)</span>:</span>    tensor_list.append(x)    <span class="hljs-keyword">return</span> tensor_listappend_tensor(tf.constant(<span class="hljs-number">5.0</span>))append_tensor(tf.constant(<span class="hljs-number">6.0</span>))print(tensor_list)  <span class="hljs-comment"># 里面只有一个空的tensor</span></code></pre></div><p><img src="https://wx2.sbimg.cn/2020/06/10/5-9.png" srcset="/img/loading.gif" alt="Decorated"></p><h1 id="2-AutoGraph的使用方法"><a href="#2-AutoGraph的使用方法" class="headerlink" title="2. AutoGraph的使用方法"></a>2. AutoGraph的使用方法</h1><p>不应该在被 <code>@tf.function</code> 修饰的函数内定义Variable，但是将其定义在函数之外又与封装的理念相违背。为了解决这个问题，一般通过继承 <code>tf.Module</code>  生成子类，将函数和变量都封装起来。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyAdd</span><span class="hljs-params">(tf.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, init_value=tf.constant<span class="hljs-params">(<span class="hljs-number">0.0</span>)</span>, name=None)</span>:</span>        super(MyAdd, self).__init__(name=name)        <span class="hljs-keyword">with</span> self.name_scope:            self.x = tf.Variable(init_value, dtype=tf.float32, trainable=<span class="hljs-literal">True</span>)<span class="hljs-meta">    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32)])</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">addprint</span><span class="hljs-params">(self, a)</span>:</span>        <span class="hljs-keyword">with</span> self.name_scope:            self.x.assign_add(a)            tf.print(self.x)            <span class="hljs-keyword">return</span> self.x</code></pre></div><p><code>with self.name_scope</code> 相当于 <code>with tf.namescope(myadd)</code>，圈定了其中变量的所属范围（但是不影响变量的使用范围）。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 执行</span>add1 = MyAdd(init_value=tf.constant(<span class="hljs-number">1.0</span>))result = add1.addprint(tf.constant(<span class="hljs-number">2.0</span>))</code></pre></div><p><img src="https://wx2.sbimg.cn/2020/06/10/5-10.png" srcset="/img/loading.gif" alt="MyAdd"><br><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 查看其中的所有变量</span>print(add1.variables)print(add1.trainable_variables)</code></pre></div><br><img src="https://wx1.sbimg.cn/2020/06/10/5-11.png" srcset="/img/loading.gif" alt="Variables"></p>]]></content>
    
    
    <categories>
      
      <category>TensorFlow2学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Python</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[TF2]4. Custom Training Process</title>
    <link href="/2020/TF2-4-Custom-Training-Process/"/>
    <url>/2020/TF2-4-Custom-Training-Process/</url>
    
    <content type="html"><![CDATA[<div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfprint(tf.__version__)<span class="hljs-comment"># 2.1.1</span></code></pre></div><h1 id="1-tf-GrandientTape"><a href="#1-tf-GrandientTape" class="headerlink" title="1. tf.GrandientTape"></a>1. tf.GrandientTape</h1><p><code>tf.GrandientTape()</code> 是一个上下文管理器，能够记录变量上的操作，以便进行求导等。</p><p>求 $ y=x^{3}+4x $ 中$ y $对$ x $的一、二阶导：<br><div class="hljs"><pre><code class="hljs Python">x = tf.constant(<span class="hljs-number">3.0</span>)<span class="hljs-comment"># 求导</span><span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> g2:  <span class="hljs-comment"># 二阶导数</span>    g2.watch(x)  <span class="hljs-comment"># 使g2追踪x</span>    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> g1:  <span class="hljs-comment"># 一阶导数</span>        g1.watch(x)        y = x * x * x + <span class="hljs-number">4</span> * x            dy_dx = g1.gradient(y, x)  <span class="hljs-comment"># 3 * x^2 + 4 = 31</span>    print(dy_dx)d2y_dx2 = g2.gradient(dy_dx, x)  <span class="hljs-comment"># 6 * x = 18</span>print(d2y_dx2)</code></pre></div><br><img src="https://wx1.sbimg.cn/2020/06/08/4-1.png" srcset="/img/loading.gif" alt="Gradient-1"></p><p>当 <code>GradientTape.gradient()</code> 方法被调用后，这个GradientTape对象就会被自动释放。如果需要多次调用这个方法（在同一个计算上多次求导），可以设置其参数 <code>persistent=True</code> ，在使用结束后手动删除它。</p><p>求 $y=x^{2}, z=y^{2}$ 中$ y $对$ x $、$ z $对$ x $的一阶导：<br><div class="hljs"><pre><code class="hljs Python">x = tf.constant(<span class="hljs-number">5.0</span>)<span class="hljs-keyword">with</span> tf.GradientTape(persistent=<span class="hljs-literal">True</span>) <span class="hljs-keyword">as</span> g:    g.watch(x)    y = x * x    z = y * ydy_dx = g.gradient(y, x)  <span class="hljs-comment"># 2 * x = 10</span>print(dy_dx)dz_dx = g.gradient(z, x)  <span class="hljs-comment"># 4 * x^3 = 500</span>print(dz_dx)<span class="hljs-keyword">del</span> g  <span class="hljs-comment"># 需要手动删除</span></code></pre></div><br><img src="https://wx2.sbimg.cn/2020/06/08/4-2.png" srcset="/img/loading.gif" alt="Gradient-2"></p><h1 id="2-自定义模型训练过程"><a href="#2-自定义模型训练过程" class="headerlink" title="2. 自定义模型训练过程"></a>2. 自定义模型训练过程</h1><p>模型的训练 <code>fit()</code> 是以下过程的循环：</p><ul><li>预测→计算损失→反向传播计算梯度→优化器使用梯度更新模型参数</li></ul><p>因此，使用GradientTape，可以方便地自定义训练过程。</p><p>首先构建模型，设置优化器、损失函数、评价指标，准备数据：<br><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 构建模型（定义前向传播过程）</span><span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModel</span><span class="hljs-params">(tf.keras.Model)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, num_classes=<span class="hljs-number">10</span>)</span>:</span>        super(MyModel, self).__init__()        self.num_classes = num_classes        self.dense1 = Dense(<span class="hljs-number">32</span>, activation=<span class="hljs-string">'relu'</span>)        self.dense2 = Dense(<span class="hljs-number">10</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, x)</span>:</span>        x = self.dense1(x)        x = self.dense2(x)        <span class="hljs-keyword">return</span> xmodel = MyModel(num_classes=<span class="hljs-number">10</span>)<span class="hljs-comment"># 定义优化器、损失函数和评价指标</span>optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="hljs-number">0.001</span>)loss = tf.keras.losses.CategoricalCrossentropy()train_metric = tf.keras.metrics.CategoricalAccuracy()val_metric = tf.keras.metrics.CategoricalAccuracy()<span class="hljs-comment"># 准备数据</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npx_train = np.random.random((<span class="hljs-number">6400</span>, <span class="hljs-number">32</span>))  <span class="hljs-comment"># 32个输入特征</span>y_train = np.random.random((<span class="hljs-number">6400</span>, <span class="hljs-number">10</span>))  <span class="hljs-comment"># 10个输出特征</span>train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="hljs-number">64</span>)x_val = np.random.random((<span class="hljs-number">640</span>, <span class="hljs-number">32</span>))y_val = np.random.random((<span class="hljs-number">640</span>, <span class="hljs-number">10</span>))val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(<span class="hljs-number">64</span>)</code></pre></div></p><p>然后自定义训练过程，也就是手动执行 <code>fit()</code> 中的各个步骤：<br><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 自定义训练</span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">5</span>):  <span class="hljs-comment"># 5个epoch</span>    loss_epoch = <span class="hljs-number">0</span>  <span class="hljs-comment"># 整个epoch的平均loss</span>    <span class="hljs-keyword">for</span> step, (x_batch, y_batch) <span class="hljs-keyword">in</span> enumerate(train_dataset):  <span class="hljs-comment"># 遍历每个batch</span>        <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:  <span class="hljs-comment"># 使用GradientTape记录模型的前向传播以便求导</span>            prediction = model(x_batch, training=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># 运行前向传播，得到预测值</span>            loss_value = loss(y_batch, prediction)  <span class="hljs-comment"># 计算损失</span>        grad = tape.gradient(loss_value, model.trainable_weights)  <span class="hljs-comment"># 得到梯度</span>        optimizer.apply_gradients(zip(grad, model.trainable_weights))  <span class="hljs-comment"># 优化器使用梯度更新权重参数</span>        loss_epoch += float(loss_value)        train_metric(y_batch, prediction)  <span class="hljs-comment"># 每个batch更新一次metric</span>    print(<span class="hljs-string">'Epoch &#123;&#125;/5: \nLoss=&#123;&#125;, Accuracy=&#123;&#125;'</span>.format(epoch + <span class="hljs-number">1</span>, loss_epoch / <span class="hljs-number">100</span>, float(train_metric.result())))    train_metric.reset_states()  <span class="hljs-comment"># 每个epoch结束后重置metric</span>    <span class="hljs-comment"># 在评价集上计算metric</span>    <span class="hljs-keyword">for</span> x_batch, y_vatch <span class="hljs-keyword">in</span> val_dataset:        validation = model(x_batch)        val_metric(y_vatch, validation)    print(<span class="hljs-string">'Accuracy on validation set: &#123;&#125;\n'</span>.format(float(val_metric.result())))    val_metric.reset_states()</code></pre></div><br><img src="https://wx2.sbimg.cn/2020/06/08/4-3.png" srcset="/img/loading.gif" alt="CustomTraining"></p><p>作为对比，使用 <code>fit()</code> 进行训练：<br><div class="hljs"><pre><code class="hljs Python">model = MyModel(num_classes=<span class="hljs-number">10</span>)model.compile(optimizer=optimizer, loss=loss, metrics=[<span class="hljs-string">'accuracy'</span>])model.fit(data, labels, batch_size=<span class="hljs-number">64</span>, epochs=<span class="hljs-number">5</span>)</code></pre></div></p><p><img src="https://wx2.sbimg.cn/2020/06/08/4-4.png" srcset="/img/loading.gif" alt="DefaultTraining"></p>]]></content>
    
    
    <categories>
      
      <category>TensorFlow2学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Python</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[TF2]3. Keras Model Training</title>
    <link href="/2020/TF2-3-Keras-Model-Training/"/>
    <url>/2020/TF2-3-Keras-Model-Training/</url>
    
    <content type="html"><![CDATA[<div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfprint(tf.__version__)<span class="hljs-comment"># 2.1.1</span></code></pre></div><p>这部分包括了使用 <code>tf.keras</code> 构建、训练模型的基本方法。</p><h1 id="1-基本模型训练流程"><a href="#1-基本模型训练流程" class="headerlink" title="1. 基本模型训练流程"></a>1. 基本模型训练流程</h1><p>模型训练的一般步骤包括：</p><ul><li>构建模型（三种方式）</li><li>训练模型：<code>model.fit()</code></li><li>验证模型：<code>model.evaluate()</code></li><li>使用模型进行预测：<code>model.predict()</code></li></ul><h2 id="1-1-构建模型"><a href="#1-1-构建模型" class="headerlink" title="1.1 构建模型"></a>1.1 构建模型</h2><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> layers<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModel</span><span class="hljs-params">(tf.keras.Model)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, num_classes=<span class="hljs-number">10</span>)</span>:</span>        super(MyModel, self).__init__()        self.num_classes = num_classes        self.Dense1 = layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>, input_shape=(<span class="hljs-number">32</span>,))  <span class="hljs-comment"># 输入维度32</span>        self.Dense2 = layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)        self.Dense3 = layers.Dense(<span class="hljs-number">10</span>)  <span class="hljs-comment"># 输出为10类</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, x)</span>:</span>        x = self.Dense1(x)        x = self.Dense2(x)        x = self.Dense3(x)        <span class="hljs-keyword">return</span> xmodel = MyModel(num_classes=<span class="hljs-number">10</span>)model.compile(    optimizer=tf.keras.optimizers.Adam(<span class="hljs-number">0.001</span>),    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),    metrics=[<span class="hljs-string">'accuracy'</span>])</code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 构建数据集</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npx_train = np.random.random((<span class="hljs-number">1000</span>, <span class="hljs-number">32</span>))  <span class="hljs-comment"># 32个特征，1000个样本</span>y_train = np.random.randint(<span class="hljs-number">10</span>, size=(<span class="hljs-number">1000</span>, ))  <span class="hljs-comment"># 10个分类</span>x_val = np.random.random((<span class="hljs-number">200</span>, <span class="hljs-number">32</span>))y_val = np.random.randint(<span class="hljs-number">10</span>, size=(<span class="hljs-number">200</span>, ))x_test = np.random.random((<span class="hljs-number">200</span>, <span class="hljs-number">32</span>))y_test = np.random.randint(<span class="hljs-number">10</span>, size=(<span class="hljs-number">200</span>, ))</code></pre></div><h2 id="1-2-训练模型"><a href="#1-2-训练模型" class="headerlink" title="1.2 训练模型"></a>1.2 训练模型</h2><p><code>model.fit()</code>中的基本参数：</p><ul><li>x、y：输入数据和目标数据。</li><li>batch_size：样本分组的大小，即每训练batch_size个样本，模型更新一次权重参数。</li><li>epochs：训练的轮数。</li></ul><p>这里还使用了验证集：<br><div class="hljs"><pre><code class="hljs Python">model.fit(x_train, y_train, batch_size=<span class="hljs-number">100</span>, epochs=<span class="hljs-number">5</span>, validation_data=(x_val, y_val))</code></pre></div></p><h2 id="1-3-模型验证和预测"><a href="#1-3-模型验证和预测" class="headerlink" title="1.3 模型验证和预测"></a>1.3 模型验证和预测</h2><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 在测试集上验证模型</span>results = model.evaluate(x_test, y_test, batch_size=<span class="hljs-number">128</span>)</code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 对x_test的前三个样本进行预测</span>predictions = model.predict(x_test[:<span class="hljs-number">3</span>])print(y_test[:<span class="hljs-number">3</span>], predictions.argmax(axis=<span class="hljs-number">1</span>))  <span class="hljs-comment"># 分类的预测输出为10个分类的概率，这里取每行最大值的索引（即对应分类）</span></code></pre></div><h1 id="2-使用类别加权和样本加权"><a href="#2-使用类别加权和样本加权" class="headerlink" title="2. 使用类别加权和样本加权"></a>2. 使用类别加权和样本加权</h1><h2 id="2-1-类别加权"><a href="#2-1-类别加权" class="headerlink" title="2.1 类别加权"></a>2.1 类别加权</h2><p><code>model.fit()</code>函数中包含参数<strong>class_weight</strong>，可以通过字典形式指定每个<strong>特征</strong>的权重。注意，该字典必须包含输入的<strong>全部特征</strong>，即字典长度等于输入的列数。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 权重字典，使模型更重视5</span>class_weight = &#123;&#125;<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):    class_weight[i] = <span class="hljs-number">1</span>class_weight[<span class="hljs-number">5</span>] = <span class="hljs-number">2</span><span class="hljs-comment"># 传入权重参数进行训练</span>model.fit(x_train, y_train, batch_size=<span class="hljs-number">100</span>, epochs=<span class="hljs-number">5</span>, validation_data=(x_val, y_val), class_weight=class_weight)</code></pre></div><h2 id="2-2-样本加权"><a href="#2-2-样本加权" class="headerlink" title="2.2 样本加权"></a>2.2 样本加权</h2><p><code>model.fit()</code>函数中包含参数<strong>sample_weight</strong>，可以通过数组形式指定每个<strong>样本</strong>的权重。数组长度应等于输入的行数。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 权重列表，使模型更重视5</span>sample_weight = np.ones(shape=(len(y_train),))  <span class="hljs-comment"># 所有样本权重为1</span>sample_weight[y_train == <span class="hljs-number">5</span>] = <span class="hljs-number">2</span>  <span class="hljs-comment"># 5的权重为2</span><span class="hljs-comment"># 传入权重参数进行训练</span>model.fit(x_train, y_train, batch_size=<span class="hljs-number">100</span>, epochs=<span class="hljs-number">5</span>, validation_data=(x_val, y_val), sample_weight=sample_weight)</code></pre></div><h1 id="3-使用回调函数"><a href="#3-使用回调函数" class="headerlink" title="3. 使用回调函数"></a>3. 使用回调函数</h1><p>回调函数（Callback Function）是模型在训练中的特定时刻被调用的对象。</p><p>可以以列表的方式将需要使用的回调函数实例化后传入<code>model.fit()</code>的callbacks参数。</p><p>这里主要介绍三个回调函数。</p><h2 id="3-1-EarlyStopping-早停"><a href="#3-1-EarlyStopping-早停" class="headerlink" title="3.1 EarlyStopping(早停)"></a>3.1 EarlyStopping(早停)</h2><p>当被监视的指标不再改善时自动停止训练。一般需要指定的参数：</p><ul><li>monitor：监视的指标。</li><li>min_delta：变化量小于它即认为“不再改善”。</li><li>patience：容忍“不再改善”的次数。</li><li>mode：在’auto’、’min’、’max’中选择。’min’意为当指标不再下降时停止训练，’max’反之。</li></ul><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 设置回调函数</span>callbacks = [    tf.keras.callbacks.EarlyStopping(        monitor=<span class="hljs-string">'val_loss'</span>,  <span class="hljs-comment"># 监视val_loss</span>        min_delta=<span class="hljs-number">0.005</span>,  <span class="hljs-comment"># 变化小于0.005即认为没有改善</span>        patience=<span class="hljs-number">3</span>,  <span class="hljs-comment"># 没有改善的情况连续出现3次则停止训练</span>        verbose=<span class="hljs-number">2</span>    )]<span class="hljs-comment">#　使用回调函数进行训练</span>model.fit(x_train, y_train, batch_size=<span class="hljs-number">100</span>, epochs=<span class="hljs-number">100</span>, validation_data=(x_val, y_val), callbacks=callbacks)</code></pre></div><p>可以看到，虽然设置了100个epoch，但是由于连续3次val_loss变化小于0.005，训练自动停止。<br><img src="https://wx1.sbimg.cn/2020/06/07/3-1.png" srcset="/img/loading.gif" alt="EarlyStopping"></p><h2 id="3-2-ModelCheckpoint-定期自动保存模型"><a href="#3-2-ModelCheckpoint-定期自动保存模型" class="headerlink" title="3.2 ModelCheckpoint(定期自动保存模型)"></a>3.2 ModelCheckpoint(定期自动保存模型)</h2><p>在指定的时间点保存模型。一般需要指定的参数：</p><ul><li>filepath：保存路径，可以使用’{epoch}’等使每次保存的文件名不同。</li><li>monitor：监视的指标。</li><li>save_best_only：设置为True时，保存时若当前次的指标更好，会覆盖之前保存的模型。</li><li>save_weights_only：设置为True是仅保存模型的权重参数，而非整个模型。</li><li>save_freq：保存频率。一般设置为’epoch’，即每个epoch后保存；也可设置为整数，表示每训练一定数量的样本，就会在当前batch结束后保存。官方文档指出后者可能更不可靠。</li></ul><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 设置回调函数</span>callbacks = [    tf.keras.callbacks.ModelCheckpoint(        filepath=<span class="hljs-string">'./checkpoints/MyModel_Checkpoint_&#123;epoch&#125;'</span>,  <span class="hljs-comment"># 每个epoch对应的独立的文件</span>        monitor=<span class="hljs-string">'val_loss'</span>,        save_best_only=<span class="hljs-literal">True</span>,  <span class="hljs-comment"># 只有当评价指标改进时保存</span>        save_weights_only=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># 保存整个模型</span>        verbose=<span class="hljs-number">1</span>    )]<span class="hljs-comment">#　使用回调函数进行训练</span>model.fit(x_train, y_train, batch_size=<span class="hljs-number">100</span>, epochs=<span class="hljs-number">3</span>, validation_data=(x_val, y_val), callbacks=callbacks)</code></pre></div><p>由于设置了<code>verbose=1</code>，每次保存的信息都在训练过程中输出：<br><img src="https://wx2.sbimg.cn/2020/06/07/3-2.png" srcset="/img/loading.gif" alt="ModelCheckpoint"></p><h2 id="3-3-ReduceLROnPlateau-动态调整学习率"><a href="#3-3-ReduceLROnPlateau-动态调整学习率" class="headerlink" title="3.3 ReduceLROnPlateau(动态调整学习率)"></a>3.3 ReduceLROnPlateau(动态调整学习率)</h2><p>当模型不再改善时自动减小学习率。一般需要指定的参数：</p><ul><li>monitor: 监视的指标。</li><li>factor: new_lr = lr * factor。</li><li>patience: 容忍“不再改善”的次数。</li><li>mode: 在’auto’、’min’、’max’中选择。’min’意为当指标不再下降时修改学习率，’max’反之。</li><li>min_delta: 变化量小于它即认为“不再改善”。</li><li>cooldown：每次执行调整后，到下一次开始计算patience的间隔。</li><li>min_lr: 学习率的最小值。</li></ul><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 设置回调函数</span>callbacks = [    tf.keras.callbacks.ReduceLROnPlateau(        monitor=<span class="hljs-string">'val_loss'</span>,        factor=<span class="hljs-number">0.5</span>,        patience=<span class="hljs-number">3</span>,        mode=<span class="hljs-string">'max'</span>,        verbose=<span class="hljs-number">1</span>    )]<span class="hljs-comment">#　使用回调函数进行训练</span>model.fit(x_train, y_train, batch_size=<span class="hljs-number">100</span>, epochs=<span class="hljs-number">20</span>, validation_data=(x_val, y_val), callbacks=callbacks)</code></pre></div><p>当未改善次数达到3次时将学习率调整为原来的0.5倍：<br><img src="https://wx1.sbimg.cn/2020/06/07/3-3.png" srcset="/img/loading.gif" alt="ReduceLROnPlateau"></p><h1 id="4-构建多输入、多输出模型"><a href="#4-构建多输入、多输出模型" class="headerlink" title="4. 构建多输入、多输出模型"></a>4. 构建多输入、多输出模型</h1><p>尝试构建一个具有2个输入、2个输出的模型。</p><p>输入：</p><ul><li>(32, 32, 3)：32×32的三通道图像。</li><li>(20, 10)：20个时间戳，10个特征。</li></ul><p>输出：</p><ul><li>(1,)：得分。</li><li>(5,)：分类。</li></ul><p>构建模型中损失函数和评价指标均可以以列表形式传入。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 构建模型</span><span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> layers<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModel</span><span class="hljs-params">(tf.keras.Model)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        super(MyModel, self).__init__()        <span class="hljs-comment"># 处理图像</span>        self.conv1 = layers.Conv2D(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)        self.pool1 = layers.GlobalMaxPooling2D()           <span class="hljs-comment"># 处理时间序列</span>        self.conv2 = layers.Conv1D(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)        self.pool2 = layers.GlobalMaxPooling1D()        <span class="hljs-comment"># 全连接层</span>        self.dense1 = layers.Dense(<span class="hljs-number">1</span>, name=<span class="hljs-string">'score_output'</span>)        self.dense2 = layers.Dense(<span class="hljs-number">5</span>, name=<span class="hljs-string">'class_output'</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, inputs)</span>:</span>        x1 = self.conv1(inputs[<span class="hljs-number">0</span>])        x1 = self.pool1(x1)        x2 = self.conv2(inputs[<span class="hljs-number">1</span>])        x2 = self.pool2(x2)        x = layers.concatenate([x1, x2])        score_output = self.dense1(x)        class_output = self.dense2(x)        <span class="hljs-keyword">return</span> score_output, class_outputmodel = MyModel()model.compile(    optimizer=tf.keras.optimizers.RMSprop(<span class="hljs-number">1e-3</span>),    <span class="hljs-comment"># 使用多个损失函数并命名</span>    loss=[        tf.keras.losses.MeanSquaredError(),        tf.keras.losses.CategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>)    ],    <span class="hljs-comment"># 指定损失函数的权重</span>    loss_weights=[<span class="hljs-number">2.</span>, <span class="hljs-number">1.</span>],    <span class="hljs-comment"># 使用多个评价指标并命名</span>    metrics=[        [            tf.keras.metrics.MeanAbsolutePercentageError(),            tf.keras.metrics.MeanAbsoluteError()        ],        [            tf.keras.metrics.CategoricalAccuracy()        ]    ])</code></pre></div><p>这个模型的可视化参考如下，注意<code>tf.keras.utils.plot_model()</code>方法无法对子类化模型使用。<br><img src="https://wx1.sbimg.cn/2020/06/07/3-4.png" srcset="/img/loading.gif" alt="Plot"></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 生成数据</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg_data = np.random.random_sample(size=(<span class="hljs-number">500</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>))ts_data = np.random.random_sample(size=(<span class="hljs-number">500</span>, <span class="hljs-number">20</span>, <span class="hljs-number">10</span>))score_targets = np.random.random_sample(size=(<span class="hljs-number">500</span>, <span class="hljs-number">1</span>))class_targets = np.random.random_sample(size=(<span class="hljs-number">500</span>, <span class="hljs-number">5</span>))</code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 训练</span>model.fit(    [img_data, ts_data],  <span class="hljs-comment"># 多输入</span>    [score_targets, class_targets],  <span class="hljs-comment"># 多输出</span>    batch_size=<span class="hljs-number">50</span>,    epochs=<span class="hljs-number">5</span>)</code></pre></div><p>训练结果：<br><img src="https://wx1.sbimg.cn/2020/06/07/3-5.png" srcset="/img/loading.gif" alt="Training"></p>]]></content>
    
    
    <categories>
      
      <category>TensorFlow2学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Python</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[TF2]2. Tensor and Basic Modeling</title>
    <link href="/2020/TF2-2-Tensor-and-Basic-Modeling/"/>
    <url>/2020/TF2-2-Tensor-and-Basic-Modeling/</url>
    
    <content type="html"><![CDATA[<p>首先导入tensorflow，查看版本和设备信息：</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfprint(tf.__version__)<span class="hljs-comment"># 2.1.1</span></code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># print(tf.test.is_gpu_available())  # Not recommended, use following instead</span>print(tf.config.list_physical_devices(<span class="hljs-string">'GPU'</span>))</code></pre></div><h1 id="1-张量与操作"><a href="#1-张量与操作" class="headerlink" title="1. 张量与操作"></a>1. 张量与操作</h1><p>tensorflow中有两种表示变量的方式：tf.Variable和tf.Tensor。Variale在Tensor基础上有一些额外的功能，通常用于单独的数据操作；Tensor则通常用于保存计算的中间结果。</p><p>使用Python自带的数据格式创建Variable：<br><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># rank意为变量的维度</span>r0 = tf.Variable(<span class="hljs-string">'rank0'</span>, tf.string)print(r0, <span class="hljs-string">'\nrank: '</span>, tf.rank(r0), <span class="hljs-string">'  shape: '</span>, tf.shape(r0), <span class="hljs-string">'\n'</span>)  <span class="hljs-comment"># rank=1, shape=(1,)</span>r2 = tf.Variable([[<span class="hljs-number">1</span>], [<span class="hljs-number">2</span>]], tf.int16)  <span class="hljs-comment"># rank=2, shape=(2,)</span>print(r2, <span class="hljs-string">'\nrank: '</span>, tf.rank(r2), <span class="hljs-string">'  shape: '</span>, tf.shape(r2), <span class="hljs-string">'\n'</span>)</code></pre></div></p><p>创建特殊矩阵、变型等方法类似于numpy：<br><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># create tensor</span>tf.constant([[<span class="hljs-number">1</span>], [<span class="hljs-number">2</span>], [<span class="hljs-number">3</span>]], tf.int16)tf.zeros((<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), tf.float16)<span class="hljs-comment"># print(tf.dtypes.cast(x, tf.int16))  # 改变格式</span></code></pre></div></p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># reshape</span>t = tf.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), tf.int16)print(tf.reshape(t, (<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)))</code></pre></div><p>Numpy与TensorFlow的一些函数对比:</p><div class="table-container"><table><thead><tr><th style="text-align:left">Numpy</th><th style="text-align:left">TensorFlow</th></tr></thead><tbody><tr><td style="text-align:left">a = np.zeros((2, 3))</td><td style="text-align:left">b = tf.zeros((2, 3))</td></tr><tr><td style="text-align:left">np.sum(a, axis=1)</td><td style="text-align:left">b = tf.reduce_sum(b, axis=1)</td></tr><tr><td style="text-align:left">a.shape</td><td style="text-align:left">b.get_shape()</td></tr><tr><td style="text-align:left">np.reshape(a, (3, 2))</td><td style="text-align:left">tf.reshape(b, (3, 2))</td></tr><tr><td style="text-align:left">a * 5 + 1</td><td style="text-align:left">b * 5 + 1</td></tr><tr><td style="text-align:left">np.dot(a, x)</td><td style="text-align:left">tf.matmul(b, x)</td></tr><tr><td style="text-align:left">a[0, 0]; a[:, 0]; a[0, :]</td><td style="text-align:left">b[0, 0]; b[:, 0]; b[0, :]</td></tr></tbody></table></div><p>常用的一些tensor操作包：</p><ul><li>tf.strings</li><li>tf.debugging</li><li>tf.dtypes</li><li>tf.math</li><li>tf.random</li><li>tf.feature_column</li></ul><h1 id="2-常用层"><a href="#2-常用层" class="headerlink" title="2. 常用层"></a>2. 常用层</h1><ul><li>tf.keras.layers：基于tf.nn高度封装的各种层</li><li>tf.nn：底层的函数库</li></ul><p>一些建立、配置层的基本操作：</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 数据</span>a = tf.random.uniform((<span class="hljs-number">10</span>, <span class="hljs-number">100</span>, <span class="hljs-number">50</span>), minval=<span class="hljs-number">-0.5</span>, maxval=<span class="hljs-number">0.5</span>)<span class="hljs-comment"># 实例化层对象</span>x = tf.keras.layers.LSTM(<span class="hljs-number">100</span>)(a)<span class="hljs-comment"># 在层中添加激活函数</span>tf.keras.layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)tf.keras.layers.Dense(<span class="hljs-number">64</span>, activation=tf.nn.relu)<span class="hljs-comment"># L1正则化应用于内核矩阵</span>tf.keras.layers.Dense(<span class="hljs-number">64</span>, kernel_regularizer=tf.keras.regularizers.l1(<span class="hljs-number">0.01</span>))<span class="hljs-comment"># L2正则化应用于偏差函数</span>tf.keras.layers.Dense(<span class="hljs-number">64</span>, bias_regularizer=tf.keras.regularizers.l2(<span class="hljs-number">0.01</span>))<span class="hljs-comment"># 内核初始化为随机正交矩阵</span>tf.keras.layers.Dense(<span class="hljs-number">64</span>, kernel_initializer=<span class="hljs-string">'orthogonal'</span>)<span class="hljs-comment"># 偏差向量初始化为2.0</span>tf.keras.layers.Dense(<span class="hljs-number">64</span>, bias_initializer=tf.keras.initializers.Constant(<span class="hljs-number">2.0</span>))</code></pre></div><h1 id="3-三种建模方式"><a href="#3-三种建模方式" class="headerlink" title="3. 三种建模方式"></a>3. 三种建模方式</h1><p>tensorflow中有三种主要的建模方式，分别为顺序式、函数式和子类式。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> layers</code></pre></div><h2 id="1-Sequential-Model-顺序模型"><a href="#1-Sequential-Model-顺序模型" class="headerlink" title="(1) Sequential Model (顺序模型)"></a>(1) Sequential Model (顺序模型)</h2><p>顺序模型使用 tf.keras.Sequential ，适用于建立较为简单的、数据流向单一的模型。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 第一种方式：逐层添加</span>model = tf.keras.Sequential()model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>))model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>))model.add(layers.Dense(<span class="hljs-number">10</span>))</code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 第二种方式：使用list传入</span>model = tf.keras.Sequential([    layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>, input_shape=(<span class="hljs-number">32</span>,)),    layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>),    layers.Dense(<span class="hljs-number">10</span>)])</code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 配置模型训练用的参数</span>model.compile(    optimizer=tf.keras.optimizers.Adam(<span class="hljs-number">0.01</span>),    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),    metrics=[<span class="hljs-string">'accuracy'</span>])<span class="hljs-comment"># 训练</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npdata = np.random.random((<span class="hljs-number">1000</span>, <span class="hljs-number">32</span>))labels = np.random.random((<span class="hljs-number">1000</span>, <span class="hljs-number">10</span>))model.fit(data, labels, epochs=<span class="hljs-number">10</span>, batch_size=<span class="hljs-number">10</span>)</code></pre></div><h2 id="2-Functional-Model-函数模型"><a href="#2-Functional-Model-函数模型" class="headerlink" title="(2) Functional Model (函数模型)"></a>(2) Functional Model (函数模型)</h2><ul><li>多输入/多输出模型</li><li>具有共享图层的模型</li><li>具有非顺序数据流的模型</li></ul><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 输入，可以有多个</span>input1 = tf.keras.Input(shape=(<span class="hljs-number">32</span>,))input2 = tf.keras.Input(shape=(<span class="hljs-number">32</span>,))<span class="hljs-comment"># 层，各输入可以经过不同的层</span>x1 = layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)(input1)x2 = layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)(input2)x = tf.concat((x1, x2), axis=<span class="hljs-number">-1</span>)x = layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)(x)<span class="hljs-comment"># 输出</span>pred = layers.Dense(<span class="hljs-number">10</span>)(x)</code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 配置模型</span>model = tf.keras.Model(inputs=(input1, input2), outputs=pred)model.compile(    optimizer=tf.keras.optimizers.RMSprop(<span class="hljs-number">0.001</span>),    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),    metrics=[<span class="hljs-string">'accuracy'</span>])<span class="hljs-comment"># 生成数据，这里有两个输入</span>data1 = np.random.random((<span class="hljs-number">1000</span>, <span class="hljs-number">32</span>))data2 = np.random.random((<span class="hljs-number">1000</span>, <span class="hljs-number">32</span>))labels = np.random.random((<span class="hljs-number">1000</span>, <span class="hljs-number">10</span>))<span class="hljs-comment"># 训练</span>model.fit((data1, data2), labels, epochs=<span class="hljs-number">10</span>, batch_size=<span class="hljs-number">10</span>)</code></pre></div><h2 id="3-Subclassing-Model-子类化模型"><a href="#3-Subclassing-Model-子类化模型" class="headerlink" title="(3) Subclassing Model (子类化模型)"></a>(3) Subclassing Model (子类化模型)</h2><p>继承tf.keras.Model，高度可定制化。</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 定义模型</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModel</span><span class="hljs-params">(tf.keras.Model)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, num_classes=<span class="hljs-number">10</span>)</span>:</span>        super(MyModel, self).__init__(name=<span class="hljs-string">'my_model'</span>)        self.num_classes = num_classes        <span class="hljs-comment"># 定义模型中的层</span>        self.dense_1 = layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)        self.dense_2 = layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)        self.dense_3 = layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)        self.dense_4 = layers.Dense(<span class="hljs-number">10</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, inputs)</span>:</span>        <span class="hljs-comment"># 使用上面定义的层进行前向传播</span>        x1 = self.dense_1(inputs[<span class="hljs-number">0</span>])        x2 = self.dense_2(inputs[<span class="hljs-number">1</span>])        x = tf.concat((x1, x2), axis=<span class="hljs-number">-1</span>)        x = self.dense_3(x)        <span class="hljs-comment"># 输出</span>        pred = self.dense_4(x)        <span class="hljs-keyword">return</span> pred</code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 配置模型</span>model = MyModel(num_classes=<span class="hljs-number">10</span>)model.compile(    optimizer=tf.keras.optimizers.RMSprop(<span class="hljs-number">0.001</span>),    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),    metrics=[<span class="hljs-string">'accuracy'</span>])<span class="hljs-comment"># 生成数据</span>data1 = np.random.random((<span class="hljs-number">1000</span>, <span class="hljs-number">32</span>))data2 = np.random.random((<span class="hljs-number">1000</span>, <span class="hljs-number">32</span>))labels = np.random.random((<span class="hljs-number">1000</span>, <span class="hljs-number">10</span>))<span class="hljs-comment"># 训练</span>model.fit((data1, data2), labels, epochs=<span class="hljs-number">10</span>, batch_size=<span class="hljs-number">10</span>)</code></pre></div><h1 id="4-练习"><a href="#4-练习" class="headerlink" title="4. 练习"></a>4. 练习</h1><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npiris = datasets.load_iris()data = iris.datalabels = iris.targetdata = np.concatenate((data,labels.reshape(<span class="hljs-number">150</span>,<span class="hljs-number">1</span>)),axis=<span class="hljs-number">-1</span>)np.random.shuffle(data)</code></pre></div><div class="hljs"><pre><code class="hljs Python">data.shape</code></pre></div><div class="hljs"><pre><code class="hljs Python">X = data[:,:<span class="hljs-number">4</span>]Y = data[:,<span class="hljs-number">-1</span>]print(X.shape, Y.shape)</code></pre></div><div class="hljs"><pre><code class="hljs Python">print(np.unique(Y))</code></pre></div><div class="hljs"><pre><code class="hljs Python">print(X[<span class="hljs-number">0</span>])</code></pre></div><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 自己完成的部分</span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">import</span> tensorflow.keras.layers <span class="hljs-keyword">as</span> layers<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModel</span><span class="hljs-params">(tf.keras.Model)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, num_classes=<span class="hljs-number">3</span>)</span>:</span>        super(MyModel, self).__init__()        self.num_classes = num_classes        self.Dense1 = layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">'relu'</span>, input_shape=(<span class="hljs-number">4</span>,))        self.Dense2 = layers.Dense(<span class="hljs-number">3</span>, activation=<span class="hljs-string">'relu'</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, inputs)</span>:</span>        x = self.Dense1(inputs)        x = self.Dense2(x)        <span class="hljs-keyword">return</span> xmodel = MyModel(num_classes=<span class="hljs-number">3</span>)</code></pre></div><div class="hljs"><pre><code class="hljs Python">model.compile(optimizer=tf.keras.optimizers.Adam(),              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])<span class="hljs-comment">#keras</span>model.fit(X, Y, batch_size=<span class="hljs-number">30</span>, epochs=<span class="hljs-number">50</span>, shuffle=<span class="hljs-literal">True</span>)</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>TensorFlow2学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Python</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Change Font of CMD</title>
    <link href="/2020/Change-Font-of-CMD/"/>
    <url>/2020/Change-Font-of-CMD/</url>
    
    <content type="html"><![CDATA[<ol><li><p>安装字体，自己用的是Microsoft Yahei Mono，下载地址 <a href="https://pan.baidu.com/s/1hFn2oYs1lmbDmrZhjJ3QBA&amp;shfl=shareset" target="_blank" rel="noopener">https://pan.baidu.com/s/1hFn2oYs1lmbDmrZhjJ3QBA&amp;shfl=shareset</a> ，提取码: tfgr。</p></li><li><p>打开注册表：win+r输入regedit。</p></li><li><p>定位到计算机 \HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion\Console\TrueTypeFont ，将936的数值数据修改为*Microsoft Yahei Mono。</p></li><li><p>打开CMD，右键点击打开属性，设置字号等。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>Others</category>
      
    </categories>
    
    
    <tags>
      
      <tag>环境设置</tag>
      
      <tag>系统设置</tag>
      
      <tag>实用技巧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Difference Between &#39;+=&#39; and &#39;=+&#39;</title>
    <link href="/2020/Difference-Between-&#39;+=&#39;-and=&#39;=+&#39;/"/>
    <url>/2020/Difference-Between-&#39;+=&#39;-and=&#39;=+&#39;/</url>
    
    <content type="html"><![CDATA[<p>+=运算会在对象原地址之上进行修改，而=…+会新生成一个对象，似乎只继承了值，而没有保留其他属性。</p><p>起因：学习PyTorch时，在一个回归问题的训练过程中更新参数：</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 初始化权重参数</span>w = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, dtype=torch.float, requires_grad=<span class="hljs-literal">True</span>)b = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, dtype=torch.float, requires_grad=<span class="hljs-literal">True</span>)<span class="hljs-comment"># 学习率</span>lr = <span class="hljs-number">0.001</span>loss = <span class="hljs-number">0</span><span class="hljs-comment"># 训练模型</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">500</span>):    <span class="hljs-comment"># 前向传播</span>    y_pred = torch.mm(torch.pow(x, <span class="hljs-number">2</span>), w) + b    <span class="hljs-comment"># 损失函数</span>    loss = torch.pow(y_pred - y, <span class="hljs-number">2</span>) / <span class="hljs-number">2</span>    loss = loss.sum()    <span class="hljs-comment"># 计算梯度</span>    loss.backward()  <span class="hljs-comment"># 梯度存储在grad属性中</span>    <span class="hljs-comment"># 手动更新参数</span>    <span class="hljs-keyword">with</span> torch.no_grad():        w = w - lr * w.grad        b = b - lr * b.grad        <span class="hljs-comment"># 梯度清零</span>        w.grad.zero_()        b.grad.zero_()</code></pre></div><p>报了如下错误：</p><div class="hljs"><pre><code class="hljs Python">AttributeError: <span class="hljs-string">'NoneType'</span> object has no attribute <span class="hljs-string">'zero_'</span></code></pre></div><p>替换为如下后正确了：</p><div class="hljs"><pre><code class="hljs Python">w -= lr * w.gradb -= lr * b.grad</code></pre></div><p>进行测试：</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-comment"># 初始化权重参数</span>w = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, dtype=torch.float, requires_grad=<span class="hljs-literal">True</span>)b = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, dtype=torch.float, requires_grad=<span class="hljs-literal">True</span>)print(<span class="hljs-string">'更新前：\nw&#123;&#125;, 梯度&#123;&#125;, 地址&#123;&#125;;  b&#123;&#125;, 梯度&#123;&#125;, 地址&#123;&#125;'</span>.format(w, w.grad, id(w), b, b.grad, id(b)))lr = <span class="hljs-number">0.001</span>y_pred = torch.mm(torch.pow(x, <span class="hljs-number">2</span>), w) + bloss = torch.pow(y_pred - y, <span class="hljs-number">2</span>) / <span class="hljs-number">2</span>loss = loss.sum()loss.backward()<span class="hljs-comment"># 不同方式更新参数</span><span class="hljs-keyword">with</span> torch.no_grad():    w = w - lr * w.grad    b -= lr * b.gradprint(<span class="hljs-string">'更新后：\nw&#123;&#125;, 梯度&#123;&#125;, 地址&#123;&#125;;  b&#123;&#125;, 梯度&#123;&#125;, 地址&#123;&#125;'</span>.format(w, w.grad, id(w), b, b.grad, id(b)))</code></pre></div><p>运行结果：</p><div class="hljs"><pre><code class="hljs Python">更新前：wtensor([[<span class="hljs-number">0.1969</span>]], requires_grad=<span class="hljs-literal">True</span>), 梯度<span class="hljs-literal">None</span>, 地址<span class="hljs-number">1166287893352</span>;  btensor([[<span class="hljs-number">-0.9487</span>]], requires_grad=<span class="hljs-literal">True</span>), 梯度<span class="hljs-literal">None</span>, 地址<span class="hljs-number">1166290443064</span>更新后：wtensor([[<span class="hljs-number">0.3587</span>]]), 梯度<span class="hljs-literal">None</span>, 地址<span class="hljs-number">1166290440344</span>;  btensor([[<span class="hljs-number">-0.5492</span>]], requires_grad=<span class="hljs-literal">True</span>), 梯度tensor([[<span class="hljs-number">-399.5237</span>]]), 地址<span class="hljs-number">1166290443064</span></code></pre></div><p>w的地址发生了改变且没有继承原来的梯度。</p>]]></content>
    
    
    <categories>
      
      <category>PyTorch学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>PyTorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pip tips</title>
    <link href="/2020/pip-tips/"/>
    <url>/2020/pip-tips/</url>
    
    <content type="html"><![CDATA[<h3 id="1-指定源"><a href="#1-指定源" class="headerlink" title="1. 指定源"></a>1. 指定源</h3><p>使用如下方法指定源：</p><div class="hljs"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> 库名 -i 源链接</code></pre></div><p>最常用的国内源是 <a href="https://pypi.tuna.tsinghua.edu.cn/simple/" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple/</a> 清华源。</p><h3 id="2-设置超时时间"><a href="#2-设置超时时间" class="headerlink" title="2. 设置超时时间"></a>2. 设置超时时间</h3><p>安装较大的库时，有时虽然有下载速度，但经常会因为超时而终止下载。这种情况可以将超时时间设置长一些。</p><p>使用如下方法设置超时时间（单位为秒）：</p><div class="hljs"><pre><code class="hljs routeros">pip install <span class="hljs-attribute">--default-time</span>=时间 库名</code></pre></div><h3 id="3-本地安装方法"><a href="#3-本地安装方法" class="headerlink" title="3. 本地安装方法"></a>3. 本地安装方法</h3><p>首先在对应的环境中执行以下代码，可以查看这个环境的pip支持的安装包类型：</p><div class="hljs"><pre><code class="hljs coffeescript"><span class="hljs-keyword">import</span> wheel.pep425tags <span class="hljs-keyword">as</span> w<span class="hljs-built_in">print</span>(w.get_supported(<span class="hljs-string">"win_amd64"</span>))  <span class="hljs-comment"># 64位</span></code></pre></div><p>然后到 <a href="https://pypi.org/" target="_blank" rel="noopener">https://pypi.org/</a> 或其他源网站，搜索需要的包。</p><ul><li>在 Release history 中可以选择自己需要的版本。</li><li>在 Download files 中可以找到安装包，选择自己的pip支持的类型下载到本地。</li></ul><p>下载完成后，使用<code>pip install 文件路径</code>即可。</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>实用技巧</tag>
      
      <tag>pip</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[TF2]1. Installation</title>
    <link href="/2020/TF2-1-Installation/"/>
    <url>/2020/TF2-1-Installation/</url>
    
    <content type="html"><![CDATA[<p>在 <a href="https://tensorflow.google.cn/install/gpu" target="_blank" rel="noopener">https://tensorflow.google.cn/install/gpu</a> 中查看各个依赖软件的版本要求。</p><h4 id="1-安装Nvidia驱动"><a href="#1-安装Nvidia驱动" class="headerlink" title="1. 安装Nvidia驱动"></a><strong>1. 安装Nvidia驱动</strong></h4><p>可以直接在GeForce Experience中更新驱动程序，或在 <a href="https://www.geforce.cn/drivers" target="_blank" rel="noopener">https://www.geforce.cn/drivers</a> 查找与自己显卡相符的驱动程序下载安装。</p><h4 id="2-安装CUDA"><a href="#2-安装CUDA" class="headerlink" title="2. 安装CUDA"></a><strong>2. 安装CUDA</strong></h4><p>进入<a href="https://developer.nvidia.com/cuda-toolkit" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit</a> ，选择下载，进入选择页面。</p><p>点击Legacy Releases，选择需要的CUDA版本。自己安装的是TensorFlow2.1.0，需要的CUDA版本为10.1。</p><p>进入系统选择页面，选择Windows-x86_64-10，安装方式建议exe(local)，方式因网络问题造成安装失败。</p><p>选择Base Installer中的Download，下载CUDA的本地安装包，安装。默认安装路径为C:\ProgramData\NVIDIA GPU Computing Toolkit。</p><p>安装完成后，在命令行运行<code>nvcc -V</code>查看CUDA版本以验证安装是否成功。</p><h4 id="3-安装CUDNN"><a href="#3-安装CUDNN" class="headerlink" title="3. 安装CUDNN"></a><strong>3. 安装CUDNN</strong></h4><p>进入<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-download</a> ，登录并填写问卷，进入下载页面。选择与CUDA版本相符的CUDNN，这里选择cuDNN v7.6.5 (November 5th, 2019) for CUDA 10.1，<br>cuDNN Library for Windows 10下载。</p><p>下载完成后，解压压缩包，将三个文件夹中的文件分别移动至CUDA安装路径中的对应文件夹里。</p><h4 id="4-安装TensorFlow"><a href="#4-安装TensorFlow" class="headerlink" title="4. 安装TensorFlow"></a><strong>4. 安装TensorFlow</strong></h4><ul><li>网络环境好：</li></ul><p>直接使用<code>pip install tensorflow-gpu==2.1.0</code>即可安装所有依赖库。</p><ul><li><p>网络环境不好，直接使用pip容易失败：</p><p>在 <a href="https://pypi.org/" target="_blank" rel="noopener">https://pypi.org/</a> 直接搜索需要的库，在release history中选择合适版本，下载后进行本地安装<code>pip install [本地路径]</code>。</p><p>查看pip支持的安装包类型，可以在需要安装TensorFlow的Python环境中执行：</p><div class="hljs"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> pip._internal.pep425tagsprint(pip._internal.pep425tags.get_supported())</code></pre></div></li></ul>]]></content>
    
    
    <categories>
      
      <category>TensorFlow2学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Python</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Test Blog</title>
    <link href="/2020/Test-Blog/"/>
    <url>/2020/Test-Blog/</url>
    
    <content type="html"><![CDATA[<p>更新后检查</p><h1 id="标题测试"><a href="#标题测试" class="headerlink" title="标题测试"></a>标题测试</h1><h2 id="LaTex"><a href="#LaTex" class="headerlink" title="LaTex"></a>LaTex</h2><ul><li>行内公式$ c = \sqrt{a^{2}+b_{xy}^{2}+e^{x}} $</li><li><p>公式块</p><script type="math/tex; mode=display">c = \sqrt{a^{2}+b_{xy}^{2} +e^{x}}</script><blockquote><p>引用</p></blockquote></li></ul><p><em>粗体</em>   <strong>斜体</strong></p><h2 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h2><div class="table-container"><table><thead><tr><th>Tables</th><th style="text-align:center">Are</th><th style="text-align:right">Cool</th></tr></thead><tbody><tr><td>col 3 is</td><td style="text-align:center">right-aligned</td><td style="text-align:right">$1600</td></tr><tr><td>col 2 is</td><td style="text-align:center">centered</td><td style="text-align:right">$12</td></tr><tr><td>zebra stripes</td><td style="text-align:center">are neat</td><td style="text-align:right">$1</td></tr></tbody></table></div><h2 id="代码段"><a href="#代码段" class="headerlink" title="代码段"></a>代码段</h2><div class="hljs"><pre><code class="hljs python3">import numpy as np</code></pre></div><h2 id="图片测试"><a href="#图片测试" class="headerlink" title="图片测试"></a>图片测试</h2><ul><li>图片<br><img src="http://ww2.sinaimg.cn/large/6aee7dbbgw1efffa67voyj20ix0ctq3n.jpg" srcset="/img/loading.gif" alt="测试图片"></li><li>图片链接<br><a href="http://ww2.sinaimg.cn/large/6aee7dbbgw1efffa67voyj20ix0ctq3n.jpg" target="_blank" rel="noopener">测试图片</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Others</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Others</tag>
      
      <tag>Blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2020/hello-world/"/>
    <url>/2020/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><div class="hljs"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">"My New Post"</span></code></pre></div><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><div class="hljs"><pre><code class="hljs bash">$ hexo server</code></pre></div><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><div class="hljs"><pre><code class="hljs bash">$ hexo generate</code></pre></div><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><div class="hljs"><pre><code class="hljs bash">$ hexo deploy</code></pre></div><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
